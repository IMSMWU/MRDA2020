<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Questionnaire design | Marketing Research Design &amp; Analysis 2020</title>
  <meta name="description" content="An Introduction to Statistics Using R" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Questionnaire design | Marketing Research Design &amp; Analysis 2020" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An Introduction to Statistics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Questionnaire design | Marketing Research Design &amp; Analysis 2020" />
  
  <meta name="twitter:description" content="An Introduction to Statistics Using R" />
  

<meta name="author" content="Mirza Mujanovic - 01553283" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="assignments.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>
<script src="libs/detect-resize-0.5.3/jquery.resize.js"></script>
<link href="libs/jquery-ui-1.11.4/jquery-ui.min.css" rel="stylesheet" />
<script src="libs/jquery-ui-1.11.4/jquery-ui.min.js"></script>
<script src="libs/d3-3.5.2/d3.min.js"></script>
<script src="libs/vega-1.4.3/vega.min.js"></script>
<script src="libs/lodash-2.2.1/lodash.min.js"></script>
<script>var lodash = _.noConflict();</script>
<link href="libs/ggvis-0.4.5/css/ggvis.css" rel="stylesheet" />
<script src="libs/ggvis-0.4.5/js/ggvis.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">MRDA 2019</a></strong></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html"><i class="fa fa-check"></i>Course materials</a><ul>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#finding-your-way-to-r"><i class="fa fa-check"></i>Finding Your Way To R</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#datacamp"><i class="fa fa-check"></i>DataCamp</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#getting-help"><i class="fa fa-check"></i><b>1.2</b> Getting help</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#functions"><i class="fa fa-check"></i><b>1.3</b> Functions</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#packages"><i class="fa fa-check"></i><b>1.4</b> Packages</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#a-typical-r-session"><i class="fa fa-check"></i><b>1.5</b> A typical R session</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>2</b> Data handling</a><ul>
<li class="chapter" data-level="2.1" data-path="data-handling.html"><a href="data-handling.html#basic-data-handling"><i class="fa fa-check"></i><b>2.1</b> Basic data handling</a><ul>
<li class="chapter" data-level="2.1.1" data-path="data-handling.html"><a href="data-handling.html#creating-objects"><i class="fa fa-check"></i><b>2.1.1</b> Creating objects</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-handling.html"><a href="data-handling.html#data-types"><i class="fa fa-check"></i><b>2.1.2</b> Data types</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-handling.html"><a href="data-handling.html#data-structures"><i class="fa fa-check"></i><b>2.1.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-handling.html"><a href="data-handling.html#advanced-data-handling"><i class="fa fa-check"></i><b>2.2</b> Advanced data handling</a><ul>
<li class="chapter" data-level="2.2.1" data-path="data-handling.html"><a href="data-handling.html#the-dplyr-package"><i class="fa fa-check"></i><b>2.2.1</b> The dplyr package</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-handling.html"><a href="data-handling.html#dealing-with-strings"><i class="fa fa-check"></i><b>2.2.2</b> Dealing with strings</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-handling.html"><a href="data-handling.html#case-study"><i class="fa fa-check"></i><b>2.2.3</b> Case study</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-handling.html"><a href="data-handling.html#data-import-and-export"><i class="fa fa-check"></i><b>2.3</b> Data import and export</a><ul>
<li class="chapter" data-level="2.3.1" data-path="data-handling.html"><a href="data-handling.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>2.3.1</b> Getting data for this course</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-handling.html"><a href="data-handling.html#import-data-created-by-other-software-packages"><i class="fa fa-check"></i><b>2.3.2</b> Import data created by other software packages</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-handling.html"><a href="data-handling.html#export-data"><i class="fa fa-check"></i><b>2.3.3</b> Export data</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-handling.html"><a href="data-handling.html#import-data-from-the-web"><i class="fa fa-check"></i><b>2.3.4</b> Import data from the Web</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>3</b> Summarizing data</a><ul>
<li class="chapter" data-level="3.1" data-path="summarizing-data.html"><a href="summarizing-data.html#summary-statistics"><i class="fa fa-check"></i><b>3.1</b> Summary statistics</a><ul>
<li class="chapter" data-level="3.1.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>3.1.1</b> Categorical variables</a></li>
<li class="chapter" data-level="3.1.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>3.1.2</b> Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="summarizing-data.html"><a href="summarizing-data.html#data-visualization"><i class="fa fa-check"></i><b>3.2</b> Data visualization</a><ul>
<li class="chapter" data-level="3.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables-1"><i class="fa fa-check"></i><b>3.2.1</b> Categorical variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables-1"><i class="fa fa-check"></i><b>3.2.2</b> Continuous variables</a></li>
<li class="chapter" data-level="3.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#saving-plots"><i class="fa fa-check"></i><b>3.2.3</b> Saving plots</a></li>
<li class="chapter" data-level="3.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#additional-options"><i class="fa fa-check"></i><b>3.2.4</b> Additional options</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="summarizing-data.html"><a href="summarizing-data.html#writing-reports-using-r-markdown"><i class="fa fa-check"></i><b>3.3</b> Writing reports using R-Markdown</a><ul>
<li class="chapter" data-level="3.3.1" data-path="summarizing-data.html"><a href="summarizing-data.html#creating-a-new-r-markdown-document"><i class="fa fa-check"></i><b>3.3.1</b> Creating a new R-Markdown document</a></li>
<li class="chapter" data-level="3.3.2" data-path="summarizing-data.html"><a href="summarizing-data.html#text-and-equations"><i class="fa fa-check"></i><b>3.3.2</b> Text and Equations</a></li>
<li class="chapter" data-level="3.3.3" data-path="summarizing-data.html"><a href="summarizing-data.html#r-code"><i class="fa fa-check"></i><b>3.3.3</b> R-Code</a></li>
<li class="chapter" data-level="3.3.4" data-path="summarizing-data.html"><a href="summarizing-data.html#latex-math"><i class="fa fa-check"></i><b>3.3.4</b> LaTeX Math</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#if-we-knew-it-all"><i class="fa fa-check"></i><b>4.1</b> If we knew it all</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#sampling-from-a-known-population"><i class="fa fa-check"></i><b>4.1.1</b> Sampling from a known population</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>4.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>4.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#using-what-we-actually-know"><i class="fa fa-check"></i><b>4.3</b> Using what we actually know</a><ul>
<li class="chapter" data-level="4.3.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#confidence-intervals-for-the-sample-mean"><i class="fa fa-check"></i><b>4.3.1</b> Confidence Intervals for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>5</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-null-hypothesis"><i class="fa fa-check"></i><b>5.1.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="5.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#statistical-inference-on-a-sample"><i class="fa fa-check"></i><b>5.1.2</b> Statistical inference on a sample</a></li>
<li class="chapter" data-level="5.1.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#choosing-the-right-test"><i class="fa fa-check"></i><b>5.1.3</b> Choosing the right test</a></li>
<li class="chapter" data-level="5.1.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#summary-1"><i class="fa fa-check"></i><b>5.1.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-t-test"><i class="fa fa-check"></i><b>5.2</b> One sample t-test</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-two-means"><i class="fa fa-check"></i><b>5.3</b> Comparing two means</a><ul>
<li class="chapter" data-level="5.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#independent-means-t-test"><i class="fa fa-check"></i><b>5.3.1</b> Independent-means t-test</a></li>
<li class="chapter" data-level="5.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#dependent-means-t-test"><i class="fa fa-check"></i><b>5.3.2</b> Dependent-means t-test</a></li>
<li class="chapter" data-level="5.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#further-considerations"><i class="fa fa-check"></i><b>5.3.3</b> Further considerations</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-several-means"><i class="fa fa-check"></i><b>5.4</b> Comparing several means</a><ul>
<li class="chapter" data-level="5.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction-1"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#decomposing-variance"><i class="fa fa-check"></i><b>5.4.2</b> Decomposing variance</a></li>
<li class="chapter" data-level="5.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-way-anova"><i class="fa fa-check"></i><b>5.4.3</b> One-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>5.5</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="5.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#mann-whitney-u-test-a.k.a.-wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>5.5.1</b> Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test)</a></li>
<li class="chapter" data-level="5.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>5.5.2</b> Wilcoxon signed-rank test</a></li>
<li class="chapter" data-level="5.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>5.5.3</b> Kruskal-Wallis test</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#categorical-data"><i class="fa fa-check"></i><b>5.6</b> Categorical data</a><ul>
<li class="chapter" data-level="5.6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#confidence-intervals-for-proportions"><i class="fa fa-check"></i><b>5.6.1</b> Confidence intervals for proportions</a></li>
<li class="chapter" data-level="5.6.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#chi-square-test"><i class="fa fa-check"></i><b>5.6.2</b> Chi-square test</a></li>
<li class="chapter" data-level="5.6.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#sample-size-1"><i class="fa fa-check"></i><b>5.6.3</b> Sample size</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#summary-of-hypothesis-testing"><i class="fa fa-check"></i><b>5.7</b> Summary of hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#correlation"><i class="fa fa-check"></i><b>6.1</b> Correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#correlation-coefficient"><i class="fa fa-check"></i><b>6.1.1</b> Correlation coefficient</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#significance-testing"><i class="fa fa-check"></i><b>6.1.2</b> Significance testing</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#regression-1"><i class="fa fa-check"></i><b>6.2</b> Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.2.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2.2</b> Multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#potential-problems"><i class="fa fa-check"></i><b>6.3</b> Potential problems</a><ul>
<li class="chapter" data-level="6.3.1" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>6.3.1</b> Outliers</a></li>
<li class="chapter" data-level="6.3.2" data-path="regression.html"><a href="regression.html#influential-observations"><i class="fa fa-check"></i><b>6.3.2</b> Influential observations</a></li>
<li class="chapter" data-level="6.3.3" data-path="regression.html"><a href="regression.html#non-linearity"><i class="fa fa-check"></i><b>6.3.3</b> Non-linearity</a></li>
<li class="chapter" data-level="6.3.4" data-path="regression.html"><a href="regression.html#non-constant-error-variance"><i class="fa fa-check"></i><b>6.3.4</b> Non-constant error variance</a></li>
<li class="chapter" data-level="6.3.5" data-path="regression.html"><a href="regression.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>6.3.5</b> Non-normally distributed errors</a></li>
<li class="chapter" data-level="6.3.6" data-path="regression.html"><a href="regression.html#correlation-of-errors"><i class="fa fa-check"></i><b>6.3.6</b> Correlation of errors</a></li>
<li class="chapter" data-level="6.3.7" data-path="regression.html"><a href="regression.html#collinearity"><i class="fa fa-check"></i><b>6.3.7</b> Collinearity</a></li>
<li class="chapter" data-level="6.3.8" data-path="regression.html"><a href="regression.html#omitted-variables"><i class="fa fa-check"></i><b>6.3.8</b> Omitted Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#categorical-predictors"><i class="fa fa-check"></i><b>6.4</b> Categorical predictors</a><ul>
<li class="chapter" data-level="6.4.1" data-path="regression.html"><a href="regression.html#two-categories"><i class="fa fa-check"></i><b>6.4.1</b> Two categories</a></li>
<li class="chapter" data-level="6.4.2" data-path="regression.html"><a href="regression.html#more-than-two-categories"><i class="fa fa-check"></i><b>6.4.2</b> More than two categories</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>6.5</b> Extensions of the linear model</a><ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#interaction-effects"><i class="fa fa-check"></i><b>6.5.1</b> Interaction effects</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#non-linear-relationships"><i class="fa fa-check"></i><b>6.5.2</b> Non-linear relationships</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="regression.html"><a href="regression.html#summary-of-regression"><i class="fa fa-check"></i><b>6.6</b> Summary of regression</a></li>
<li class="chapter" data-level="6.7" data-path="regression.html"><a href="regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.7</b> Logistic regression</a><ul>
<li class="chapter" data-level="6.7.1" data-path="regression.html"><a href="regression.html#motivation-and-intuition"><i class="fa fa-check"></i><b>6.7.1</b> Motivation and intuition</a></li>
<li class="chapter" data-level="6.7.2" data-path="regression.html"><a href="regression.html#technical-details-of-the-model"><i class="fa fa-check"></i><b>6.7.2</b> Technical details of the model</a></li>
<li class="chapter" data-level="6.7.3" data-path="regression.html"><a href="regression.html#estimation-in-r"><i class="fa fa-check"></i><b>6.7.3</b> Estimation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html"><i class="fa fa-check"></i><b>7</b> Exploratory factor analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#steps-in-factor-analysis"><i class="fa fa-check"></i><b>7.2</b> Steps in factor analysis</a><ul>
<li class="chapter" data-level="7.2.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#are-the-assumptions-satisfied"><i class="fa fa-check"></i><b>7.2.1</b> Are the assumptions satisfied?</a></li>
<li class="chapter" data-level="7.2.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#deriving-factors"><i class="fa fa-check"></i><b>7.2.2</b> Deriving factors</a></li>
<li class="chapter" data-level="7.2.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#factor-interpretation"><i class="fa fa-check"></i><b>7.2.3</b> Factor interpretation</a></li>
<li class="chapter" data-level="7.2.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#creating-new-variables"><i class="fa fa-check"></i><b>7.2.4</b> Creating new variables</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#reliability-analysis"><i class="fa fa-check"></i><b>7.3</b> Reliability analysis</a></li>
<li class="chapter" data-level="7.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#summary-of-factor-analysis"><i class="fa fa-check"></i><b>7.4</b> Summary of factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>8</b> Appendix</a><ul>
<li class="chapter" data-level="8.1" data-path="appendix.html"><a href="appendix.html#random-variables-probability-distributions"><i class="fa fa-check"></i><b>8.1</b> Random Variables &amp; Probability Distributions</a><ul>
<li class="chapter" data-level="8.1.1" data-path="appendix.html"><a href="appendix.html#random-variables"><i class="fa fa-check"></i><b>8.1.1</b> Random variables</a></li>
<li class="chapter" data-level="8.1.2" data-path="appendix.html"><a href="appendix.html#probability-distributions"><i class="fa fa-check"></i><b>8.1.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="8.1.3" data-path="appendix.html"><a href="appendix.html#appendix-1"><i class="fa fa-check"></i><b>8.1.3</b> Appendix</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="appendix.html"><a href="appendix.html#regression-2"><i class="fa fa-check"></i><b>8.2</b> Regression</a><ul>
<li class="chapter" data-level="8.2.1" data-path="appendix.html"><a href="appendix.html#linear-regression"><i class="fa fa-check"></i><b>8.2.1</b> Linear regression</a></li>
<li class="chapter" data-level="8.2.2" data-path="appendix.html"><a href="appendix.html#logistic-regression-1"><i class="fa fa-check"></i><b>8.2.2</b> Logistic regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="assignments.html"><a href="assignments.html"><i class="fa fa-check"></i><b>9</b> Assignments</a><ul>
<li class="chapter" data-level="9.1" data-path="assignments.html"><a href="assignments.html#assignment-2-hypothesis-testing"><i class="fa fa-check"></i><b>9.1</b> Assignment 2 (Hypothesis Testing)</a></li>
<li class="chapter" data-level="9.2" data-path="assignments.html"><a href="assignments.html#assignment-3-hypothesis-testing-2"><i class="fa fa-check"></i><b>9.2</b> Assignment 3 (Hypothesis Testing 2)</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="questionnaire-design.html"><a href="questionnaire-design.html"><i class="fa fa-check"></i><b>10</b> Questionnaire design</a><ul>
<li class="chapter" data-level="10.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-design-process"><i class="fa fa-check"></i><b>10.1</b> Questionnaire design process</a><ul>
<li class="chapter" data-level="10.1.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#specification-of-the-information-needed"><i class="fa fa-check"></i><b>10.1.1</b> Specification of the information needed</a></li>
<li class="chapter" data-level="10.1.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#specify-the-interviewing-method"><i class="fa fa-check"></i><b>10.1.2</b> Specify the interviewing method</a></li>
<li class="chapter" data-level="10.1.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#determine-the-content-of-questions"><i class="fa fa-check"></i><b>10.1.3</b> Determine the content of questions</a></li>
<li class="chapter" data-level="10.1.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#inability-and-unwillingness-to-answer"><i class="fa fa-check"></i><b>10.1.4</b> Inability and unwillingness to answer</a></li>
<li class="chapter" data-level="10.1.5" data-path="questionnaire-design.html"><a href="questionnaire-design.html#decide-on-measurement-scales-and-scaling-techniques"><i class="fa fa-check"></i><b>10.1.5</b> Decide on measurement scales and scaling techniques</a></li>
<li class="chapter" data-level="10.1.6" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-structure"><i class="fa fa-check"></i><b>10.1.6</b> Questionnaire structure</a></li>
<li class="chapter" data-level="10.1.7" data-path="questionnaire-design.html"><a href="questionnaire-design.html#question-wording"><i class="fa fa-check"></i><b>10.1.7</b> Question wording</a></li>
<li class="chapter" data-level="10.1.8" data-path="questionnaire-design.html"><a href="questionnaire-design.html#choose-adequate-order"><i class="fa fa-check"></i><b>10.1.8</b> Choose adequate order</a></li>
<li class="chapter" data-level="10.1.9" data-path="questionnaire-design.html"><a href="questionnaire-design.html#test-your-questionnaire"><i class="fa fa-check"></i><b>10.1.9</b> Test your questionnaire</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-in-qualtrics"><i class="fa fa-check"></i><b>10.2</b> Questionnaire in Qualtrics</a></li>
<li class="chapter" data-level="10.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#question-types-and-data-analysis"><i class="fa fa-check"></i><b>10.3</b> Question-Types-and-Data-Analysis</a><ul>
<li class="chapter" data-level="10.3.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#multiple-choice-with-a-single-answer"><i class="fa fa-check"></i><b>10.3.1</b> Multiple choice with a single answer</a></li>
<li class="chapter" data-level="10.3.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#multiple-choice-with-multiple-answers"><i class="fa fa-check"></i><b>10.3.2</b> Multiple choice with multiple answers</a></li>
<li class="chapter" data-level="10.3.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#rank-order-question"><i class="fa fa-check"></i><b>10.3.3</b> Rank order question</a></li>
<li class="chapter" data-level="10.3.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#constant-sum-question"><i class="fa fa-check"></i><b>10.3.4</b> Constant Sum question</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#gradient-boosting"><i class="fa fa-check"></i><b>10.4</b> Gradient Boosting</a><ul>
<li class="chapter" data-level="10.4.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#elements-of-supervised-machine-learning"><i class="fa fa-check"></i><b>10.4.1</b> Elements of supervised machine learning</a></li>
<li class="chapter" data-level="10.4.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#principle-behind-boosting"><i class="fa fa-check"></i><b>10.4.2</b> Principle behind boosting</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="questionnaire-design.html"><a href="questionnaire-design.html#xgboost-package"><i class="fa fa-check"></i><b>10.5</b> xgboost package</a><ul>
<li class="chapter" data-level="10.5.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#introduction-4"><i class="fa fa-check"></i><b>10.5.1</b> Introduction</a></li>
<li class="chapter" data-level="10.5.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#data-preparation"><i class="fa fa-check"></i><b>10.5.2</b> Data preparation</a></li>
<li class="chapter" data-level="10.5.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#engineering"><i class="fa fa-check"></i><b>10.5.3</b> Engineering</a></li>
<li class="chapter" data-level="10.5.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#strategy-for-tuning"><i class="fa fa-check"></i><b>10.5.4</b> [STRATEGY FOR TUNING]</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="questionnaire-design.html"><a href="questionnaire-design.html#exercise-in-progress"><i class="fa fa-check"></i><b>10.6</b> Exercise (IN PROGRESS)</a><ul>
<li class="chapter" data-level="10.6.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#exercise-to-download-in-progress"><i class="fa fa-check"></i><b>10.6.1</b> Exercise to download (IN PROGRESS)</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Marketing Research Design &amp; Analysis 2020</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="questionnaire-design" class="section level1">
<h1><span class="header-section-number">10</span> Questionnaire design</h1>
<p>Welcome to the questionnaire design guide!</p>
<p>An aim of this course is to develop your ability to translate business problems into actionable research questions and to design an adequate research plan to answer these questions. Therefore, you need to be equiped with knowledge on how to create a survey and properly conduct a research.</p>
<p>Generally, what you can expect from the survey design is similar to what one experiences in a relationship. If you try to take more than you commit, it doesn’t work out. Now on a serious note, if you follow guidelines mentioned here, you will certainly avoid usual traps your fellow collegues were caught in.</p>
<p>In a research process, conducting a survey is a part of (primary) data collection. Before we collect data, we have to make sure that preceding steps are correctly done. However, in the following sections we will focus on the process of designing a questionnaire. Eventually, you will be able to collect relevant data and apply appropriate statistical tests.</p>
<p><img src="research-process.PNG" width="70%" style="display: block; margin: auto;" /></p>
<div id="questionnaire-design-process" class="section level2">
<h2><span class="header-section-number">10.1</span> Questionnaire design process</h2>
<p>A structured questionnaire is a research instrument designed to elicit specific information from a sample of a target population. Usually it is used in a standardized way with fixed-alternative questions (same questions and response options for all respondents).</p>
<p>An objective of a questionnaire is threefold:</p>
<ul>
<li>to translate the information need into a set of specific questions that the respondent can and will answer,</li>
<li>to motivate, and encourage respondents to become involved, to cooperate, and to complete the questionnaire,</li>
<li>to minimize response error.</li>
</ul>
<p>In order to meet these objectives, a questionnaire design process suggests the following sequence of steps:</p>
<p><img src="questionnaire-design-process.png" width="70%" style="display: block; margin: auto;" /></p>
<div id="specification-of-the-information-needed" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Specification of the information needed</h3>
<p>The questionnaire design should be aligned with the research design!
In order to do make it aligned, it is necessary to review components of the problem and the approach. In particular, you should review the research questions, hypotheses and characteristics that influence the research design.</p>
<p>If you are interested in the causal effect of one particular (independent) variable on another (dependent) variable, think about an experimental design that might allow you to manipulate this variable. In this case, you particularly have to decide on the following:</p>
<ul>
<li>Which variable to manipulate?<br />
</li>
<li>Whether to use a between-subjects or within-subjects design?<br />
</li>
<li>The cause-effect sequence (the cause must occur before the effect)<br />
</li>
<li>The number of experimental conditions<br />
</li>
<li>Potential interactions and relationships with other variables (does the effect depend on another variable?)</li>
</ul>
<p>What you need to be careful about is the effect of <strong>reversed causation</strong>. The effect refers to the situation where the causal relationship could possible have an opposite direction from what we assumed at the first place. For instance, it is often assumed that an increase in individual income leads to increase in well-being (happiness). However, some <a href="https://www.ncbi.nlm.nih.gov/pubmed/16949692">researches</a> suggest that this causation could have an opposite direction, i.e. that actually increase in well-being of an individual leads to an increase in income.</p>
<p>Here are some examples of causal research design applications:</p>
<ul>
<li>To assess how a product’s country-of-origin impacts attractiveness across different countries.<br />
</li>
<li>To analyse the effects of rebranding on customer loyalty.</li>
</ul>
<p><img src="causation-effect.png" width="70%" style="display: block; margin: auto;" /></p>
<p>If you would like to analyze the effects of multiple categorical or continuous (independent) variables on one continuous (dependent) variable, you might use a regression model. When doing this, you particularly have to decide on:</p>
<ul>
<li><p>How to measure <strong>the dependent variable (DV)</strong>. This is particularly important, since you need a variable that is powerful in uncovering variation between subjects (e.g., open-ended questions, such as “How much are you willing to pay for this product” are good candidates). Moreover, you also need to consider the nature of your DV,i.e. whether it is an interval variable, ordinal or categorical variable. The nature of your DV will heavily influence your choice of a correct statistical test.</p></li>
<li><p>How to measure <strong>the independent variables (IV)</strong> (single-item vs. multi-item scales, categorical vs. continuous). Bear in mind that the nature of the IV, together with DV, affects your choice of a statistical test as well.</p></li>
<li><p>What other variables might cause the effect that you would like to investigate (to prevent omitted variable bias, i.e. variables that are not part of your model but still influence the dependent variable).</p></li>
<li><p>Potential interactions (e.g., is the effect of variable X stronger for group A vs. B?)</p></li>
</ul>
<p><img src="mlp-regression.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="specify-the-interviewing-method" class="section level3">
<h3><span class="header-section-number">10.1.2</span> Specify the interviewing method</h3>
<p>In the next step you should review the type of interviewing method you will use.
At this point you need to think in which setting you aim to conduct your survery. For instance, should you do it in a face-to-face setting or rather online. Here you can find some advantages and disadvantages of online surveys:
<img src="adv-disadv-online-questionnaire.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Additionally, here is the list of the online tools you can use to conduct an online survey (usually for free):</p>
<ul>
<li><a href="http://www.qualtrics.com/free-account/">Qualtrics</a></li>
<li><a href="https://www.google.com/forms/about/">Google form</a></li>
<li><a href="https://www.surveymonkey.com/">Survey monkey</a></li>
<li><a href="http://freeonlinesurveys.com/">Free online surverys</a></li>
<li><a href="http://kwiksurveys.com/">Kwik surveys</a></li>
</ul>
</div>
<div id="determine-the-content-of-questions" class="section level3">
<h3><span class="header-section-number">10.1.3</span> Determine the content of questions</h3>
<p>In this step you are starting to work on the content of you questions. There are several questions you should ask yourself when writing questions:</p>
<ul>
<li>Is the question necessary?</li>
<li>Will I obtain the needed information?<br />
</li>
<li>Are several questions needed instead of one?<br />
</li>
<li>What type of data can I collect by asking that question (categorical or continuious)?</li>
</ul>
<p>In your survey try to avoid asking <strong>double-barrelled questions.</strong>Those are
a single question that attempts to cover two issues. Such questions can be confusing to respondents and result in ambiguous responses. Instead, you might ask multiple questions in order to obtain the inteded information.</p>
<p><span style="color: red;"><B>Incorrect:</B></span><br />
Do you think Nike Town offers better variety and prices than other Nike stores?</p>
<p><span style="color: green;"><B>Correct:</B></span><br />
Do you think Nike Town offers better variety than other Nike stores?<br />
Do you think Nike Town offers better prices than other Nike stores?</p>
</div>
<div id="inability-and-unwillingness-to-answer" class="section level3">
<h3><span class="header-section-number">10.1.4</span> Inability and unwillingness to answer</h3>
<p>The quality of collected data you highly depends on your ability to address correct participants. Therefore, you need to make sure that your respondents are able to meaningfully answer your questions.</p>
<p>Examples:</p>
<ul>
<li>Not every household member might be informed about monthly expenses for groceries purchases if someone else makes these purchases.<br />
</li>
<li>Use filter questions that measure familiarity and product use.<br />
</li>
<li>Include a “don’t know” option.<br />
</li>
<li>If you ask participants for monteray values (e.g. how much are you ready to pay for the XY product?) across several EU, make sure you indicate correct currency (e.g. HRK for Croatia or HUF for Hungary).<br />
</li>
<li>Think about how mobile friendly is the layout of your survey (if it is an online survey).</li>
<li>Good case practices suggest that there should not be more than 2 questions per page (for online surveys displayed on mobile phones).</li>
</ul>
<p>If you are asking participants to recall certain brands for instance, make sure you use <strong>unaided recall question:</strong></p>
<p><span style="color: green;"><B>Example of unaided recall question:</B></span><br />
What brands of soft drinks do you remember being advertised on TV last night?<br />
<span style="color: red;"><B>Example of aided recall question:</B></span><br />
Which of these brands were advertised last night on TV?<br />
a) Coca-Cola<br />
b) Pepsi<br />
c) Red Bull<br />
d) Evian<br />
e) Don’t know</p>
<p>If you are asking participants to list something, the good case practice is <strong>to minimize the effort required by respondents:</strong></p>
<p><span style="color: red;"><B>Incorrect:</B></span><br />
Please list all the departments from which you purchased merchandise on your most recent shopping trip to department store X.<br />
<span style="color: green;"><B>Correct:</B></span><br />
Please check all the departments from which you purchased merchandise on your most recent shopping trip to a department store:<br />
a) Women’s dresses<br />
b) Men’s apparel<br />
c) Children’s apparel<br />
d) Cosmetics<br />
e) Jewelry<br />
f) Other (please specify) ___________</p>
<p>In a case you are asking for information that could be considered sensitive (e.g. money, family life, political beliefs, religion), they should come at the end of the questionnaire. Moreover, it is recommendable to provide response categories rather than asking for specific figures:</p>
<p><span style="color: red;"><B>Incorrect:</B></span><br />
What is your household’s exact annual income?<br />
<span style="color: green;"><B>Correct:</B></span><br />
Which one of the following categories best describes your household’s annual gross income?<br />
a) under 25.001 €<br />
b) 25.001€ to 50.000 €<br />
c) 50.001€ to 75.000 €<br />
d) 75.001€ to 100.000 €<br />
e) over 100.000 €</p>
</div>
<div id="decide-on-measurement-scales-and-scaling-techniques" class="section level3">
<h3><span class="header-section-number">10.1.5</span> Decide on measurement scales and scaling techniques</h3>
<p>Every statistical analysis requires that variables have a specific levels of measurement. Measurement scales you choose for your questions in a survey will affect the answers you get and eventually statistical test you can apply.
For instance, it would not make sense to compute an average of genders. An average of a categorical variable does not make much sense. Moreover, if you tried to compute the average of genders defined in numeric values (e.g. male=0, female=1), the output would be interpretable.</p>
<p>Therefore, it is crucial to become familiar with possibilities of each scale <strong>before</strong> you choose to add another question to your survey. Consequently, chances to obtain data you did not intend to collect and chances that you will not be able to apply tests you intended are significantly lower.</p>
<p>In the following table you can get a quick overview of possibilities per each measurement scale. :</p>
<p><img src="measurement-scale.png" width="90%" style="display: block; margin: auto;" /></p>
<p>In the table below you can find general procedure for choosing a correct analysis based on the measurement scale of your data and number of variables. It shows statistical analyses we covered during the course and aims to help you choose among them based on the nature of dependent variables on the side, and the nature and the number of your independent variables on the other side:</p>
<p><img src="overview-statistical-test.jpg" width="90%" style="display: block; margin: auto;" /></p>
<p>When it comes to scaling techniques, they are meant to study the relationship between objects. The basic scaling techniques classification is on <strong>comparative</strong> and <strong>non-comparative scales</strong>.</p>
<p><img src="scales.png" width="90%" style="display: block; margin: auto;" /></p>
<p><strong>The noncomparative scale</strong> each object is scaled independently of the other objects. The resulting data is supposed to be measured in an interval and ratio scaled.</p>
<p><strong>Comparative scales (or nonmetric scaling)</strong> compare direclty the stimulus object. For example, the respondent might be asked directly about his preference between domestic and foreign beer brands. As a result, the comparative data collected can only be interpreted in relative terms. In the following sections we will walk through both types of comparative scales and briefly introduce them.</p>
<div id="comparative-scale-paired-comparison" class="section level4">
<h4><span class="header-section-number">10.1.5.1</span> Comparative scale: Paired Comparison</h4>
<ul>
<li>Respondent is presented with two objects and asked to select one according to some criterion.</li>
<li>The nature of resulting data is ordinal</li>
<li>Assumption of transitivity (if X &gt; Y and Y &gt; Z, then X &gt; Z) enables the paired comparison data to be converted into a rank order. To do so, you need to indetify the number of times the object is preferred by adding up all the matrices.</li>
<li>Effective when the number of objects is limited as it requires the direct comparison, and a bigger number of objects makes the comparison becomes unmanagable.</li>
<li><em>Example:</em><br />
For each pair, please indicate which of the two brands of beer in the pair you prefer.
<img src="paired%20comparison.png" width="90%" style="display: block; margin: auto;" /></li>
</ul>
</div>
<div id="comparative-scale-rank-order" class="section level4">
<h4><span class="header-section-number">10.1.5.2</span> Comparative scale: Rank Order</h4>
<ul>
<li>Allow a certain set of brands or products to be simultaneously ranked based upon a specific attribute or characteristic.</li>
<li>The rank order scaling is a good proxy for to the shopping setting as there are simultanious comparisons of objects.</li>
<li>The rank order scaling results in the data of ordinal nature.</li>
<li><em>Example:</em><br />
Rank the various brands of beer in order of preference. Begin by picking out the one brand that you like most and assign it a number 1. Then find the second most preferred brand and assign it a number 2. Continue this procedure until you have ranked all the brands of beer in order of preference.
No two brands should received the same rank number.</li>
</ul>
<p><img src="rank-order-scale.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="comparative-scale-constant-sum" class="section level4">
<h4><span class="header-section-number">10.1.5.3</span> Comparative scale: Constant sum</h4>
<ul>
<li>Respondents allocate a constant sum of units (e.g., points, dollars) among a set of stimulus objects with respect to some criterion.<br />
</li>
<li>Constant sum is similar to rank order, but it carries specific units.<br />
</li>
<li>The resulting data does not just indicate important factors, but also by how much a factor supersedes another one.<br />
</li>
<li>Constant sum scaling can be used to observe the comparative significance respondents assigned to various factors of a subject.<br />
</li>
<li><em>Example:</em><br />
There are 8 attributes of bottled beers. Please allocate 100 points among the attributes so that your allocation reflects the relative importance you attach to each attribute.</li>
</ul>
<p><img src="constant-sum-scale.png" width="80%" style="display: block; margin: auto;" /></p>
<ul>
<li>Basic analysis of constant-sum data involves tabulation of responses and presenting them as either quantities (e.g., “on average, 7 points were allocated to”high alcohol level“), or, as proportions (”On average, 7% of points were allocated to “high alcohol level”).</li>
</ul>
</div>
<div id="non-comparative-scales-continuous-rating-scales" class="section level4">
<h4><span class="header-section-number">10.1.5.4</span> Non-Comparative Scales: Continuous Rating Scales</h4>
<ul>
<li>Participants rate the objects by placing a mark at the appropriate position on a line that runs from one extreme of the criterion variable to the other.<br />
</li>
<li>One of the advantages of the continuous rating scale is that it is easy to administer.</li>
</ul>
<p><img src="continuous-rating-scale.png" width="70%" style="display: block; margin: auto;" /></p>
<ul>
<li>Once the ratings are collected, you can splits up the obtained ratings into categories and then assign those depending on the category in which the ratings fall.</li>
</ul>
</div>
<div id="non-comparative-scales-itemized-rating-scales" class="section level4">
<h4><span class="header-section-number">10.1.5.5</span> Non-Comparative Scales: Itemized Rating Scales</h4>
<ul>
<li>The respondents are provided with a scale that has a number or brief description associated with each category.<br />
</li>
<li>The categories are ordered in terms of scale position, and the respondents are required to select the specified category that best describes the object being rated.<br />
</li>
<li>The commonly used itemized rating scales are <strong>the Likert, semantic differential and Stapel scales.</strong></li>
</ul>
<div id="itemized-rating-scales-likert-scale" class="section level5">
<h5><span class="header-section-number">10.1.5.5.1</span> Itemized Rating Scales: Likert scale</h5>
<ul>
<li>Requires respondents to indicate their attitude towards the given object through the degree of agreement or disagreement with each of a series of statements within typically five or seven categories.<br />
</li>
<li>Reversed code of some items increases validity.<br />
</li>
<li>One limitation is time required to answer a question on a Likert scale. Compared to other itemized scaling techniques, Likert scale is more time consuming as each respondent is required to read every statement given in a questionnaire before assigning a numerical value to it.</li>
</ul>
<p><img src="likert.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In the table below you can find a couple of commonly measured constructs in marketing research such as attitude, importance, purchase intention and similar.</p>
<p><img src="likert-marketing-reserach.png" width="72%" style="display: block; margin: auto;" /></p>
</div>
<div id="itemized-rating-scales-semantic-differential" class="section level5">
<h5><span class="header-section-number">10.1.5.5.2</span> Itemized Rating Scales: Semantic Differential</h5>
<ul>
<li><p>Typically, participants rate objects on a number of itemized, seven-point rating scales bounded at each end by one of two bipolar adjectives.</p></li>
<li><p>Semantic differential can measure respondent attitudes towards something (products,concepts, items, people…).</p></li>
<li><p>It helps you find the repondent’s position is on a scale between two bipolar adjectives such as “Sweet-Sour” or “Bright-Dark”. In comparison to Likert scale, which uses generic scales (e.g. extremely dissatisfied to extremely satisfied), semantic differential questions are posed within the context of evaluating attitudes.</p></li>
<li><p>Widely used rating scale in marketing research due to its versatility</p></li>
</ul>
<p><img src="semantic-differential.png" width="72%" style="display: block; margin: auto;" /></p>
<p>When creating a semantical difference question, you should consider the following:</p>
<ul>
<li><strong>Number of categories:</strong></li>
</ul>
<p><img src="semantic-differential-1.png" width="72%" style="display: block; margin: auto auto auto 0;" /></p>
<ul>
<li><strong>Balanced vs. unbalanced:</strong></li>
</ul>
<p><img src="semantic-differential-2.png" width="72%" style="display: block; margin: auto auto auto 0;" /></p>
<ul>
<li><strong>Odd/even number of categories:</strong></li>
</ul>
<p><img src="semantic-differential-3.png" width="72%" style="display: block; margin: auto auto auto 0;" /></p>
<ul>
<li><strong>Forced vs. non-forced response</strong></li>
</ul>
<p><img src="semantic-differential-4.png" width="72%" style="display: block; margin: auto auto auto 0;" /></p>
<ul>
<li><strong>Verbal description:</strong></li>
</ul>
<p><img src="semantic-differential-5.png" width="72%" style="display: block; margin: auto auto auto 0;" /></p>
</div>
</div>
</div>
<div id="questionnaire-structure" class="section level3">
<h3><span class="header-section-number">10.1.6</span> Questionnaire structure</h3>
<p>The sequnece of questions in a questionnaire could play imporant role. For instance, more sensitive questions (such as demographic-related questions) are usually placed at the end as they can trigger change in respondent’s behavior.</p>
<p>If you plan to conduct an online survey, then you need to think about the respondent’s experience while doing your questionnaire. For instance, spread the content over more short pages and do not have fewer long pages. In online surveys, two questions on one page is a useful rule of thumb. Generally, respondents are reluctant to read and fill out long questionnaire pages. Hence, long pages will lead to a higher dropout rate.
In order to reduce dropout rate state how long the survey will approximately take in the introduction of the questionnaire. Take into account that tools like Qualtrics provide the estimated response time in the survey overview.</p>
<p>Consider that the most of people usually use their phones to fill it out. Think about how the questionnaire will appear on a phone screen too. In that regard, think of length of questions especially.</p>
<p>In the end, the questionnaire structure has to be aligned with the research design. For example, if your research design features an experiment, this needs to be reflected in the questionnaire (e.g., you need to assign the respondents randomly to the experimental conditions in case of a between-subjects comparison).</p>
<div id="questionnaire-structure-for-a-between-subjects-design" class="section level4">
<h4><span class="header-section-number">10.1.6.1</span> Questionnaire structure for a between-subjects design</h4>
<p>In a between-subject design you randomly assign each respondent to different experimental conditions. They would then complete tasks only in the condition to which they are assigned.</p>
<p>For instance, we would like to test the effect of two advertisements on purchase intention. Therefore, one group of (randomly assigned) respondents will be exposed to one advertisement version while the other group (of randomly assigned respondents) will be exposed to another version. After that, both groups of respondents should express their willingness to buy the advertised product. Evenutally, if the dependent variable (e.g. willingness to buy) is measured on interval or ratio scale, then you can use independent t-test to compare group means. The whole experimental design should be organised as following:</p>
<p><img src="between-subject-design.png" width="72%" style="display: block; margin: auto;" /></p>
</div>
<div id="questionnaire-structure-for-a-within-subjects-design" class="section level4">
<h4><span class="header-section-number">10.1.6.2</span> Questionnaire structure for a within-subjects design</h4>
<p>This type of experimental design involves exposing each respondent to all of the user experimental conditions you’re testing. This way, each respondent will test all of the conditions.</p>
<p>For instance, we would like to test again the effect of two advertisements on purchase intentions, but this time in a within-subject design. First, each respondent will be exposed to the first version of advertisement and right after that asked to rate his/her willingness to buy the advertised product. Subsequently, each participant will be shown another version of advertisement and again rate his/her willingness to purchase the advertised product. Finally, we can compare group means with paired sample t-test (given that data is measured on interval or ratio scale).</p>
<p><img src="within-subject-design.png" width="72%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="question-wording" class="section level3">
<h3><span class="header-section-number">10.1.7</span> Question wording</h3>
<p>Generally, question wording should enable each respondent to understand questions and to be able to answer them with reliability. Reliability means that, if a respondent was asked the same question again, he/she would give the same answer again. A number of common problems regarding the question wording have been identified, so we will address the most important ones.</p>
<p>In order to ensure reliability, the issue in terms of <strong>who, what, when and where</strong> should be defined in each question.<br />
<em>Example:</em> Which brand of shampoo do you use?<br />
<strong>Who (the respondent):</strong> It is not clear whether this question relates to the individual respondent or the respondent’s total household.<br />
<strong>What (the brand of shampoo):</strong> It is unclear how the respondent is to answer this question if more than one brand is used.<br />
<strong>When (unclear):</strong> The time frame is not specified in this question. The respondent could interpret it as meaning the shampoo used this morning, this week, or over the past year.<br />
<strong>Where (not specified):</strong> At home, at the gym? Where?</p>
<p><em>A more clearly defined question is:</em><br />
Which brand or brands of shampoo have you personally used at home during the last month? In the case of more than one brand, please list all the brands that apply.</p>
<p><strong>Use ordinary words.</strong> Words should match the vocabulary level of the participants.<br />
<span style="color: red;"><B>Incorrect:</B></span><br />
“Do you think the distribution of soft drinks is adequate?”<br />
<span style="color: green;"><B>Correct:</B></span><br />
“Do you think soft drinks are easily available when you want to buy them?”</p>
<p><strong>Avoid double negative form</strong>. Double negative question forms can confuse respondents, especially when they need to answer with “Agree” or “Disagree”.</p>
<p><span style="color: red;"><B>Incorrect:</B></span><br />
Do you think that it is not uncommon that boys play basketball?<br />
<span style="color: green;"><B>Correct:</B></span><br />
In your opinion, is it common that boys play basketball?</p>
<p><strong>Avoid leading questions.</strong>Leading questions clue the participant to what the answer should be. Such questions introduce a bias in a particular direction.<br />
<span style="color: red;"><B>Incorrect:</B></span><br />
“Is Colgate your favorite toothpaste?”<br />
<span style="color: green;"><B>Correct:</B></span><br />
“What is your favorite brand of toothpaste?”</p>
<p><strong>Avoid ambiguous words.</strong> Words such as usually, normally, frequently, often, regularly, and other similar words, do not define frequency clearly enough.</p>
<p><span style="color: red;"><B>Incorrect:</B></span><br />
“In a typically month, how often do you go to a movie theater to see a movie?”<br />
a) Never<br />
b) Occasionally<br />
c) Sometimes<br />
d) Often<br />
e) Regularly</p>
<p><span style="color: green;"><B>Correct:</B></span><br />
“In a typically month, how often do you go to a movie theater to see a movie?”<br />
a) Less than once<br />
b) 1 or 2 times<br />
c) 3 or 4 times<br />
d) More than 4 times</p>
</div>
<div id="choose-adequate-order" class="section level3">
<h3><span class="header-section-number">10.1.8</span> Choose adequate order</h3>
<p>One of the last steps in a process of designing a questionnaire is choosing adequate order of questions and instructions for respondents.</p>
<p>At the begining, you should provide a short and easy-to-understand introduction to the topic. Use simple language and avoid technical terms (e.g., not many people will know the terms “manufacturer brand” and “store brand”). Additionally, in the introduction you should state how long the survey will approximately take.</p>
<p>The opening questions should be interesting, simple and non-threatening.
They are crucial because it is the respondent’s first exposure to the questionnaire and is likely to set the tone for the rest of questions in the questionnaire. If too difficult to understand, or sensitive in some way, respondents are likely to stop answering your questions. Qualifying questions (or screening questions) should serve as the opening questions (if applicable). Their purpose is to identify a potential respondent that is eligable to proceed with the research survey.</p>
<p>After the opening part, you should establish an optimal question flow.
General questions should precede the specific questions. Questions on one subject, or one particular aspect of a subject, should be grouped together. It may feel confusing to be asked to return to some subject they thought they already gave their opinions about.</p>
<p>As respondents are moving towards the end of the questionnaire, they are likely to become increasingly indifferent and might give careless answers. Therefore, questions of special importance should ideally be included in the earlier part of the questionnaire.</p>
<p>Finally, you should pay particular attention to provide all prescribed definitions and explanations before you ask a question. This ensures that the questions are undestood in consistent way by every respondent.</p>
</div>
<div id="test-your-questionnaire" class="section level3">
<h3><span class="header-section-number">10.1.9</span> Test your questionnaire</h3>
<p>Finally, before you distribute the final questionnaire, there are some things to consider. First, you should always pretest your questionnaire before sharing it!
Test all aspects of the questionnaire (content, wording, sequence, form &amp; layout, etc.). If possible, use respondents in the pretest that are similar to those who will be included in the actual survey. Ideally, the pretest sample size should be small (in a real scenario this could varyfrom 15 to 30 respondents; for the group project, a lower number will be sufficient). After each significant revision of the questionnaire, conduct another pretest, using a different sample of respondents. Eventually, code and analyze the responses obtained from the pretest so that you make sure that you collected information you intended to collect.</p>
<p>After testing your questionnaire you should be able to determine whether:</p>
<ul>
<li>The questions are properly framed<br />
</li>
<li>The questions wording triggers any biases<br />
</li>
<li>The questions are placed in the optimal order<br />
</li>
<li>The questions are understandable<br />
</li>
<li>Specifying questions are needed or some need to be eliminated</li>
</ul>
</div>
</div>
<div id="questionnaire-in-qualtrics" class="section level2">
<h2><span class="header-section-number">10.2</span> Questionnaire in Qualtrics</h2>
<p>A questionnaire creation in Qualtrics starts with creation of a Qulatrics project. Each project consists of a survey, distribution record, and collection of responses and reports. There are three ways to create a questionnaire.First, you can create a new survey project from scratch. Second, you can create a new questionnaire from a copy of an existing questionnaire. Eventually, you can create from a template in your Survey Library, or from an exported QSF file.</p>
<p>In order to create a completely new questionnaire, you need to do the following:</p>
<p>Go to the Projects page by clicking the Qualtric XM logo or clicking Projects on the top-right.</p>
<p><img src="create-new-project.png" width="72%" style="display: block; margin: auto;" /></p>
<p>Create new project by clicking the blue button on the right side.<br />
In the “Create your own” section click on the survey button.</p>
<p><img src="create-new-project-2.png" width="72%" style="display: block; margin: auto;" /></p>
<p>Enter a name for your survey and get started with a survey creation.</p>
<p><img src="new-survey.png" width="72%" style="display: block; margin: auto;" /></p>
<p>If you would like to create a new questionnaire on a basis of an already existing one, then you choose “From a Copy”. Subseqeuntly, you need to indicate the questionnaire you would like to copy. Now you are good to go!</p>
<p><img src="survey-copy.png" width="72%" style="display: block; margin: auto;" /></p>
<p>If there is a questionnaire in the Qualtrics Library you would like to use, then you need to choose “From Library”, and indicate one library name in the dropdown menu.</p>
<p><img src="library-survey.png" width="72%" style="display: block; margin: auto;" /></p>
</div>
<div id="question-types-and-data-analysis" class="section level2">
<h2><span class="header-section-number">10.3</span> Question-Types-and-Data-Analysis</h2>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["StartDate"],"name":[1],"type":["chr"],"align":["left"]},{"label":["EndDate"],"name":[2],"type":["chr"],"align":["left"]},{"label":["Status"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["IPAddress"],"name":[4],"type":["lgl"],"align":["right"]},{"label":["Progress"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Duration (in seconds)"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Finished"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["RecordedDate"],"name":[8],"type":["chr"],"align":["left"]},{"label":["ResponseId"],"name":[9],"type":["chr"],"align":["left"]},{"label":["RecipientLastName"],"name":[10],"type":["lgl"],"align":["right"]},{"label":["RecipientFirstName"],"name":[11],"type":["lgl"],"align":["right"]},{"label":["RecipientEmail"],"name":[12],"type":["lgl"],"align":["right"]},{"label":["ExternalReference"],"name":[13],"type":["lgl"],"align":["right"]},{"label":["LocationLatitude"],"name":[14],"type":["chr"],"align":["left"]},{"label":["LocationLongitude"],"name":[15],"type":["chr"],"align":["left"]},{"label":["DistributionChannel"],"name":[16],"type":["chr"],"align":["left"]},{"label":["UserLanguage"],"name":[17],"type":["chr"],"align":["left"]},{"label":["Q3_Rank_Order_1"],"name":[18],"type":["dbl"],"align":["right"]},{"label":["Q3_Rank_Order_2"],"name":[19],"type":["dbl"],"align":["right"]},{"label":["Q3_Rank_Order_3"],"name":[20],"type":["dbl"],"align":["right"]},{"label":["Q3_Rank_Order_4"],"name":[21],"type":["dbl"],"align":["right"]},{"label":["Q3_Rank_Order_5"],"name":[22],"type":["dbl"],"align":["right"]},{"label":["Q4_Constant_Sum_1"],"name":[23],"type":["dbl"],"align":["right"]},{"label":["Q4_Constant_Sum_2"],"name":[24],"type":["dbl"],"align":["right"]},{"label":["Q4_Constant_Sum_3"],"name":[25],"type":["dbl"],"align":["right"]},{"label":["Q4_Constant_Sum_4"],"name":[26],"type":["dbl"],"align":["right"]},{"label":["Q5_Matrix_table_1"],"name":[27],"type":["dbl"],"align":["right"]},{"label":["Q5_Matrix_table_2"],"name":[28],"type":["dbl"],"align":["right"]},{"label":["Q5_Matrix_table_3"],"name":[29],"type":["dbl"],"align":["right"]},{"label":["Q5_Matrix_table_4"],"name":[30],"type":["dbl"],"align":["right"]},{"label":["Q5_Matrix_table_5"],"name":[31],"type":["dbl"],"align":["right"]},{"label":["Q6_Bipolar_scale_1"],"name":[32],"type":["dbl"],"align":["right"]},{"label":["Q6_Bipolar_scale_2"],"name":[33],"type":["dbl"],"align":["right"]},{"label":["Q6_Bipolar_scale_3"],"name":[34],"type":["dbl"],"align":["right"]},{"label":["Q7_MC_sa_country"],"name":[35],"type":["dbl"],"align":["right"]},{"label":["Q7_MC_sa_country_3_TEXT"],"name":[36],"type":["lgl"],"align":["right"]},{"label":["Q8_MC_SA_Likert_Time"],"name":[37],"type":["dbl"],"align":["right"]},{"label":["Q9_MC_ma_1"],"name":[38],"type":["dbl"],"align":["right"]},{"label":["Q9_MC_ma_2"],"name":[39],"type":["dbl"],"align":["right"]},{"label":["Q9_MC_ma_3"],"name":[40],"type":["dbl"],"align":["right"]},{"label":["Q9_MC_ma_4"],"name":[41],"type":["dbl"],"align":["right"]},{"label":["Q10_Slider_1"],"name":[42],"type":["dbl"],"align":["right"]},{"label":["Q11_Open_ended_EUR_1"],"name":[43],"type":["dbl"],"align":["right"]},{"label":["Q12_cond_question"],"name":[44],"type":["dbl"],"align":["right"]},{"label":["Q13_YES_following_q"],"name":[45],"type":["dbl"],"align":["right"]},{"label":["Q17_1"],"name":[46],"type":["dbl"],"align":["right"]},{"label":["Q18"],"name":[47],"type":["dbl"],"align":["right"]},{"label":["Q19"],"name":[48],"type":["dbl"],"align":["right"]},{"label":["Q21#1_1_1"],"name":[49],"type":["dbl"],"align":["right"]},{"label":["Q21#2_1_1"],"name":[50],"type":["dbl"],"align":["right"]},{"label":["Q23_Gender"],"name":[51],"type":["dbl"],"align":["right"]},{"label":["Q23_Gender_3_TEXT"],"name":[52],"type":["lgl"],"align":["right"]},{"label":["Q24"],"name":[53],"type":["dbl"],"align":["right"]},{"label":["Condition"],"name":[54],"type":["chr"],"align":["left"]}],"data":[{"1":"25- 5-2 2   3:2 :45 PM","2":"25- 5-2 2   3:22:22 PM","3":"1","4":"NA","5":"1","6":"96","7":"1","8":"25- 5-2 2   3:22:23 PM","9":"R_2P5PIcN9kqNJNCG","10":"NA","11":"NA","12":"NA","13":"NA","14":"44.5384 6372 7 3","15":"18.667 98999 234","16":"preview","17":"EN","18":"2","19":"4","20":"1","21":"5","22":"3","23":"32","24":"23","25":"32","26":"13","27":"2","28":"3","29":"4","30":"4","31":"4","32":"7","33":"2","34":"5","35":"2","36":"NA","37":"3","38":"NA","39":"1","40":"NA","41":"1","42":"57","43":"4","44":"1","45":"2","46":"23","47":"5","48":"7","49":"12","50":"54","51":"2","52":"NA","53":"3","54":"A"},{"1":"4- 6-2 2   2:56:36 AM","2":"4- 6-2 2   2:58:26 AM","3":"1","4":"NA","5":"1","6":"11","7":"1","8":"4- 6-2 2   2:58:26 AM","9":"R_3JgUt8HfVB6vFXf","10":"NA","11":"NA","12":"NA","13":"NA","14":"48.19819641","15":"16.39169312","16":"preview","17":"EN","18":"1","19":"3","20":"5","21":"2","22":"4","23":"25","24":"30","25":"22","26":"23","27":"1","28":"2","29":"3","30":"4","31":"5","32":"4","33":"3","34":"4","35":"2","36":"NA","37":"4","38":"1","39":"1","40":"NA","41":"NA","42":"42","43":"4","44":"2","45":"NA","46":"5","47":"6","48":"6","49":"4","50":"3","51":"2","52":"NA","53":"4","54":"B"},{"1":"4- 6-2 2   3: 1:58 AM","2":"4- 6-2 2   3: 4:52 AM","3":"1","4":"NA","5":"1","6":"173","7":"1","8":"4- 6-2 2   3: 4:52 AM","9":"R_zerJWBGzioeaoLv","10":"NA","11":"NA","12":"NA","13":"NA","14":"48.19819641","15":"16.39169312","16":"preview","17":"EN","18":"1","19":"2","20":"4","21":"5","22":"3","23":"19","24":"21","25":"30","26":"30","27":"4","28":"4","29":"3","30":"4","31":"4","32":"3","33":"6","34":"7","35":"1","36":"NA","37":"5","38":"1","39":"NA","40":"NA","41":"1","42":"84","43":"3","44":"1","45":"2","46":"21","47":"6","48":"2","49":"21","50":"32","51":"2","52":"NA","53":"5","54":"A"},{"1":"NA","2":"NA","3":"NA","4":"NA","5":"NA","6":"NA","7":"NA","8":"NA","9":"NA","10":"NA","11":"NA","12":"NA","13":"NA","14":"NA","15":"NA","16":"NA","17":"NA","18":"2","19":"1","20":"3","21":"4","22":"5","23":"20","24":"20","25":"20","26":"40","27":"NA","28":"NA","29":"NA","30":"NA","31":"NA","32":"NA","33":"NA","34":"NA","35":"1","36":"NA","37":"4","38":"1","39":"NA","40":"1","41":"NA","42":"NA","43":"NA","44":"NA","45":"NA","46":"NA","47":"NA","48":"NA","49":"NA","50":"NA","51":"2","52":"NA","53":"4","54":"NA"},{"1":"NA","2":"NA","3":"NA","4":"NA","5":"NA","6":"NA","7":"NA","8":"NA","9":"NA","10":"NA","11":"NA","12":"NA","13":"NA","14":"NA","15":"NA","16":"NA","17":"NA","18":"5","19":"4","20":"3","21":"1","22":"2","23":"30","24":"30","25":"10","26":"30","27":"NA","28":"NA","29":"NA","30":"NA","31":"NA","32":"NA","33":"NA","34":"NA","35":"1","36":"NA","37":"5","38":"1","39":"NA","40":"1","41":"1","42":"NA","43":"NA","44":"NA","45":"NA","46":"NA","47":"NA","48":"NA","49":"NA","50":"NA","51":"2","52":"NA","53":"5","54":"NA"},{"1":"NA","2":"NA","3":"NA","4":"NA","5":"NA","6":"NA","7":"NA","8":"NA","9":"NA","10":"NA","11":"NA","12":"NA","13":"NA","14":"NA","15":"NA","16":"NA","17":"NA","18":"4","19":"2","20":"1","21":"3","22":"5","23":"0","24":"20","25":"20","26":"60","27":"NA","28":"NA","29":"NA","30":"NA","31":"NA","32":"NA","33":"NA","34":"NA","35":"1","36":"NA","37":"2","38":"1","39":"NA","40":"1","41":"NA","42":"NA","43":"NA","44":"NA","45":"NA","46":"NA","47":"NA","48":"NA","49":"NA","50":"NA","51":"1","52":"NA","53":"2","54":"NA"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>[1] 117 54
[1] 2 2 2 2 2 1 2 1 1 1 2 1 2 1 1 1 2 2 2 2 2 1 1 1 1 2 1 2 1 1 1 2 2 1 2 2 2
[38] 2 2 2 1 2 1 2 1 1 1 1 2 2 2 1 2 2 2 2 1 2 1 2 2 2 1 1 2 2 2 1 2 2 2 1 2 1
[75] 2 1 1 2 2 1 1 2 2 2 1 2 1 1 2 2 2 1 2 2 2 1 2 1 1 2 1 2 1 2 2 1 2 2 1 2 1
[112] 1 2 2 2 1 2
attr(,“label”)
Q23_Gender
" Selected Choice"
[1] 2 2 1 1 1 1 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 1 1 1 2 2 2 2 1 2 2 1 2 2 1
[38] 2 2 1 2 1 2 2 1 2 2 1 2 2 2 2 2 1 2 2 2 2 1 2 2 1 2 2 2 1 2 2 1 2 2 2 1 2
[75] 2 1 2 2 1 2 2 1 2 1 1 2 2 2 2 2 2 1 2 1 1 1 2 2 2 2 2 2 2 2 2 2 1 1 1 2 1
[112] 2 2 2 2 2 2
attr(,“label”)
Q7_MC_sa_country
" Selected Choice"</p>
<p>In this chapter we will encounter the nature of data you collect when conducting a survey. It will help you choose a type of a question depending on the nature of data you want to collect and on the type of statistical tests you want to apply.</p>
<p><a href="./ExampleQuestionnaireQualtrics.qsf">Here you can find an example of a questionnaire in Qualtrics with guidelines and suggestions related to each question type.</a></p>
<div id="multiple-choice-with-a-single-answer" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Multiple choice with a single answer</h3>
<p>Multiple Choice with a single answer is a type of closed-ended question that lets respondents select <strong>one answer</strong> from a defined list of choices.</p>
<p><img src="support-multiple-choice-question.png" width="72%" style="display: block; margin: auto;" /></p>
Type of data you obtain is <strong>categorical</strong>, and the output comes in the following form:<br />

<table>
<caption>
<span id="tab:unnamed-chunk-521">Table 10.1: </span>Multiple Choice Question with Single Answer
</caption>
<thead>
<tr>
<th style="text-align:right;">
In a typical week, how many hours do you spend watching movies or TV series on Netflix?
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
</tr>
</tbody>
</table>
<div id="data-handling-1" class="section level4">
<h4><span class="header-section-number">10.3.1.1</span> Data handling</h4>
<p>What to do with this data now? First, we need to load it in R and prepare for analysis. The numbers you see in the output R recognizes <strong>as numeric</strong>. In order to conduct statistical modelling and properly visualize our results, we need to convert our data to <strong>a factor class.</strong></p>
<p>A factor (or coding variable) represents different groups of data by using numbers (integers). In fact, factors appear as numeric variables, but they hold meaning of labels/names of data groups, i.e. nominal variable. These data groups are represented in a form of ‘levels’.<br />
In our case, our multiple choice question output will contain 4 data groups (‘Grocery Store’, ‘Online shop’, ‘Specialised coffee shop’, ‘other’) after converting it to factor:</p>
<div class="sourceCode" id="cb842"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb842-1" title="1"><span class="co"># Convert numeric value to factors</span></a>
<a class="sourceLine" id="cb842-2" title="2">qualtrics<span class="op">$</span><span class="st">&quot;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&quot;</span> &lt;-<span class="st"> </span><span class="kw">factor</span>(qualtrics<span class="op">$</span><span class="st">&quot;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&quot;</span>, </a>
<a class="sourceLine" id="cb842-3" title="3">    <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Never&quot;</span>, <span class="st">&quot;1-2 hours&quot;</span>, </a>
<a class="sourceLine" id="cb842-4" title="4">        <span class="st">&quot;3-4 hours&quot;</span>, <span class="st">&quot;5-6 hours&quot;</span>, <span class="st">&quot;more than 6 hours&quot;</span>))</a>
<a class="sourceLine" id="cb842-5" title="5"></a>
<a class="sourceLine" id="cb842-6" title="6">qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice_1</span><span class="st">`</span> &lt;-<span class="st"> </span><span class="kw">factor</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice_1</span><span class="st">`</span>, </a>
<a class="sourceLine" id="cb842-7" title="7">    <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>))</a>
<a class="sourceLine" id="cb842-8" title="8"></a>
<a class="sourceLine" id="cb842-9" title="9">qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice</span><span class="st">`</span> &lt;-<span class="st"> </span><span class="kw">factor</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice</span><span class="st">`</span>, </a>
<a class="sourceLine" id="cb842-10" title="10">    <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Austria&quot;</span>, <span class="st">&quot;Germany&quot;</span>))</a>
<a class="sourceLine" id="cb842-11" title="11"></a>
<a class="sourceLine" id="cb842-12" title="12"></a>
<a class="sourceLine" id="cb842-13" title="13"><span class="co"># Table</span></a>
<a class="sourceLine" id="cb842-14" title="14"><span class="kw">table</span>(qualtrics<span class="op">$</span><span class="st">&quot;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&quot;</span>)</a></code></pre></div>
<pre><code>## 
##             Never         1-2 hours         3-4 hours         5-6 hours 
##                19                18                22                35 
## more than 6 hours 
##                23</code></pre>
<div class="sourceCode" id="cb844"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb844-1" title="1"><span class="kw">table</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice</span><span class="st">`</span>)  <span class="co">#countries</span></a></code></pre></div>
<pre><code>## 
## Austria Germany 
##      35      82</code></pre>
<div class="sourceCode" id="cb846"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb846-1" title="1"><span class="kw">table</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice_1</span><span class="st">`</span>)  <span class="co">#gender</span></a></code></pre></div>
<pre><code>## 
##   Male Female 
##     49     68</code></pre>
</div>
<div id="visualisation" class="section level4">
<h4><span class="header-section-number">10.3.1.2</span> Visualisation</h4>
<p>Second, you might want to visualize your results. In order to do so, the data format needs to be in the appropriate format.Here we proceed with data fromat adaptation from the point where we stopped:</p>
<div class="sourceCode" id="cb848"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb848-1" title="1"><span class="co"># Converting long format to the</span></a>
<a class="sourceLine" id="cb848-2" title="2"><span class="co"># visualisation-friendly format</span></a>
<a class="sourceLine" id="cb848-3" title="3">mlc_visualisation &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(qualtrics<span class="op">$</span><span class="st">&quot;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&quot;</span>))</a>
<a class="sourceLine" id="cb848-4" title="4"></a>
<a class="sourceLine" id="cb848-5" title="5"><span class="co"># Naming columns</span></a>
<a class="sourceLine" id="cb848-6" title="6"><span class="kw">names</span>(mlc_visualisation) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Time&quot;</span>, <span class="st">&quot;Count&quot;</span>)</a>
<a class="sourceLine" id="cb848-7" title="7"></a>
<a class="sourceLine" id="cb848-8" title="8"><span class="co"># Observing</span></a>
<a class="sourceLine" id="cb848-9" title="9">mlc_visualisation</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Time"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["Count"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"Never","2":"19"},{"1":"1-2 hours","2":"18"},{"1":"3-4 hours","2":"22"},{"1":"5-6 hours","2":"35"},{"1":"more than 6 hours","2":"23"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The simpliest way to visualize data obtained from multiple choice question with a single answer is <strong>a bar chart</strong>:</p>
<div class="sourceCode" id="cb849"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb849-1" title="1"><span class="co">## Basic bar chart</span></a>
<a class="sourceLine" id="cb849-2" title="2">labels &lt;-<span class="st"> </span><span class="kw">as.character</span>(mlc_visualisation<span class="op">$</span>Time) <span class="co">#Save labels for x-axis in the barplot</span></a>
<a class="sourceLine" id="cb849-3" title="3"><span class="kw">barplot</span>(mlc_visualisation<span class="op">$</span>Count, <span class="co"># Column to visualize</span></a>
<a class="sourceLine" id="cb849-4" title="4">        <span class="dt">xlab=</span><span class="st">&#39;Time&#39;</span>, <span class="co"># X-axis label</span></a>
<a class="sourceLine" id="cb849-5" title="5">        <span class="dt">ylab =</span> <span class="st">&#39;Count(answers)&#39;</span>, <span class="co"># Y-axis label</span></a>
<a class="sourceLine" id="cb849-6" title="6">        <span class="dt">names.arg =</span> labels,</a>
<a class="sourceLine" id="cb849-7" title="7">        <span class="dt">main =</span> <span class="st">&#39;How many hours do you spend watching movies or series on Netflix?&#39;</span>) <span class="co"># Title</span></a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-524-1.png" width="672" /></p>
<p>R package <strong>ggplot2</strong> allows you to create visually appealing graphs:</p>
<div class="sourceCode" id="cb850"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb850-1" title="1"><span class="co">## ggplot2 bar chart</span></a>
<a class="sourceLine" id="cb850-2" title="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb850-3" title="3">p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> mlc_visualisation, <span class="kw">aes</span>(<span class="dt">x =</span> Time, </a>
<a class="sourceLine" id="cb850-4" title="4">    <span class="dt">y =</span> Count, <span class="dt">fill =</span> Time)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb850-5" title="5"><span class="st">    </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;In a typical week, how many hours do you spend watching movies or series on Netflix?&quot;</span>)</a>
<a class="sourceLine" id="cb850-6" title="6">p</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-525-1.png" width="672" /></p>
<p>Another R library which can help you make amazing interactive charts in a minute is <strong>plotly</strong>. Here we use a function called <strong>ggplotly()</strong>, which allows you to turn any <strong>ggplot2</strong> chart interactive. Since we have already created a bar chart using ggplot2 and saved it as “p”, we will just turn it into plotly graph:</p>
<div class="sourceCode" id="cb851"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb851-1" title="1"><span class="co">## ggplotly bar chart</span></a>
<a class="sourceLine" id="cb851-2" title="2"></a>
<a class="sourceLine" id="cb851-3" title="3"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb851-4" title="4"><span class="kw">ggplotly</span>(p)</a></code></pre></div>
<div id="htmlwidget-9feace85ad66a7a06526" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-9feace85ad66a7a06526">{"x":{"data":[{"orientation":"v","width":0.9,"base":0,"x":[1],"y":[19],"text":"Time: Never<br />Count: 19<br />Time: Never","type":"bar","marker":{"autocolorscale":false,"color":"rgba(248,118,109,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"Never","legendgroup":"Never","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.9,"base":0,"x":[2],"y":[18],"text":"Time: 1-2 hours<br />Count: 18<br />Time: 1-2 hours","type":"bar","marker":{"autocolorscale":false,"color":"rgba(163,165,0,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"1-2 hours","legendgroup":"1-2 hours","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.9,"base":0,"x":[3],"y":[22],"text":"Time: 3-4 hours<br />Count: 22<br />Time: 3-4 hours","type":"bar","marker":{"autocolorscale":false,"color":"rgba(0,191,125,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"3-4 hours","legendgroup":"3-4 hours","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.9,"base":0,"x":[4],"y":[35],"text":"Time: 5-6 hours<br />Count: 35<br />Time: 5-6 hours","type":"bar","marker":{"autocolorscale":false,"color":"rgba(0,176,246,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"5-6 hours","legendgroup":"5-6 hours","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.9,"base":0,"x":[5],"y":[23],"text":"Time: more than 6 hours<br />Count: 23<br />Time: more than 6 hours","type":"bar","marker":{"autocolorscale":false,"color":"rgba(231,107,243,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"more than 6 hours","legendgroup":"more than 6 hours","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":37.2602739726027},"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"In a typical week, how many hours do you spend watching movies or series on Netflix?","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.4,5.6],"tickmode":"array","ticktext":["Never","1-2 hours","3-4 hours","5-6 hours","more than 6 hours"],"tickvals":[1,2,3,4,5],"categoryorder":"array","categoryarray":["Never","1-2 hours","3-4 hours","5-6 hours","more than 6 hours"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Time","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-1.75,36.75],"tickmode":"array","ticktext":["0","10","20","30"],"tickvals":[0,10,20,30],"categoryorder":"array","categoryarray":["0","10","20","30"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Count","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":0.96751968503937},"annotations":[{"text":"Time","x":1.02,"y":1,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"left","yanchor":"bottom","legendTitle":true}],"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"174815347774":{"x":{},"y":{},"fill":{},"type":"bar"}},"cur_data":"174815347774","visdat":{"174815347774":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>An improved version of ggplot2 package is the packaged called <strong>ggvis</strong>, which is still in developing:</p>
<div class="sourceCode" id="cb852"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb852-1" title="1"><span class="co">## ggvis bar chart</span></a>
<a class="sourceLine" id="cb852-2" title="2"></a>
<a class="sourceLine" id="cb852-3" title="3"><span class="kw">library</span>(ggvis)</a>
<a class="sourceLine" id="cb852-4" title="4"><span class="kw">ggvis</span>(mlc_visualisation, <span class="dt">x =</span> <span class="op">~</span>Time, <span class="dt">y =</span> <span class="op">~</span>Count, <span class="dt">fill =</span> <span class="op">~</span>Time)</a></code></pre></div>
<div id="plot_id548674276-container" class="ggvis-output-container">
<div id="plot_id548674276" class="ggvis-output"></div>
<div class="plot-gear-icon">
<nav class="ggvis-control">
<a class="ggvis-dropdown-toggle" title="Controls" onclick="return false;"></a>
<ul class="ggvis-dropdown">
<li>
Renderer: 
<a id="plot_id548674276_renderer_svg" class="ggvis-renderer-button" onclick="return false;" data-plot-id="plot_id548674276" data-renderer="svg">SVG</a>
 | 
<a id="plot_id548674276_renderer_canvas" class="ggvis-renderer-button" onclick="return false;" data-plot-id="plot_id548674276" data-renderer="canvas">Canvas</a>
</li>
<li>
<a id="plot_id548674276_download" class="ggvis-download" data-plot-id="plot_id548674276">Download</a>
</li>
</ul>
</nav>
</div>
</div>
<script type="text/javascript">
var plot_id548674276_spec = {
  "data": [
    {
      "name": "mlc_visualisation0/group_by1/count2/stack3_flat",
      "format": {
        "type": "csv",
        "parse": {
          "stack_lwr_": "number",
          "stack_upr_": "number"
        }
      },
      "values": "\"Time\",\"x_\",\"stack_lwr_\",\"stack_upr_\"\n\"Never\",\"Never\",0,19\n\"1-2 hours\",\"1-2 hours\",0,18\n\"3-4 hours\",\"3-4 hours\",0,22\n\"5-6 hours\",\"5-6 hours\",0,35\n\"more than 6 hours\",\"more than 6 hours\",0,23"
    },
    {
      "name": "mlc_visualisation0/group_by1/count2/stack3",
      "source": "mlc_visualisation0/group_by1/count2/stack3_flat",
      "transform": [
        {
          "type": "treefacet",
          "keys": [
            "data.Time"
          ]
        }
      ]
    },
    {
      "name": "scale/fill",
      "format": {
        "type": "csv",
        "parse": {}
      },
      "values": "\"domain\"\n\"Never\"\n\"1-2 hours\"\n\"3-4 hours\"\n\"5-6 hours\"\n\"more than 6 hours\""
    },
    {
      "name": "scale/x",
      "format": {
        "type": "csv",
        "parse": {}
      },
      "values": "\"domain\"\n\"Never\"\n\"1-2 hours\"\n\"3-4 hours\"\n\"5-6 hours\"\n\"more than 6 hours\""
    },
    {
      "name": "scale/y",
      "format": {
        "type": "csv",
        "parse": {
          "domain": "number"
        }
      },
      "values": "\"domain\"\n0\n36.75"
    }
  ],
  "scales": [
    {
      "name": "fill",
      "type": "ordinal",
      "domain": {
        "data": "scale/fill",
        "field": "data.domain"
      },
      "points": true,
      "sort": false,
      "range": "category10"
    },
    {
      "domain": {
        "data": "scale/x",
        "field": "data.domain"
      },
      "name": "x",
      "type": "ordinal",
      "points": false,
      "padding": 0.1,
      "sort": false,
      "range": "width"
    },
    {
      "name": "y",
      "domain": {
        "data": "scale/y",
        "field": "data.domain"
      },
      "zero": false,
      "nice": false,
      "clamp": false,
      "range": "height"
    }
  ],
  "marks": [
    {
      "type": "group",
      "from": {
        "data": "mlc_visualisation0/group_by1/count2/stack3"
      },
      "marks": [
        {
          "type": "rect",
          "properties": {
            "update": {
              "stroke": {
                "value": "#000000"
              },
              "fill": {
                "scale": "fill",
                "field": "data.Time"
              },
              "x": {
                "scale": "x",
                "field": "data.x_"
              },
              "y": {
                "scale": "y",
                "field": "data.stack_lwr_"
              },
              "y2": {
                "scale": "y",
                "field": "data.stack_upr_"
              },
              "width": {
                "scale": "x",
                "band": true
              }
            },
            "ggvis": {
              "data": {
                "value": "mlc_visualisation0/group_by1/count2/stack3"
              }
            }
          }
        }
      ]
    }
  ],
  "legends": [
    {
      "orient": "right",
      "fill": "fill",
      "title": "Time"
    }
  ],
  "axes": [
    {
      "type": "x",
      "scale": "x",
      "orient": "bottom",
      "layer": "back",
      "grid": true,
      "title": "Time"
    },
    {
      "type": "y",
      "scale": "y",
      "orient": "left",
      "layer": "back",
      "grid": true,
      "title": "Count"
    }
  ],
  "padding": null,
  "ggvis_opts": {
    "keep_aspect": false,
    "resizable": true,
    "padding": {},
    "duration": 250,
    "renderer": "svg",
    "hover_duration": 0,
    "width": 672,
    "height": 480
  },
  "handlers": null
};
ggvis.getPlot("plot_id548674276").parseSpec(plot_id548674276_spec);
</script>
</div>
<div id="statistical-analysis" class="section level4">
<h4><span class="header-section-number">10.3.1.3</span> Statistical analysis</h4>
<p>Data type collected from the previous question is ordinal as we are able to make a natural order of the levels. Since it is ordinal data type, it belongs to categorical data. For the analysis of categorical data we can use Chi-square test or Fisher’s test if a count for some level is less than 5.</p>
<div id="fischers-exact" class="section level5">
<h5><span class="header-section-number">10.3.1.3.1</span> Fischer’s exact</h5>
Fisher’s exact test is used to test a hypothesis with data obtained from multiple choice questions with single answer. Results from multiple choice questions with multiple answers are treated with different test.
<ul>
<li>
<B> Application: </B> when you have <B> 1 dependent variable and 1 independent variable with 2 or more levels/factors </B>
</ul>
</li>
<ul>
<li>
Used when frequency in at least one cell is <B> less than 5 </B>. When frequencies in each cell are greater than 5, Chi-square test should be used.
</ul>
</li>
<ul>
<li>
<B>Hypothesis:</B> Is there a significant difference in frequencies between values observed in cells and values expected in cells ? (R for Marketing and Research Analytics)
</ul>
</li>
<ul>
<li>
<B>H0:</B> There is no relationship between the two categorical variables.Therefore, two categorical variables are <B> independent.</B> Knowing the value of one variable does not help to predict the value of the other variable.
</ul>
</li>
<ul>
<li>
<B>H1:</B> There is a relationship between the two categorical variables.Therefore, two categorical variables are <B> dependent.</B>Knowing the value of one variable helps to predict the value of the other variable.
</ul>
</li>
<ul>
<li>
Usually, this type of test is used on 2x2 contingency tables. However, it can be applicable on contingency tables of larger dimensions.
</ul>
</li>
<p><B>Example:</B> We would like to know whether a number of hours spent watching Netflix depends on the respondents’ country of origin.</p>
<div class="sourceCode" id="cb853"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb853-1" title="1"><span class="co"># Creation of contingency table</span></a>
<a class="sourceLine" id="cb853-2" title="2">fisher_test_table &lt;-<span class="st"> </span><span class="kw">table</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice</span><span class="st">`</span>, </a>
<a class="sourceLine" id="cb853-3" title="3">    qualtrics<span class="op">$</span><span class="st">&quot;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&quot;</span>)</a>
<a class="sourceLine" id="cb853-4" title="4"><span class="co"># Check how our contigency table looks like</span></a>
<a class="sourceLine" id="cb853-5" title="5">fisher_test_table</a></code></pre></div>
<pre><code>##          
##           Never 1-2 hours 3-4 hours 5-6 hours more than 6 hours
##   Austria     3         7         6        11                 8
##   Germany    16        11        16        24                15</code></pre>
<div class="sourceCode" id="cb855"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb855-1" title="1"><span class="co"># Since we have a count less than 5, we should</span></a>
<a class="sourceLine" id="cb855-2" title="2"><span class="co"># apply Fisher&#39;s test instead of Chi-square.</span></a>
<a class="sourceLine" id="cb855-3" title="3"></a>
<a class="sourceLine" id="cb855-4" title="4"><span class="co"># Fisher&#39;s test</span></a>
<a class="sourceLine" id="cb855-5" title="5">test &lt;-<span class="st"> </span><span class="kw">fisher.test</span>(fisher_test_table)</a>
<a class="sourceLine" id="cb855-6" title="6">test</a></code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  fisher_test_table
## p-value = 0.575
## alternative hypothesis: two.sided</code></pre>
<div class="sourceCode" id="cb857"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb857-1" title="1"><span class="co"># p-value</span></a>
<a class="sourceLine" id="cb857-2" title="2">test<span class="op">$</span>p.value</a></code></pre></div>
<pre><code>## [1] 0.5750401</code></pre>
<p>From the output and from test$p.value we see that the p-value is higher than the significance level of 5%. Like any other statistical test, if the p-value is higher than the significance level, we can not reject the null hypothesis.</p>
<p>In our case, not rejecting the null hypothesis for the Fisher’s exact test of independence means that there is no significant relationship between the two categorical variables. Therefore, knowing the value of one variable does not help to predict the value of the other variable.</p>
</div>
<div id="chi-square-test-goodness-of-fit-independence-test" class="section level5">
<h5><span class="header-section-number">10.3.1.3.2</span> Chi-square test: Goodness of fit &amp; Independence test</h5>
<ol style="list-style-type: decimal">
<li>Goodness of fit
<div>
<ul>
<li>
<B> Application: </B>when you only have <B> 1 dependent variable and none independent variables </B>
</ul>
</li>
<ul>
<li>
<B> Hypothesis:</B> Is there a significant difference in frequencies between values observed in cells and values expected in cells ? (R for Marketing and Research Analytics)
</ul>
</li>
<ul>
<li>
<B> H0: </B> There is no significant difference between the observed and the expected frequencies.
</ul>
</li>
<ul>
<li>
<B> H1: </B> There is a significant difference between the observed and the expected frequencies.
</ul>
</li>
<ul>
<li>
If we don’t specify expected frequency per cell (see in the code below), then it is expected that all cells show an eqaul frequency.
</ul>
</li>
<ul>
<li>
<B> Example</B> :‘Do the numbers of respondents who are spending different amount of hours watching Netflix <B> significantly differ from each other?</B>’
</ul>
</li>
</div>
<ul>
<li>
<B> Note that we did not assume any specific distribution, so we are assuming that each count will have the same or similar number.
<ul>
<li>
<B></li>
</ol>
<div class="sourceCode" id="cb859"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb859-1" title="1"><span class="co"># Creating table</span></a>
<a class="sourceLine" id="cb859-2" title="2">(mlc_chi_square &lt;-<span class="st"> </span><span class="kw">table</span>(qualtrics<span class="op">$</span><span class="st">&quot;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&quot;</span>))</a></code></pre></div>
<pre><code>## 
##             Never         1-2 hours         3-4 hours         5-6 hours 
##                19                18                22                35 
## more than 6 hours 
##                23</code></pre>
<div class="sourceCode" id="cb861"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb861-1" title="1"><span class="co"># Chi-square test (without given expected values =</span></a>
<a class="sourceLine" id="cb861-2" title="2"><span class="co"># equal values )</span></a>
<a class="sourceLine" id="cb861-3" title="3"><span class="kw">chisq.test</span>(mlc_chi_square)</a></code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  mlc_chi_square
## X-squared = 7.9145, df = 4, p-value = 0.09476</code></pre>
<p>The p-value of the test is higher than 0.05. We can conclude that the numbers of respondents who spent different amount of hours watching Netflix are commonly distributed. Observed distribution does not differ significantly from the expected. This result does not surprise if you take a look at the values for each level in the table we created before conducting the test. There you can see that count of answers in each level is more or less not deviating too much. It is visible if you take a look at the previous visualisations as well.</p>
<p>If we are interested in testing more specific distribution, i.e. expect that 40% of our respondents are watching Netflix 3-4 hours, we can introduce corresponding distribution in the test.</p>
<div class="sourceCode" id="cb863"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb863-1" title="1"><span class="co"># Expected values in percentages for each alternative. The sum must be 1.</span></a>
<a class="sourceLine" id="cb863-2" title="2">expected_values &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="co"># We expect that 10% of our respondents do not watch Netflix at all (&quot;Never&quot;).</span></a>
<a class="sourceLine" id="cb863-3" title="3">                     <span class="fl">0.20</span>, <span class="co"># We expect that 20% of our respondents watch Netflix 1-2 hours a week.  </span></a>
<a class="sourceLine" id="cb863-4" title="4">                     <span class="fl">0.40</span>, <span class="co"># We expect that 40% of our respondents watch Netflix 3-4 hours a week.</span></a>
<a class="sourceLine" id="cb863-5" title="5">                     <span class="fl">0.20</span>, <span class="co"># We expect that 20% of our respondents watch Netflix 5-6 hours a week.</span></a>
<a class="sourceLine" id="cb863-6" title="6">                     <span class="fl">0.10</span> <span class="co"># We expect that 10% of our respondents watch Netflix more than 6 hours a week.</span></a>
<a class="sourceLine" id="cb863-7" title="7">                    )</a>
<a class="sourceLine" id="cb863-8" title="8"><span class="co"># Chi-square test with expected values</span></a>
<a class="sourceLine" id="cb863-9" title="9"><span class="kw">chisq.test</span>(mlc_chi_square, <span class="dt">p=</span>expected_values)</a></code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  mlc_chi_square
## X-squared = 35.607, df = 4, p-value = 0.0000003486</code></pre>
<p>This time the p-value of the test is lower than 0.05.We have an evidance that observed distribution does significantly differ from the expected distribution (10%/20%/40%/20%/10%).</p>
<ol start="2" style="list-style-type: decimal">
<li>Chi-Square Test of Independence
<div>
<ul>
<li>
<B> Application: </B>when you have <B> 1 dependent variable and 1 independent variable with 2 or more levels/factors </B>
</ul>
</li>
<ul>
<li>
<B> Hypothesis: </B> Is there an association between categorical variable X and categorical variable Y?
</ul>
</li>
<ul>
<li>
<B> H0: </B> There is no association between the two variables.</B>
</ul>
</li>
<ul>
<li>
<B> H1: </B> There is an association between the two variables.</B>
</ul>
</li>
<ul>
<li>
<B> Example: </B> Is there an association between gender and the hours spent watching Neflix during a week?
</ul>
</li>
</div></li>
</ol>
<div class="sourceCode" id="cb865"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb865-1" title="1"><span class="co"># Creation of contingency table</span></a>
<a class="sourceLine" id="cb865-2" title="2">chi_square_table &lt;-<span class="st"> </span><span class="kw">table</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Selected Choice_1</span><span class="st">`</span>, </a>
<a class="sourceLine" id="cb865-3" title="3">    qualtrics<span class="op">$</span><span class="st">&quot;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&quot;</span>)</a>
<a class="sourceLine" id="cb865-4" title="4"></a>
<a class="sourceLine" id="cb865-5" title="5"><span class="co"># Chi-square independence test</span></a>
<a class="sourceLine" id="cb865-6" title="6"><span class="kw">chisq.test</span>(chi_square_table)</a></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  chi_square_table
## X-squared = 1.5739, df = 4, p-value = 0.8135</code></pre>
<p>Since the p-value (0.8135) is higher than the significance level (0.05), we cannot reject the null hypothesis. Thus, we conclude that there is no association relationship between gender and number of hours spent watching Netflix. Therefore, we can say that the hours spent is independent from the gender of participant.</p>
</div>
</div>
</div>
<div id="multiple-choice-with-multiple-answers" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Multiple choice with multiple answers</h3>
<p><img src="multiple-choice-question-multiple-answers.png" width="72%" style="display: block; margin: auto;" /></p>
<p>Before we conduct any test, we will do some simple calculatios and visualise our data.</p>
<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb867-1" title="1"><span class="co"># Rename columns</span></a>
<a class="sourceLine" id="cb867-2" title="2"><span class="kw">colnames</span>(qualtrics)[<span class="dv">38</span>] &lt;-<span class="st"> &quot;ja!Naturlich&quot;</span></a>
<a class="sourceLine" id="cb867-3" title="3"><span class="kw">colnames</span>(qualtrics)[<span class="dv">39</span>] &lt;-<span class="st"> &quot;Clever&quot;</span></a>
<a class="sourceLine" id="cb867-4" title="4"><span class="kw">colnames</span>(qualtrics)[<span class="dv">40</span>] &lt;-<span class="st"> &quot;Spar Vital&quot;</span></a>
<a class="sourceLine" id="cb867-5" title="5"><span class="kw">colnames</span>(qualtrics)[<span class="dv">41</span>] &lt;-<span class="st"> &quot;...&quot;</span></a>
<a class="sourceLine" id="cb867-6" title="6"></a>
<a class="sourceLine" id="cb867-7" title="7"><span class="co"># Replacing NA with 0</span></a>
<a class="sourceLine" id="cb867-8" title="8">qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">ja!Naturlich</span><span class="st">`</span>[<span class="kw">is.na</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">ja!Naturlich</span><span class="st">`</span>)] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb867-9" title="9">qualtrics<span class="op">$</span>Clever[<span class="kw">is.na</span>(qualtrics<span class="op">$</span>Clever)] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb867-10" title="10">qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">Spar Vital</span><span class="st">`</span>[<span class="kw">is.na</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">Spar Vital</span><span class="st">`</span>)] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb867-11" title="11">qualtrics<span class="op">$</span>...[<span class="kw">is.na</span>(qualtrics<span class="op">$</span>...)] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb867-12" title="12"></a>
<a class="sourceLine" id="cb867-13" title="13"><span class="co"># Calculating frequency, percentage of respondents</span></a>
<a class="sourceLine" id="cb867-14" title="14"><span class="co"># and percentage of cases</span></a>
<a class="sourceLine" id="cb867-15" title="15">df.cochran &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Frequnecy =</span> <span class="kw">colSums</span>(qualtrics[<span class="dv">38</span><span class="op">:</span><span class="dv">41</span>]), </a>
<a class="sourceLine" id="cb867-16" title="16">    <span class="dt">Share_of_respondents =</span> (<span class="kw">colSums</span>(qualtrics[<span class="dv">38</span><span class="op">:</span><span class="dv">41</span>])<span class="op">/</span><span class="kw">sum</span>(qualtrics[<span class="dv">38</span><span class="op">:</span><span class="dv">41</span>])) <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb867-17" title="17"><span class="st">        </span><span class="dv">100</span>, <span class="dt">Share_of_cases =</span> ((<span class="kw">colSums</span>(qualtrics[<span class="dv">38</span><span class="op">:</span><span class="dv">41</span>]))<span class="op">/</span><span class="kw">nrow</span>(qualtrics[<span class="dv">38</span><span class="op">:</span><span class="dv">41</span>])) <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb867-18" title="18"><span class="st">        </span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb867-19" title="19"><span class="co"># Observing</span></a>
<a class="sourceLine" id="cb867-20" title="20">df.cochran</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Frequnecy"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Share_of_respondents"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Share_of_cases"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"80","2":"31.25000","3":"68.37607"},{"1":"62","2":"24.21875","3":"52.99145"},{"1":"71","2":"27.73438","3":"60.68376"},{"1":"43","2":"16.79688","3":"36.75214"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb868"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb868-1" title="1"><span class="co"># Visualisation</span></a>
<a class="sourceLine" id="cb868-2" title="2"><span class="kw">barplot</span>(df.cochran[, <span class="dv">3</span>], <span class="dt">names.arg =</span> <span class="kw">row.names</span>(df.cochran), </a>
<a class="sourceLine" id="cb868-3" title="3">    <span class="dt">main =</span> <span class="st">&quot;% of Respondents familiar with brands&quot;</span>, </a>
<a class="sourceLine" id="cb868-4" title="4">    <span class="dt">xlab =</span> <span class="st">&quot;Brand&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Percentage&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-533-1.png" width="672" /></p>
<p>The visualisation above depicts the fact that more than 60% percent of people are familiar with the brand “ja!Naturlich”, while we can not say the same for other brands considered in our question.</p>
<p>For the analysis of results collected with multiple choice question with multiple possible answers, we can use <strong>Cochran’s Q test.</strong>Although we did not mention it before, it is not too different from what you have already learned about other tests.</p>
<p>The Cochran’s Q test and associated multiple comparisons require the following assumptions:
1. Responses are dichotomous and from k number of matched samples.
2. The subjects are independent of one another and were selected at random from a larger population.
3. The sample size is sufficiently “large”. (As a rule of thumb, the number of subjects for which the
responses are not all 0’s or 1’s, n, should be ≥ 4 and nk should be ≥ 24)</p>
<p>In a within-subjects experiment design with three or more observations of a dichotomous(= just two levels such as “Yes” or “No”) categorical outcome, you utilize Cochran’s Q test to assess main effects.Similarly, in our multiple choice question with multiple answers we have the same respondent going through three or more potential answers with dichotomous(=yes or no) categorical outcome.</p>
<div class="sourceCode" id="cb869"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb869-1" title="1"><span class="kw">library</span>(nonpar)</a>
<a class="sourceLine" id="cb869-2" title="2"></a>
<a class="sourceLine" id="cb869-3" title="3"><span class="co"># Creation of matrix</span></a>
<a class="sourceLine" id="cb869-4" title="4">matrix.cochran &lt;-<span class="st"> </span><span class="kw">cbind</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">ja!Naturlich</span><span class="st">`</span>, qualtrics<span class="op">$</span>Clever, </a>
<a class="sourceLine" id="cb869-5" title="5">    qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">Spar Vital</span><span class="st">`</span>, qualtrics<span class="op">$</span>...)</a>
<a class="sourceLine" id="cb869-6" title="6"><span class="co"># Turning NAs to 0</span></a>
<a class="sourceLine" id="cb869-7" title="7">matrix.cochran[<span class="kw">is.na</span>(matrix.cochran)] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb869-8" title="8"></a>
<a class="sourceLine" id="cb869-9" title="9"><span class="co"># Cochran test</span></a>
<a class="sourceLine" id="cb869-10" title="10"><span class="kw">cochrans.q</span>(matrix.cochran, <span class="dt">alpha =</span> <span class="fl">0.05</span>)</a></code></pre></div>
<pre><code>## 
##  Cochran&#39;s Q Test 
##  
##  H0: There is no difference in the effectiveness of treatments. 
##  HA: There is a difference in the effectiveness of treatments. 
##  
##  Q = 25.5681818181818 
##  
##  Degrees of Freedom = 3 
##  
##  Significance Level = 0.05 
##  The p-value is  0.0000117439848891232 
##  There is enough evidence to conclude that the effectiveness of at least two treatments differ. 
## </code></pre>
<p>The p-value less than 0.05 indicates that there is enough evidence to conclude that some of the store brands are better known among our respondents than other. In order to take a closer look at it, we need to conduct a post hoc test.</p>
<div class="sourceCode" id="cb871"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb871-1" title="1"><span class="kw">library</span>(DescTools)</a>
<a class="sourceLine" id="cb871-2" title="2">list.cochran &lt;-<span class="st"> </span><span class="kw">list</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">ja!Naturlich</span><span class="st">`</span>, qualtrics<span class="op">$</span>Clever, </a>
<a class="sourceLine" id="cb871-3" title="3">    qualtrics<span class="op">$</span><span class="st">`</span><span class="dt">Spar Vital</span><span class="st">`</span>, qualtrics<span class="op">$</span>...)</a>
<a class="sourceLine" id="cb871-4" title="4"></a>
<a class="sourceLine" id="cb871-5" title="5"><span class="co"># Replacing NAs in the list with 0 in order to be</span></a>
<a class="sourceLine" id="cb871-6" title="6"><span class="co"># able to run the test</span></a>
<a class="sourceLine" id="cb871-7" title="7">list.cochran &lt;-<span class="st"> </span><span class="kw">rapply</span>(list.cochran, <span class="dt">f =</span> <span class="cf">function</span>(x) <span class="kw">ifelse</span>(<span class="kw">is.na</span>(x), </a>
<a class="sourceLine" id="cb871-8" title="8">    <span class="dv">0</span>, x), <span class="dt">how =</span> <span class="st">&quot;replace&quot;</span>)</a>
<a class="sourceLine" id="cb871-9" title="9"></a>
<a class="sourceLine" id="cb871-10" title="10"><span class="co"># Post hoc test (Dunn Test)</span></a>
<a class="sourceLine" id="cb871-11" title="11"><span class="kw">DunnTest</span>(list.cochran, <span class="dt">method =</span> <span class="st">&quot;bonferroni&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Dunn&#39;s test of multiple comparisons using rank sums : bonferroni  
## 
##     mean.rank.diff      pval    
## 2-1            -36    0.1093    
## 3-1            -18    1.0000    
## 4-1            -74 0.0000073 ***
## 3-2             18    1.0000    
## 4-2            -38    0.0761 .  
## 4-3            -56    0.0014 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From the results of the Dunn Test, we can see that there is a big difference between 1 (“ja!Natürlich”) and 4(“…”), as well as between 4(“…”) and 3(“Spar Vital”).</p>
</div>
<div id="rank-order-question" class="section level3">
<h3><span class="header-section-number">10.3.3</span> Rank order question</h3>
<p><img src="rank-order-question.png" width="72%" style="display: block; margin: auto;" /></p>
<p>A rank order question asks respondents to compare items to each other by placing them in order of preference. Note that the data obtained from a rank order question shows an order of a respondent’s pereference, but not the difference between items. For instance, if the the most important feature of a fitness tracker for a respondendt XY is “Measuring steps” and the second most important feature “Calories burned”, we don’t know for how much more important is the former one in comparison to the latter one.</p>
<p>Intuitive question to ask is the following: which feature of the fitness tracker is the most important for our respondents?</p>
<p>We can answer this question by calculating a mean rank for each feature. Before we do so, we will create a separate data frame and add columns of the response data.</p>
<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb873-1" title="1">rank.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Measuring steps</span><span class="st">`</span>, </a>
<a class="sourceLine" id="cb873-2" title="2">    qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Calories burned</span><span class="st">`</span>, qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Measuring heartbeat</span><span class="st">`</span>, </a>
<a class="sourceLine" id="cb873-3" title="3">    qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Exercise tracking</span><span class="st">`</span>, qualtrics<span class="op">$</span><span class="st">`</span><span class="dt"> Measuring distance</span><span class="st">`</span>)</a>
<a class="sourceLine" id="cb873-4" title="4"><span class="kw">colnames</span>(rank.data) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Measuring steps&quot;</span>, <span class="st">&quot;Calories burned&quot;</span>, </a>
<a class="sourceLine" id="cb873-5" title="5">    <span class="st">&quot;Measuring heartbeat&quot;</span>, <span class="st">&quot;Exercise tracking&quot;</span>, <span class="st">&quot;Measuring distance&quot;</span>)</a></code></pre></div>
<p>First information we would like to know is how many preference combinations there are, and how repetitive they are. We can obtain that information by creating a summary of the ranking data frame we created.</p>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb874-1" title="1"><span class="kw">library</span>(pmr)</a>
<a class="sourceLine" id="cb874-2" title="2">test &lt;-<span class="st"> </span><span class="kw">rankagg</span>(rank.data)</a>
<a class="sourceLine" id="cb874-3" title="3">test</a></code></pre></div>
<pre><code>##                  n
##  [1,] 2 1 3 4 5 10
##  [2,] 1 3 2 4 5 19
##  [3,] 2 3 1 4 5 17
##  [4,] 1 2 4 3 5  4
##  [5,] 4 2 1 3 5  3
##  [6,] 3 2 1 5 4 15
##  [7,] 1 3 5 2 4 10
##  [8,] 1 2 4 5 3 10
##  [9,] 2 4 1 5 3  9
## [10,] 1 2 5 4 3  9
## [11,] 5 4 3 1 2  3
## [12,] 2 3 4 5 1  8</code></pre>
<p>The matrix we received as an output is the summary of our ranking data. It shows that, for instance, the preference combination “2,1,3,4,5” repeats 10 times in the data frame. More specifically, it means that there are 10 respondents who prefer the item 2(“Calories burned”) the most, then the item 1(“Measuring steps”), and so on.</p>
<p>Now we can calculate the mean rank for each feature and conclude which feature is the most important to our respondents:</p>
<div class="sourceCode" id="cb876"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb876-1" title="1"><span class="co"># Mean rank of each fitness tracker feature</span></a>
<a class="sourceLine" id="cb876-2" title="2"><span class="kw">destat</span>(test)<span class="op">$</span>mean.rank</a></code></pre></div>
<pre><code>## [1] 1.811966 2.581197 2.598291 4.051282 3.957265</code></pre>
<p>As we can observe from the output, the item 1(“Measuring steps”) shows the best mean rank among all items. Therefore, we can assume that the “Measuring steps” is most important for our respondents. However, in order to statistically prove it and become sure that this is not just by mere chance, we can conduct <strong>Friedman rank sum test</strong>.</p>
<p>Friedman rank sum test is used to identify whether there are any statistically significant differences between the distributions of 3 or more paired groups. It is used when the normality assumptions for using one-way repeated measures ANOVA are not met. Another case when Friedman rank rum test is used is when the dependent variable is measured on an ordinal scale, as in our case.</p>
<p>Before we conduct the Friedman rank sum test, we will visualise our data:</p>
<div class="sourceCode" id="cb878"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb878-1" title="1"><span class="co"># Preparing data frame for Friedman rank sum test</span></a>
<a class="sourceLine" id="cb878-2" title="2"><span class="kw">library</span>(reshape2)</a>
<a class="sourceLine" id="cb878-3" title="3"><span class="kw">library</span>(ggpubr)</a>
<a class="sourceLine" id="cb878-4" title="4"><span class="kw">library</span>(rstatix)</a>
<a class="sourceLine" id="cb878-5" title="5"><span class="kw">library</span>(ggstatsplot)</a>
<a class="sourceLine" id="cb878-6" title="6"></a>
<a class="sourceLine" id="cb878-7" title="7">rank.data.long &lt;-<span class="st"> </span><span class="kw">melt</span>(rank.data,<span class="dt">value.name =</span> <span class="st">&quot;Rank&quot;</span>,<span class="dt">variable.name =</span> <span class="st">&quot;Feature&quot;</span>)</a>
<a class="sourceLine" id="cb878-8" title="8"><span class="co"># We have just turned our data frame from the wide format to the long format by using function melt(). If we take a look at head and tail of our new data frame, we can see that it contains just two columns, &quot;Rank&quot; and &quot;Feature&quot;.</span></a>
<a class="sourceLine" id="cb878-9" title="9"><span class="kw">tail</span>(rank.data.long)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Feature"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["Rank"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"Measuring distance","2":"4"},{"1":"Measuring distance","2":"3"},{"1":"Measuring distance","2":"4"},{"1":"Measuring distance","2":"3"},{"1":"Measuring distance","2":"5"},{"1":"Measuring distance","2":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb879"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb879-1" title="1"><span class="kw">head</span>(rank.data.long)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Feature"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["Rank"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"Measuring steps","2":"2"},{"1":"Measuring steps","2":"1"},{"1":"Measuring steps","2":"1"},{"1":"Measuring steps","2":"2"},{"1":"Measuring steps","2":"5"},{"1":"Measuring steps","2":"4"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb880"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb880-1" title="1"><span class="co"># Quick visualisation</span></a>
<a class="sourceLine" id="cb880-2" title="2">p &lt;-<span class="st"> </span><span class="kw">ggboxplot</span>(rank.data.long, <span class="dt">x =</span> <span class="st">&quot;Feature&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Rank&quot;</span>, <span class="dt">add =</span> <span class="st">&quot;jitter&quot;</span>,<span class="dt">title =</span> <span class="st">&quot;What features are important to you when evualting fitness trackers?&quot;</span>)</a>
<a class="sourceLine" id="cb880-3" title="3"><span class="kw">ggplotly</span>(p)</a></code></pre></div>
<div id="htmlwidget-6bfb772b87292a7f4ad9" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-6bfb772b87292a7f4ad9">{"x":{"data":[{"x":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],"y":[2,1,1,2,5,4,1,2,3,1,2,2,3,2,1,1,2,1,1,1,2,2,1,1,2,5,4,1,2,3,1,2,2,1,1,1,1,2,5,4,1,2,3,1,2,2,3,2,1,1,2,1,1,1,2,3,1,2,2,3,2,1,1,2,1,1,1,2,3,1,2,2,3,2,1,1,2,1,1,1,2,3,1,2,2,3,2,1,1,2,1,1,1,2,3,1,2,2,3,2,1,1,2,1,1,1,2,3,1,2,2,3,2,1,1,2,1,4,3,2,1,4,2,2,3,2,3,3,3,2,4,3,2,1,2,2,2,3,4,3,2,1,4,2,2,3,2,3,3,3,2,2,3,2,1,4,2,2,3,2,3,3,3,2,4,3,2,1,3,3,2,3,2,3,3,3,2,4,3,2,1,3,3,2,3,2,3,3,3,2,4,3,2,1,3,3,2,3,2,3,3,3,2,4,3,2,1,3,3,2,3,2,3,3,3,2,4,3,2,1,3,3,2,3,2,3,3,3,2,4,3,2,1,3,1,5,4,3,3,1,5,1,1,2,4,1,1,1,5,4,3,4,4,5,1,1,5,4,3,3,1,5,1,1,2,4,1,4,4,5,4,3,3,1,5,1,1,2,4,1,1,1,5,4,3,2,2,5,1,1,2,4,1,1,1,5,4,3,2,2,5,1,1,2,4,1,1,1,5,4,3,2,2,5,1,1,2,4,1,1,1,5,4,3,2,2,5,1,1,2,4,1,1,1,5,4,3,2,2,5,1,1,2,4,1,1,1,5,4,3,2,5,2,5,4,1,3,4,4,5,4,5,4,5,5,2,5,4,3,3,4,4,5,2,5,4,1,3,4,4,5,4,5,4,3,3,2,5,4,1,3,4,4,5,4,5,4,5,5,2,5,4,4,4,4,4,5,4,5,4,5,5,2,5,4,4,4,4,4,5,4,5,4,5,5,2,5,4,4,4,4,4,5,4,5,4,5,5,2,5,4,4,4,4,4,5,4,5,4,5,5,2,5,4,4,4,4,4,5,4,5,4,5,5,2,5,4,4,3,4,3,5,2,5,3,5,4,5,1,5,4,3,4,3,5,5,5,3,5,3,4,3,5,2,5,3,5,4,5,1,5,5,5,4,3,5,2,5,3,5,4,5,1,5,4,3,4,3,5,5,5,3,5,4,5,1,5,4,3,4,3,5,5,5,3,5,4,5,1,5,4,3,4,3,5,5,5,3,5,4,5,1,5,4,3,4,3,5,5,5,3,5,4,5,1,5,4,3,4,3,5,5,5,3,5,4,5,1,5,4,3,4,3,5,5],"hoverinfo":"y","type":"box","fillcolor":"rgba(255,255,255,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(0,0,0,1)","width":1.88976377952756},"showlegend":false,"xaxis":"x","yaxis":"y","frame":null},{"x":[0.976530259754509,1.00094594899565,1.11628112802282,0.866797353699803,1.07162684556097,1.00273997588083,0.956147995311767,1.05488950712606,1.05710880225524,0.833552875835448,1.11173273315653,0.82030069893226,1.1444900745526,1.04960069255903,1.19318711161613,0.898542057629675,1.15441347025335,1.00262318477035,0.969448933843523,1.095318311546,0.984276702534407,1.10388032179326,0.871722332015634,1.19392172750086,0.968929864931852,1.17680831244215,0.976492625009268,0.975712794810534,0.929771881923079,0.971494795009494,1.00191831849515,0.878680299781263,1.01027735564858,0.843464046344161,0.86352124242112,0.917736751772463,0.985345979034901,0.838768521044403,0.972573239915073,0.965306181274354,0.961649287305772,0.918964268174022,1.11211551595479,0.94276320990175,1.10236607352272,1.07426131609827,0.859381050709635,0.911526871379465,0.977097717486322,1.04934287276119,1.10927181318402,1.15451023969799,0.809869436733425,1.1647608297877,1.07737670717761,0.979216550849378,0.992860591690987,1.16475817840546,1.16902759484947,0.87616532817483,0.953004604298621,1.04415892008692,0.809952278528362,1.02048261240125,0.944984182249755,1.09524259790778,1.13513023490086,0.820850799325854,1.12009456101805,1.15937644727528,1.15126200001687,0.866858321987092,1.03126736367121,1.18662115046754,1.05362495277077,0.915410454384983,0.873044949769974,1.17450169669464,0.936629833187908,1.03460789844394,0.932795654702932,0.968153930362314,0.946718669589609,0.920649222936481,0.899219526816159,1.1443792858161,1.01324300598353,1.02075085490942,1.09781574318185,1.19997824635357,1.01379838362336,0.844912124425173,1.03720735870302,1.10862094461918,1.19646935397759,0.845883621834219,0.861481008864939,0.819498876761645,1.06181426551193,0.802209640480578,0.950317558459938,0.858469741977751,1.0685332370922,0.989062301721424,1.01657785456628,0.974822805449367,1.08091145539656,1.14053809093311,0.857978531718254,1.13376290006563,0.953308135643601,1.02408481389284,0.884585988149047,0.938899569492787,1.11578224748373,1.05657002944499,0.822296723723412,1.80047055706382,1.91414315653965,2.07507273890078,2.00805583577603,1.88293748116121,2.00953992856666,2.1065968378447,2.07225322257727,2.1241330771707,1.92401297977194,1.97296180697158,2.04175736196339,2.13674219939858,1.98547838013619,1.89085398130119,1.97221194971353,1.81768128471449,1.98087142100558,2.00102921584621,2.11133707631379,2.02974348459393,1.90404647365212,2.10390043677762,2.01943153496832,1.86726181935519,1.97544032819569,2.03821277078241,1.87153016421944,2.1656217539683,2.02050063544884,1.86103283241391,2.16632957318798,1.88089694688097,1.89491342594847,1.8334741037339,1.90566441388801,1.81731621390209,2.15449365312234,1.98427461208776,2.1519989669323,1.95374866342172,2.10442320071161,2.02265339652076,2.13002916062251,2.15769912274554,2.07242010170594,2.10799025064334,2.00577721549198,2.17271401295438,1.92217068420723,1.86393903223798,1.82121365647763,1.89401460047811,1.94825451001525,1.95364627018571,1.99344732994214,2.11711404239759,1.97241931958124,1.91055919677019,2.00610815389082,1.97204928901047,2.18031973727047,1.83013294246048,1.89640135811642,2.0255582340993,2.04593056645244,1.9959998479113,1.86865445664153,1.8719016412273,1.8950523163192,2.1328685566783,2.11216512965038,1.98289154907689,2.19275605687872,1.8510023557581,1.89592727711424,2.18607723414898,2.0114912928082,1.93188015203923,1.86165659120306,1.91038881670684,1.95970277218148,1.88875724654645,2.01864600451663,1.83578269733116,1.92954314816743,1.83595763081685,2.18840984823182,2.04102657781914,1.80718979099765,2.14357131607831,1.91034588078037,1.94448622446507,2.11926916763186,2.12452993998304,1.99706853022799,1.89678314216435,1.8560865608044,1.95868986947462,1.96635273555294,1.80081204855815,1.89581308122724,1.94099901048467,1.93031432516873,1.8135683581233,2.18344187512994,2.09603672912344,1.98544987430796,1.88348695319146,1.83417504820973,1.9622288165614,2.02521673524752,2.05393332811072,1.91807078002021,1.87235189620405,1.87142246020958,1.84015181707218,2.95057162865996,2.8915776017122,3.13482574522495,3.02496143728495,2.80755504379049,2.84407312702388,2.96630778452381,2.84007232701406,2.89186511747539,2.87091915719211,2.82385892039165,3.14335218733177,2.96842266330495,3.04330385420471,3.05145070552826,3.06376568628475,2.89867687355727,3.00317896809429,3.06442845566198,2.93361555794254,2.93444957537577,2.90313626155257,2.80831372831017,2.91045401655138,3.12207035217434,3.08559126025066,3.07043312899768,2.98556282836944,2.88282264396548,2.95013616448268,3.08040074026212,3.01452308846638,2.89509882787243,2.82651319587603,2.82052193358541,3.02938634855673,3.17379425633699,3.17513186596334,2.96814288422465,3.08249499686062,2.87965726358816,3.01486056093127,2.85292545165867,2.96595698380843,3.03056515073404,2.96238689934835,2.89790579834953,3.03187466384843,3.02195984972641,2.86557330451906,3.04716441938654,3.14284738423303,2.93426475971937,3.15495909359306,3.02916484382004,2.92507376056165,2.88839722564444,2.94963990673423,3.12516130143777,3.0780210277997,3.01870096260682,3.139958228264,3.0182304427959,3.10802698880434,2.8853490581736,3.01970588071272,2.97413159916177,2.89797781091183,3.18706980571151,3.04800050640479,3.06031593810767,3.16084599895403,3.11094360696152,2.90407030889764,3.11475278781727,3.04751184936613,2.86997447377071,2.88163272449747,2.8980030125007,3.19418129185215,2.80425966558978,3.15949368923903,2.97515141526237,2.83762047179043,2.87285812329501,2.8049050158821,2.81648825760931,3.03393710311502,3.19757215781137,3.10847079809755,3.16048389999196,2.8240616293624,3.16828701710328,2.81912519307807,2.96773382928222,3.04066972723231,3.0537744436413,3.15809079557657,3.19820256615058,2.97178074037656,3.02981364410371,2.99983123764396,2.83971141474321,3.09682415770367,3.12509981412441,2.85129321590066,2.8070074592717,3.02633999567479,2.92444872921333,2.90322908880189,2.81579349134117,2.86143711945042,2.92653609011322,3.12878323886544,2.84577754680067,2.80151914749295,2.95643704384565,4.15050657680258,4.02770365504548,4.16020375154912,3.9431920375675,4.12571588726714,3.90962705453858,3.96920035360381,4.03043090812862,4.13485270710662,3.94165629651397,4.13886377355084,3.85200986284763,3.89689659550786,4.08706343099475,4.1664084084332,3.94706128016114,3.84640955664217,3.85828752731904,3.99428376220167,3.8267971875146,4.14390664128587,4.02453966643661,4.05908849248663,4.13744971249253,4.17958361003548,4.06664038822055,4.16115087671205,4.07957259807736,3.89854853916913,4.17516840882599,3.91036588093266,3.80985898571089,4.09017162388191,3.98511171666905,4.08261625384912,4.14555563107133,4.0233386005275,4.13270424548537,4.16456345496699,4.15115706203505,4.02617893647403,3.94184752227738,4.0748940189369,3.96887559676543,3.82347003826872,3.9258857652545,4.14774384275079,3.87840656936169,4.16621959069744,3.88623537681997,4.08536208765581,4.04953493298963,4.13501642905176,3.98427465036511,3.98998962948099,4.01831864695996,4.08373269988224,3.90094852140173,3.94001399958506,3.92175899874419,3.88211323376745,3.88393807727844,4.05014823116362,4.14955506175756,3.80471569988877,4.07529743779451,3.97797679640353,3.98948709424585,3.8562676621601,4.0801903385669,4.06904062349349,4.05255151763558,4.00673843482509,4.05061533683911,4.13530205171555,3.82057694373652,4.19814853128046,3.98918334310874,3.97875813618302,4.12384084882215,3.85073677459732,4.11187108410522,4.02807840956375,3.95970969805494,3.82326878234744,3.87414954071865,4.18466992015019,4.16223110407591,4.06861392213032,4.11216923333704,3.98065643506125,4.08785935956985,3.86716838842258,3.85638150088489,3.98557298704982,4.01195946428925,3.85147188324481,3.90677512316033,3.99729037405923,4.13837600769475,4.02744547007605,3.95147535940632,3.84813439175487,3.95376991275698,4.18472720412537,4.07383799552917,3.97687459886074,4.14649236360565,4.13991194488481,3.90232993094251,4.17250755270943,4.04649787265807,3.83166680941358,4.18964885687456,4.17454356821254,3.8503439610824,3.99229926299304,5.01504717450589,4.90774514926597,4.86337856575847,5.1265025774017,5.11246984591708,5.15164753990248,5.11664149397984,4.97367300223559,4.82245361804962,5.03425635686144,4.98542407592759,5.01658012792468,5.18469798555598,5.06960675185546,5.01664073159918,4.91245600227267,4.82735783737153,4.91323411781341,4.92226745011285,4.94368326477706,4.96473060147837,4.95644210428,4.9561926914379,5.07292009340599,4.8773534652777,4.97144159395248,5.10743192117661,4.95251780925319,5.02300998140126,5.02010119846091,5.10600612275302,4.9505633354187,4.84303213786334,5.15268881795928,4.93780552074313,4.87547047613189,4.8520478464663,5.18990939995274,5.05475526973605,4.87163759330288,4.99975950429216,5.07708611870185,4.88507460448891,4.93865972850472,4.80863573877141,5.13754107709974,4.94665073370561,4.8464431331493,4.98502219412476,4.99944301294163,5.18115676501766,4.91164606940001,4.94042941434309,5.1293349747546,5.07005896409973,5.12373011484742,4.94950637649745,4.87146937744692,5.15788194956258,4.92732635801658,5.11374086765572,4.91695554004982,4.92517078891396,4.80335642211139,5.07820323025808,5.18413239354268,4.91159650599584,4.96912246774882,5.13964067203924,5.01546248178929,4.97118595149368,4.9450979359448,5.19587946133688,5.08319509886205,5.19108472391963,4.85423206668347,5.00114304134622,5.08510149279609,5.18109925519675,5.19445582861081,5.10078138327226,5.15781056825072,5.01280339835212,4.89406607486308,5.01451349919662,4.97163647497073,4.87275558821857,4.96201082132757,5.15018790205941,4.86648016674444,4.89052898082882,5.02695895750076,5.14727472811937,4.97714433064684,5.10689392089844,4.90570235094056,5.13230133326724,5.03061860175803,4.98185616349801,4.87447634693235,5.02633617985994,5.00483741872013,4.91101099764928,4.80292224017903,5.12555698342621,5.02099565984681,5.09911334225908,5.19743778416887,4.85312857795507,5.02921817777678,5.16207746751606,4.86204695953056,4.85909188063815,4.95957350283861,5.13897715266794,5.06467070821673,5.01806475315243],"y":[1.64874309748411,1.26705790329725,0.790456586331129,2.08672456108034,4.6714328981936,3.93417762368917,0.730246517248452,2.39476990159601,3.19215563256294,1.27922516539693,1.81774585600942,1.88449896462262,2.83783790096641,2.37888580244035,1.1447476901114,1.18202504981309,1.72950376048684,1.09866254590452,1.31941580045968,1.37491388898343,2.05167098734528,1.82171001769602,1.10994264539331,1.38723500166088,2.27463826108724,5.06581704877317,3.69588580653071,0.81377905793488,2.17698744535446,3.35873734876513,0.993701216205955,2.21088736448437,1.87057554386556,0.978935432620347,0.620270445197821,0.636343186162412,1.23563265409321,1.8183291785419,5.21062234882265,4.3633336879313,1.10703188162297,2.01452037598938,2.9535823315382,1.10038897227496,2.38450856078416,2.37263730186969,2.69280562046915,1.72608978506178,1.38155377209187,1.0935803277418,1.67674194127321,1.29571016319096,1.06905408762395,1.04047920256853,1.62306899428368,3.3833072276786,0.839533664099872,1.90001512803137,1.82643286343664,2.81975681651384,1.91782742086798,0.877165987156332,1.1751047750935,1.62408138178289,1.25047351177782,0.687213706225157,0.711710348725319,2.16357659455389,3.32177269533277,1.2870373243466,1.71146434508264,2.38358430750668,3.35103052631021,1.61031464841217,0.768853033334017,0.744873021915555,1.89519383627921,1.36593251600862,0.721027957089245,1.0686880543828,2.14663953911513,3.0169569125399,0.702320438809693,1.90793007854372,1.82037659287453,3.28943040836602,1.88896938227117,1.08274041078985,1.14093360099941,1.89006267972291,1.10638263393193,0.803858575969934,0.932969200052321,1.8395535396412,2.97129841651768,0.994513619877398,1.79894878566265,1.92207359727472,3.06226422935724,2.15596232172102,0.679988881386816,1.36682270038873,2.19541830606759,0.834920629672706,1.37367308661342,0.915949618816376,1.73199810497463,2.97715289704502,0.916940763033926,1.991698243469,2.3656527493149,3.25433964133263,2.10094775222242,0.930100842192769,1.00124308746308,1.60014606472105,1.38378424104303,3.90639768037945,2.96057543344796,1.82668333463371,0.71281772647053,4.26985140759498,1.64037774540484,2.19435429275036,2.87835977133363,1.91988404579461,2.82837515920401,3.37976661492139,2.71732075065374,2.17173954062164,4.35799965430051,3.34977002963424,2.39036331381649,1.36125959157944,2.20332358013839,1.65997123010457,1.85340308099985,3.10998796690255,3.87174958735704,3.19678679220378,1.77692601829767,0.694361405447125,3.76203743927181,2.28123722821474,2.35631900746375,2.77878666967154,1.73819825164974,3.34001867622137,2.85148543342948,2.79377124514431,1.79793680757284,2.28278181068599,3.23013841155916,2.313903532736,1.12668589800596,4.09153018891811,1.69372837748379,1.76213656011969,3.21912418231368,2.26390359867364,2.82490188907832,3.04950836282223,2.67152372077107,2.0784026414156,3.71343458965421,3.24517948012799,2.2987294876948,0.867339958995581,3.32051796223968,3.11556409541517,2.26884385030717,2.81037210356444,1.71940386537462,3.16741212699562,3.01177170649171,2.71375466622412,1.99106477703899,3.98558827210218,3.27791071999818,1.92590549644083,0.913378613814712,3.28804978430271,2.80763334687799,1.95461968760937,2.93308973778039,1.72291330881417,3.27504289019853,3.0533492565155,3.20844655223191,2.09745200797915,3.93846534751356,3.32574765589088,1.93747608233243,1.34920525643975,2.96694346778095,2.69398029372096,1.79186735674739,2.98072113618255,2.24079609680921,2.93617932181805,2.84123798683286,3.21674404088408,1.96376219131052,3.78546902220696,3.04656136594713,2.00588626172394,1.26720619108528,2.99399823099375,3.29079584870487,1.6345731196925,2.88698247745633,1.74845475237817,2.74865734111518,2.91445736978203,2.67347703464329,2.23571752700955,4.01242510583252,3.39498173370957,2.24487161338329,1.2916444785893,2.61228686850518,3.35305705443025,2.18878986872733,2.97923210766166,2.17605479676276,2.75066856127232,3.25066338758916,2.96899799648672,1.77562815435231,4.31576785203069,3.00667826030403,2.29370114468038,0.600518204830587,3.24954585246742,0.648768451809883,4.79049298763275,3.66792006250471,2.79772407524288,2.61626452319324,1.22716010641307,4.66562404371798,0.793672668002546,0.754097786732018,2.0540950300172,4.33787077348679,0.655606301315129,0.867021141573787,1.32381147444248,5.24800015799701,3.69047032799572,3.2858854258433,3.63476585689932,3.92309280894697,5.20760488826781,1.06213511787355,1.01203903742135,4.67191188894212,4.22602811362594,3.36388640422374,3.05962055977434,0.952468070760369,5.27402859758586,0.7455706236884,0.785390342026949,2.14437028169632,4.13874367028475,1.30260761082172,3.61600658409297,3.89737108703703,4.82383079025894,3.73027393948287,3.03547035325319,3.31987270694226,1.37128905653954,4.62514290772378,0.604776155948639,0.826927648112178,2.17427653279155,4.38147420808673,1.13309369720519,0.670074710436165,1.05721781272441,5.30406610723585,4.16799075193703,3.09736463744193,2.30780500210822,2.11358555071056,4.92963816709816,0.964986477419734,0.809980252198875,2.12033813614398,4.00121202357113,1.15291328597814,1.02726695481688,0.932008213736117,4.81457511689514,3.95158861763775,3.35653902776539,1.7398044642061,2.24346924163401,5.05283218286931,0.761951572261751,0.648851219005883,1.87924523018301,3.98201113902032,0.920196011662483,0.906197758018971,0.987961985357106,4.76356473769993,4.24590966962278,3.17569179795682,2.10051135569811,2.39842327143997,5.34147178456187,1.24713774416596,1.07696899343282,1.99865099191666,4.32457337863743,0.737671328522265,1.21299544535577,0.973663412593305,4.64571898411959,4.36497965697199,2.91983280908316,2.14862006325275,1.61347527466714,4.75647383984178,1.01157052386552,1.31872111614794,2.14211873449385,3.69969438947737,1.00262937452644,0.885547053068876,1.30128616262227,4.75226107407361,3.72028921917081,2.6270301386714,2.03856723029166,2.27478245440871,4.71362105645239,0.747295978851616,1.04151571523398,1.69617808833718,4.10264074448496,0.767648949660361,1.38425662741065,0.719133603386581,4.79378685001284,4.3687880480662,2.94110811166465,2.03563968259841,4.70820647459477,2.2003586538136,5.29915253091603,3.64950228147209,1.3815092517063,3.17147574406117,3.64648418314755,3.86456071119756,4.93494338653982,4.35144177675247,5.10970135536045,4.2063893115148,4.87004145570099,5.33005786221474,1.97672592084855,4.86394237075001,3.81019583940506,2.73066949788481,2.9657294748351,3.84020130317658,3.77360516004264,4.96184412427247,1.87738290578127,4.82414841391146,3.60652185287327,0.685676673427224,2.64219163116068,4.1406875917688,4.37318689115345,4.75922130942345,4.11114387400448,4.7275958808139,4.022494594194,2.64434758778661,2.92057480923831,1.60240468382835,4.98611977752298,3.96621041186154,0.634792193956673,3.3948558786884,4.16750555317849,4.25832605101168,4.96161381397396,3.93002806808799,5.25815546661615,3.90060744397342,4.63683806266636,4.6725653802976,1.65300303716213,5.25345126073807,3.66686554402113,4.24354997482151,4.07338887620717,4.00836309436709,3.82702233474702,4.67138126604259,3.82458835765719,5.35720717553049,3.84158930443227,4.62163558434695,4.93870972953737,1.7227978091687,4.90889752339572,3.81871667262167,3.74610085021704,3.66022662781179,3.9319480702281,3.73103934638202,5.39819803796709,3.7196580817923,5.11824826542288,3.64738465771079,4.65159070882946,5.252727490291,2.24614147730172,4.6013289809227,3.68896971736103,3.73782205041498,3.73214044980705,3.84456418566406,3.74045920595527,4.74047116804868,4.30336157660931,5.31693820301443,3.70469695087522,5.37457788456231,5.32476986609399,2.1067038860172,5.14306630417705,3.98383312094957,3.78992458432913,3.73884539231658,4.39288404751569,3.64272723458707,4.69804112873971,4.2769822485745,4.70305719915777,4.11283348016441,4.82849613968283,4.6519505225122,2.28127517793328,4.65149530805647,4.25866962783039,3.81681262236089,3.73535008635372,4.29968871530145,3.75284722149372,4.84298001527786,3.82721799463034,4.87348617315292,3.7988583765924,4.82183902300894,5.16303544845432,2.370068359375,5.31633355543017,3.98399817980826,4.26396961417049,2.84928707070649,3.94122282434255,2.75505600776523,4.68702506050468,1.83539146948606,5.03821207359433,2.96397608891129,4.92279077339917,4.30822291318327,5.23600391484797,1.24906767942011,5.06995458435267,3.88418685793877,3.08935882672667,4.20567538365722,3.18548600897193,5.33281028270721,5.06345525700599,5.34355601388961,2.93697835113853,5.3971519337967,2.71614099051803,3.96054092012346,3.20495013818145,5.28728873785585,2.38117971960455,4.75825049299747,2.66774837020785,5.05895173605531,3.62400693129748,5.35234979409725,1.1410262381658,5.18268307112157,5.32578121926636,4.87915131207555,4.20584205221385,2.65135946497321,5.19457077328116,2.38147829324007,5.00605540499091,3.07276790216565,5.04327449146658,3.861309617199,4.85776618234813,0.984739013575017,5.09131608251482,3.98295379225165,2.82100889701396,4.21055371463299,2.97669929489493,5.20006632097065,4.99714418873191,5.33635565210134,2.72169472649693,4.63915884774178,4.03501813411713,5.24901417251676,0.869854065589607,5.3875566419214,3.83668734002858,2.79203542973846,4.30338167455047,3.10522852484137,5.1670531636104,5.12461259569973,4.62864766828716,3.04468090776354,5.01472746990621,4.36783835794777,5.23539729695767,1.39518849123269,5.25634948629886,3.96333906333894,3.11632500849664,4.0492250226438,2.88545568231493,4.66625143084675,5.03344468642026,4.63654191847891,3.06741343960166,4.78724174629897,3.63512109555304,5.36684495359659,1.32443143352866,5.24547959398478,4.28312402032316,2.95687071997672,3.89581756759435,3.24722383990884,4.90938559509814,5.39606581311673,4.83803689423948,2.66713481713086,4.68436017204076,4.04749263916165,5.05228386428207,0.708574990369379,5.39194996077567,3.75478747151792,3.15546019487083,4.33478494938463,2.81239222176373,4.85577983688563,5.28629707638174,5.02332812901586,3.38158785365522,5.22191741522402,3.91930179223418,5.34026888683438,1.05740613937378,4.75142210330814,4.30597960818559,3.21536703128368,4.11488955505192,2.95529387164861,5.25869306717068,4.96961391232908],"text":["Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 5","Feature: Measuring steps<br />Rank: 4","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 5","Feature: Measuring steps<br />Rank: 4","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 5","Feature: Measuring steps<br />Rank: 4","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 3","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 1","Feature: Measuring steps<br />Rank: 2","Feature: Measuring steps<br />Rank: 1","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 4","Feature: Calories burned<br />Rank: 3","Feature: Calories burned<br />Rank: 2","Feature: Calories burned<br />Rank: 1","Feature: Calories burned<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 2","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 1","Feature: Measuring heartbeat<br />Rank: 5","Feature: Measuring heartbeat<br />Rank: 4","Feature: Measuring heartbeat<br />Rank: 3","Feature: Measuring heartbeat<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 1","Feature: Exercise tracking<br />Rank: 3","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 3","Feature: Exercise tracking<br />Rank: 3","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 1","Feature: Exercise tracking<br />Rank: 3","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 3","Feature: Exercise tracking<br />Rank: 3","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 1","Feature: Exercise tracking<br />Rank: 3","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 2","Feature: Exercise tracking<br />Rank: 5","Feature: Exercise tracking<br />Rank: 4","Feature: Exercise tracking<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 2","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 2","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 2","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 1","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 4","Feature: Measuring distance<br />Rank: 3","Feature: Measuring distance<br />Rank: 5","Feature: Measuring distance<br />Rank: 5"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":46.2864259028643,"r":7.97011207970112,"b":47.0236612702366,"l":35.865504358655},"plot_bgcolor":"rgba(255,255,255,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":15.9402241594022},"title":{"text":"What features are important to you when evualting fitness trackers?","font":{"color":"rgba(0,0,0,1)","family":"","size":19.1282689912827},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.4,5.6],"tickmode":"array","ticktext":["Measuring steps","Calories burned","Measuring heartbeat","Exercise tracking","Measuring distance"],"tickvals":[1,2,3,4,5],"categoryorder":"array","categoryarray":["Measuring steps","Calories burned","Measuring heartbeat","Exercise tracking","Measuring distance"],"nticks":null,"ticks":"outside","tickcolor":"rgba(0,0,0,1)","ticklen":3.98505603985056,"tickwidth":0.724555643609193,"showticklabels":true,"tickfont":{"color":"rgba(0,0,0,1)","family":"","size":15.9402241594022},"tickangle":-0,"showline":true,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"y","title":{"text":"Feature","font":{"color":"rgba(0,0,0,1)","family":"","size":15.9402241594022}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.360634213173762,5.63808202962391],"tickmode":"array","ticktext":["1","2","3","4","5"],"tickvals":[1,2,3,4,5],"categoryorder":"array","categoryarray":["1","2","3","4","5"],"nticks":null,"ticks":"outside","tickcolor":"rgba(0,0,0,1)","ticklen":3.98505603985056,"tickwidth":0.724555643609193,"showticklabels":true,"tickfont":{"color":"rgba(0,0,0,1)","family":"","size":15.9402241594022},"tickangle":-0,"showline":true,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"x","title":{"text":"Rank","font":{"color":"rgba(0,0,0,1)","family":"","size":15.9402241594022}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":2.06156048675734,"font":{"color":"rgba(0,0,0,1)","family":"","size":12.7521793275218}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"1748798a78bd":{"x":{},"y":{},"type":"box"},"17485d273869":{"x":{},"y":{}}},"cur_data":"1748798a78bd","visdat":{"1748798a78bd":["function (y) ","x"],"17485d273869":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb881"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb881-1" title="1"><span class="co"># Advanced visualisation</span></a>
<a class="sourceLine" id="cb881-2" title="2">ggstatsplot<span class="op">::</span><span class="kw">ggwithinstats</span>(</a>
<a class="sourceLine" id="cb881-3" title="3">  <span class="dt">data =</span> rank.data.long,</a>
<a class="sourceLine" id="cb881-4" title="4">  <span class="dt">x =</span> Feature,</a>
<a class="sourceLine" id="cb881-5" title="5">  <span class="dt">y =</span> Rank,</a>
<a class="sourceLine" id="cb881-6" title="6">  <span class="dt">type =</span> <span class="st">&quot;np&quot;</span>,</a>
<a class="sourceLine" id="cb881-7" title="7">  <span class="dt">pairwise.comparisons =</span> <span class="ot">TRUE</span>, <span class="co"># show pairwise comparison test results</span></a>
<a class="sourceLine" id="cb881-8" title="8">  <span class="dt">title =</span> <span class="st">&quot;What features are important to you when evualting fitness trackers?&quot;</span>)</a></code></pre></div>
<pre><code>## Note: 95% CI for effect size estimate was computed with 100 bootstrap samples.</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-540-2.png" width="672" /></p>
<p>Already from the advanced visualisation, that includes Friedman rank sum test and pairwise comparison, we can have an insight in significance of differences among features.</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb883-1" title="1"><span class="co"># Friedman test</span></a>
<a class="sourceLine" id="cb883-2" title="2"><span class="kw">friedman.test</span>(<span class="kw">as.matrix</span>(rank.data))</a></code></pre></div>
<pre><code>## 
##  Friedman rank sum test
## 
## data:  as.matrix(rank.data)
## Friedman chi-squared = 176.42, df = 4, p-value &lt; 0.00000000000000022</code></pre>
<p>Friedman rank sum test has a p-value lower than 0.05, so we can conclude that here are significant differences between at least two features (what we have already seen in our visualisation). Even though we have identified differences between preferences towards features in our advanced visualisation, we will conduct a post hoc test in order to demonstrate traditional way of calculating pairwise comparisons.</p>
<div class="sourceCode" id="cb885"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb885-1" title="1"><span class="kw">wilcox_test</span>(Rank <span class="op">~</span><span class="st"> </span>Feature, <span class="dt">paired =</span> <span class="ot">TRUE</span>, <span class="dt">p.adjust.method =</span> <span class="st">&quot;bonferroni&quot;</span>, </a>
<a class="sourceLine" id="cb885-2" title="2">    <span class="dt">data =</span> rank.data.long)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[".y."],"name":[1],"type":["chr"],"align":["left"]},{"label":["group1"],"name":[2],"type":["chr"],"align":["left"]},{"label":["group2"],"name":[3],"type":["chr"],"align":["left"]},{"label":["n1"],"name":[4],"type":["int"],"align":["right"]},{"label":["n2"],"name":[5],"type":["int"],"align":["right"]},{"label":["statistic"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["p"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["p.adj"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["p.adj.signif"],"name":[9],"type":["chr"],"align":["left"]}],"data":[{"1":"Rank","2":"Measuring steps","3":"Calories burned","4":"117","5":"117","6":"1369.0","7":"0.0000000036800000000","8":"0.000000036800000000","9":"****"},{"1":"Rank","2":"Measuring steps","3":"Measuring heartbeat","4":"117","5":"117","6":"2231.0","7":"0.0007530000000000000","8":"0.008000000000000000","9":"**"},{"1":"Rank","2":"Measuring steps","3":"Exercise tracking","4":"117","5":"117","6":"354.0","7":"0.0000000000000000122","8":"0.000000000000000122","9":"****"},{"1":"Rank","2":"Measuring steps","3":"Measuring distance","4":"117","5":"117","6":"367.5","7":"0.0000000000000000247","8":"0.000000000000000247","9":"****"},{"1":"Rank","2":"Calories burned","3":"Measuring heartbeat","4":"117","5":"117","6":"3214.5","7":"0.5120000000000000107","8":"1.000000000000000000","9":"ns"},{"1":"Rank","2":"Calories burned","3":"Exercise tracking","4":"117","5":"117","6":"610.5","7":"0.0000000000000025900","8":"0.000000000000025900","9":"****"},{"1":"Rank","2":"Calories burned","3":"Measuring distance","4":"117","5":"117","6":"940.0","7":"0.0000000000027500000","8":"0.000000000027500000","9":"****"},{"1":"Rank","2":"Measuring heartbeat","3":"Exercise tracking","4":"117","5":"117","6":"1235.0","7":"0.0000000011800000000","8":"0.000000011800000000","9":"****"},{"1":"Rank","2":"Measuring heartbeat","3":"Measuring distance","4":"117","5":"117","6":"1307.5","7":"0.0000000039500000000","8":"0.000000039500000000","9":"****"},{"1":"Rank","2":"Exercise tracking","3":"Measuring distance","4":"117","5":"117","6":"3534.5","7":"0.8159999999999999476","8":"1.000000000000000000","9":"ns"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The output table provides us with p-values referring to significance of difference in mean ranks of each pair. For instance, the first 4 rows proves that the differences between the mean rank of the feature “Measuring steps” and each of the rest of features are significant. Consequently, we can conclude that this feature is by far the most important among our respondents.</p>
<p>Another question that may be interesting to explore is whether there are any complementary feautres ? Or features which overlap each other in its funcionality? In order to have a look at that, we can investigate the correlation between ranks assigned to each feature.</p>
<div class="sourceCode" id="cb886"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb886-1" title="1"><span class="co"># Correlation Matrix</span></a>
<a class="sourceLine" id="cb886-2" title="2">cor.matrix &lt;-<span class="st"> </span><span class="kw">cor</span>(rank.data, <span class="dt">method =</span> <span class="kw">c</span>(<span class="st">&quot;spearman&quot;</span>))</a>
<a class="sourceLine" id="cb886-3" title="3">cor.matrix</a></code></pre></div>
<pre><code>##                     Measuring steps Calories burned Measuring heartbeat
## Measuring steps          1.00000000     -0.04651331          -0.6569094
## Calories burned         -0.04651331      1.00000000          -0.2221626
## Measuring heartbeat     -0.65690943     -0.22216264           1.0000000
## Exercise tracking        0.29633223     -0.10838758          -0.3255840
## Measuring distance      -0.05958032     -0.11694481          -0.3817895
##                     Exercise tracking Measuring distance
## Measuring steps             0.2963322        -0.05958032
## Calories burned            -0.1083876        -0.11694481
## Measuring heartbeat        -0.3255840        -0.38178948
## Exercise tracking           1.0000000        -0.47176821
## Measuring distance         -0.4717682         1.00000000</code></pre>
<p>At the first glance we can observe a lot of negative values, meaning that many features correlate negatively relative to each other. In order to make the interpretation easier, we will try to visualise correlations in a form of a correlation matrix.</p>
<div class="sourceCode" id="cb888"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb888-1" title="1"><span class="kw">library</span>(corrplot)</a>
<a class="sourceLine" id="cb888-2" title="2"><span class="kw">corrplot</span>(cor.matrix, <span class="dt">method =</span> <span class="st">&quot;color&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;upper&quot;</span>, </a>
<a class="sourceLine" id="cb888-3" title="3">    <span class="dt">order =</span> <span class="st">&quot;hclust&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-544-1.png" width="672" /></p>
<p>From the correlation matrix we can confirm that almost all features negatively correlate to each other. An exception is the relationship between feature “Measuring steps” and “Exercise tracking”, which correlates positvely. This matrix can be useful for digging deeper in relationship between preferences for features. For instance, we can assume that feature “Measuring steps” and “Exercise tracking” correlate positively because users see them as complementary features. Moreover, if we say that walking is a type of exercise (in case of longer walking routes), we can assume that users, who ranked “Exercise tracking” high, ranked “Measuring steps” high as well, because they perceive it as another type of “Exercise tracking”.</p>
</div>
<div id="constant-sum-question" class="section level3">
<h3><span class="header-section-number">10.3.4</span> Constant Sum question</h3>
<p><img src="constant-sum-question.png" width="72%" style="display: block; margin: auto;" /></p>
<p>If you wish to obtain information about how much one attribute is preferred over another one, you may use a constant sum scale. The total box should always be displayed at the bottom to make it easier for respondents.A constant sum question permits collection of ratio data type. With data obtained we would be able to express the relative importance of the options.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-546">Table 10.2: </span>Constant Sum Question
</caption>
<thead>
<tr>
<th style="text-align:right;">
Location
</th>
<th style="text-align:right;">
Price
</th>
<th style="text-align:right;">
Ambience
</th>
<th style="text-align:right;">
Customer Service
</th>
<th style="text-align:right;">
id
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
6
</td>
</tr>
</tbody>
</table>
<div id="data-visualisation" class="section level4">
<h4><span class="header-section-number">10.3.4.1</span> Data visualisation</h4>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb889-1" title="1"><span class="co"># Compute descriptive statistics</span></a>
<a class="sourceLine" id="cb889-2" title="2"><span class="kw">library</span>(pastecs)</a>
<a class="sourceLine" id="cb889-3" title="3">res &lt;-<span class="st"> </span><span class="kw">stat.desc</span>(constant.sum)</a>
<a class="sourceLine" id="cb889-4" title="4"><span class="kw">round</span>(res[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dv">2</span>)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[" Location"],"name":[1],"type":["dbl"],"align":["right"]},{"label":[" Price"],"name":[2],"type":["dbl"],"align":["right"]},{"label":[" Ambience"],"name":[3],"type":["dbl"],"align":["right"]},{"label":[" Customer Service"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"117.00","2":"117.00","3":"117.00","4":"117.00"},{"1":"41.00","2":"2.00","3":"6.00","4":"9.00"},{"1":"0.00","2":"0.00","3":"0.00","4":"0.00"},{"1":"0.00","2":"0.00","3":"0.00","4":"0.00"},{"1":"40.00","2":"100.00","3":"60.00","4":"100.00"},{"1":"40.00","2":"100.00","3":"60.00","4":"100.00"},{"1":"1421.00","2":"3684.00","3":"3014.00","4":"3581.00"},{"1":"10.00","2":"30.00","3":"20.00","4":"30.00"},{"1":"12.15","2":"31.49","3":"25.76","4":"30.61"},{"1":"1.00","2":"1.50","3":"1.29","4":"1.48"},{"1":"1.99","2":"2.97","3":"2.56","4":"2.93"},{"1":"117.90","2":"263.11","3":"195.39","4":"255.34"},{"1":"10.86","2":"16.22","3":"13.98","4":"15.98"},{"1":"0.89","2":"0.52","3":"0.54","4":"0.52"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb890"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb890-1" title="1"><span class="co"># Creation of the long version of data frame</span></a>
<a class="sourceLine" id="cb890-2" title="2">constant.sum.long &lt;-<span class="st"> </span><span class="kw">melt</span>(constant.sum, <span class="dt">variable.name =</span> <span class="st">&quot;Factor&quot;</span>, </a>
<a class="sourceLine" id="cb890-3" title="3">    <span class="dt">value.name =</span> <span class="st">&quot;Points&quot;</span>)</a>
<a class="sourceLine" id="cb890-4" title="4">constant.sum.long</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Factor"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["Points"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"Location","2":"32"},{"1":"Location","2":"25"},{"1":"Location","2":"19"},{"1":"Location","2":"20"},{"1":"Location","2":"30"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"30"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"10"},{"1":"Location","2":"10"},{"1":"Location","2":"10"},{"1":"Location","2":"30"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"40"},{"1":"Location","2":"10"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"10"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"30"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"10"},{"1":"Location","2":"30"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"30"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"30"},{"1":"Location","2":"15"},{"1":"Location","2":"0"},{"1":"Location","2":"30"},{"1":"Location","2":"20"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"30"},{"1":"Location","2":"10"},{"1":"Location","2":"0"},{"1":"Location","2":"40"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"10"},{"1":"Location","2":"20"},{"1":"Location","2":"0"},{"1":"Location","2":"20"},{"1":"Location","2":"30"},{"1":"Location","2":"10"},{"1":"Price","2":"23"},{"1":"Price","2":"30"},{"1":"Price","2":"21"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"40"},{"1":"Price","2":"40"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"40"},{"1":"Price","2":"40"},{"1":"Price","2":"30"},{"1":"Price","2":"40"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"10"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"30"},{"1":"Price","2":"60"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"80"},{"1":"Price","2":"40"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"50"},{"1":"Price","2":"60"},{"1":"Price","2":"30"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"50"},{"1":"Price","2":"30"},{"1":"Price","2":"0"},{"1":"Price","2":"40"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"50"},{"1":"Price","2":"40"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"50"},{"1":"Price","2":"30"},{"1":"Price","2":"100"},{"1":"Price","2":"30"},{"1":"Price","2":"50"},{"1":"Price","2":"30"},{"1":"Price","2":"60"},{"1":"Price","2":"60"},{"1":"Price","2":"40"},{"1":"Price","2":"50"},{"1":"Price","2":"50"},{"1":"Price","2":"10"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"50"},{"1":"Price","2":"30"},{"1":"Price","2":"25"},{"1":"Price","2":"30"},{"1":"Price","2":"10"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"60"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"10"},{"1":"Price","2":"40"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"40"},{"1":"Price","2":"50"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"50"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"30"},{"1":"Price","2":"25"},{"1":"Price","2":"30"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"100"},{"1":"Price","2":"20"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"50"},{"1":"Price","2":"30"},{"1":"Price","2":"40"},{"1":"Price","2":"10"},{"1":"Price","2":"40"},{"1":"Price","2":"30"},{"1":"Price","2":"50"},{"1":"Price","2":"20"},{"1":"Price","2":"30"},{"1":"Price","2":"30"},{"1":"Price","2":"20"},{"1":"Price","2":"0"},{"1":"Price","2":"30"},{"1":"Price","2":"40"},{"1":"Price","2":"10"},{"1":"Ambience","2":"32"},{"1":"Ambience","2":"22"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"0"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"60"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"60"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"0"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"0"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"60"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"0"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"0"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"10"},{"1":"Ambience","2":"50"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"40"},{"1":"Ambience","2":"0"},{"1":"Ambience","2":"30"},{"1":"Ambience","2":"20"},{"1":"Ambience","2":"50"},{"1":"Customer Service","2":"13"},{"1":"Customer Service","2":"23"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"60"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"10"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"10"},{"1":"Customer Service","2":"10"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"60"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"60"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"35"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"60"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"50"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"10"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"10"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"0"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"30"},{"1":"Customer Service","2":"40"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"100"},{"1":"Customer Service","2":"20"},{"1":"Customer Service","2":"10"},{"1":"Customer Service","2":"30"},{"1":"id","2":"1"},{"1":"id","2":"2"},{"1":"id","2":"3"},{"1":"id","2":"4"},{"1":"id","2":"5"},{"1":"id","2":"6"},{"1":"id","2":"7"},{"1":"id","2":"8"},{"1":"id","2":"9"},{"1":"id","2":"10"},{"1":"id","2":"11"},{"1":"id","2":"12"},{"1":"id","2":"13"},{"1":"id","2":"14"},{"1":"id","2":"15"},{"1":"id","2":"16"},{"1":"id","2":"17"},{"1":"id","2":"18"},{"1":"id","2":"19"},{"1":"id","2":"20"},{"1":"id","2":"21"},{"1":"id","2":"22"},{"1":"id","2":"23"},{"1":"id","2":"24"},{"1":"id","2":"25"},{"1":"id","2":"26"},{"1":"id","2":"27"},{"1":"id","2":"28"},{"1":"id","2":"29"},{"1":"id","2":"30"},{"1":"id","2":"31"},{"1":"id","2":"32"},{"1":"id","2":"33"},{"1":"id","2":"34"},{"1":"id","2":"35"},{"1":"id","2":"36"},{"1":"id","2":"37"},{"1":"id","2":"38"},{"1":"id","2":"39"},{"1":"id","2":"40"},{"1":"id","2":"41"},{"1":"id","2":"42"},{"1":"id","2":"43"},{"1":"id","2":"44"},{"1":"id","2":"45"},{"1":"id","2":"46"},{"1":"id","2":"47"},{"1":"id","2":"48"},{"1":"id","2":"49"},{"1":"id","2":"50"},{"1":"id","2":"51"},{"1":"id","2":"52"},{"1":"id","2":"53"},{"1":"id","2":"54"},{"1":"id","2":"55"},{"1":"id","2":"56"},{"1":"id","2":"57"},{"1":"id","2":"58"},{"1":"id","2":"59"},{"1":"id","2":"60"},{"1":"id","2":"61"},{"1":"id","2":"62"},{"1":"id","2":"63"},{"1":"id","2":"64"},{"1":"id","2":"65"},{"1":"id","2":"66"},{"1":"id","2":"67"},{"1":"id","2":"68"},{"1":"id","2":"69"},{"1":"id","2":"70"},{"1":"id","2":"71"},{"1":"id","2":"72"},{"1":"id","2":"73"},{"1":"id","2":"74"},{"1":"id","2":"75"},{"1":"id","2":"76"},{"1":"id","2":"77"},{"1":"id","2":"78"},{"1":"id","2":"79"},{"1":"id","2":"80"},{"1":"id","2":"81"},{"1":"id","2":"82"},{"1":"id","2":"83"},{"1":"id","2":"84"},{"1":"id","2":"85"},{"1":"id","2":"86"},{"1":"id","2":"87"},{"1":"id","2":"88"},{"1":"id","2":"89"},{"1":"id","2":"90"},{"1":"id","2":"91"},{"1":"id","2":"92"},{"1":"id","2":"93"},{"1":"id","2":"94"},{"1":"id","2":"95"},{"1":"id","2":"96"},{"1":"id","2":"97"},{"1":"id","2":"98"},{"1":"id","2":"99"},{"1":"id","2":"100"},{"1":"id","2":"101"},{"1":"id","2":"102"},{"1":"id","2":"103"},{"1":"id","2":"104"},{"1":"id","2":"105"},{"1":"id","2":"106"},{"1":"id","2":"107"},{"1":"id","2":"108"},{"1":"id","2":"109"},{"1":"id","2":"110"},{"1":"id","2":"111"},{"1":"id","2":"112"},{"1":"id","2":"113"},{"1":"id","2":"114"},{"1":"id","2":"115"},{"1":"id","2":"116"},{"1":"id","2":"117"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb891"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb891-1" title="1"><span class="co"># Boxplot basic</span></a>
<a class="sourceLine" id="cb891-2" title="2">constant.sum[, <span class="dv">-5</span>] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">boxplot</span>(constant.sum.long<span class="op">$</span>Points <span class="op">~</span><span class="st"> </span></a>
<a class="sourceLine" id="cb891-3" title="3"><span class="st">    </span>constant.sum.long<span class="op">$</span>Factor, <span class="dt">col =</span> <span class="kw">rgb</span>(<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>, </a>
<a class="sourceLine" id="cb891-4" title="4">    <span class="fl">0.6</span>), <span class="dt">ylab =</span> <span class="st">&quot;Points&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;What factors do you consider when choosing a place to go for a dinner?&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-548-1.png" width="672" /></p>
<div class="sourceCode" id="cb892"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb892-1" title="1"><span class="co"># Boxplot ggplot2</span></a>
<a class="sourceLine" id="cb892-2" title="2">p &lt;-<span class="st"> </span>constant.sum.long <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Factor <span class="op">!=</span><span class="st"> &quot;id&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb892-3" title="3"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Factor, <span class="dt">y =</span> Points, <span class="dt">fill =</span> Factor)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb892-4" title="4"><span class="st">    </span><span class="kw">geom_boxplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;What factors do you consider when choosing a place to go for a dinner?&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb892-5" title="5"><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb892-6" title="6"><span class="kw">ggplotly</span>(p)</a></code></pre></div>
<div id="htmlwidget-bd2cac915df63d9f4de5" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-bd2cac915df63d9f4de5">{"x":{"data":[{"x":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"y":[32,25,19,20,30,0,0,20,0,20,20,0,0,0,20,10,20,20,30,20,20,10,20,20,0,0,10,10,10,30,0,0,20,20,10,0,20,10,10,0,20,0,10,0,0,0,10,0,40,10,20,0,0,0,10,0,20,0,0,0,0,0,0,10,10,0,30,20,10,0,10,30,20,20,20,10,20,0,20,20,10,10,0,30,20,20,20,10,0,20,10,0,30,15,0,30,20,10,0,20,20,0,0,20,30,10,0,40,0,20,0,10,20,0,20,30,10],"hoverinfo":"y","type":"box","fillcolor":"rgba(248,118,109,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(51,51,51,1)","width":1.88976377952756},"name":" Location","legendgroup":" Location","showlegend":true,"xaxis":"x","yaxis":"y","frame":null},{"x":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],"y":[23,30,21,20,30,20,30,40,40,20,20,30,40,40,30,40,20,30,30,20,10,20,30,30,60,20,30,30,20,20,20,80,40,20,30,50,60,30,30,20,20,50,30,0,40,30,20,50,40,30,20,50,30,100,30,50,30,60,60,40,50,50,10,20,30,50,30,25,30,10,20,20,30,60,20,30,10,40,30,20,20,40,50,20,20,20,30,20,50,20,20,30,30,25,30,30,20,20,100,20,20,30,50,30,40,10,40,30,50,20,30,30,20,0,30,40,10],"hoverinfo":"y","type":"box","fillcolor":"rgba(124,174,0,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(51,51,51,1)","width":1.88976377952756},"name":" Price","legendgroup":" Price","showlegend":true,"xaxis":"x","yaxis":"y","frame":null},{"x":[3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3],"y":[32,22,30,20,10,20,40,0,30,40,20,50,60,20,20,30,30,10,20,20,40,20,10,10,30,50,30,30,30,10,40,20,20,30,20,40,10,30,40,30,10,30,10,60,40,50,20,10,20,20,10,20,20,0,40,30,30,20,20,0,20,30,50,10,20,30,10,20,20,40,10,10,20,20,20,10,30,60,10,20,30,30,50,20,20,30,30,40,0,20,30,30,20,20,50,10,10,30,0,50,20,40,30,30,20,50,40,10,50,20,40,20,40,0,30,20,50],"hoverinfo":"y","type":"box","fillcolor":"rgba(0,191,196,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(51,51,51,1)","width":1.88976377952756},"name":" Ambience","legendgroup":" Ambience","showlegend":true,"xaxis":"x","yaxis":"y","frame":null},{"x":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],"y":[13,23,30,40,30,60,30,40,30,20,40,20,0,40,30,20,30,40,20,40,30,50,40,40,10,30,30,30,40,40,40,0,20,30,40,10,10,30,20,50,50,20,50,40,20,20,50,40,0,40,50,30,50,0,20,20,20,20,20,60,30,20,40,60,40,20,30,35,40,50,60,40,30,0,40,50,40,0,40,40,40,20,0,30,40,30,20,30,50,40,40,40,20,40,20,30,50,40,0,10,40,30,20,20,10,30,20,20,0,40,30,40,20,100,20,10,30],"hoverinfo":"y","type":"box","fillcolor":"rgba(199,124,255,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(51,51,51,1)","width":1.88976377952756},"name":" Customer Service","legendgroup":" Customer Service","showlegend":true,"xaxis":"x","yaxis":"y","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":25.5707762557078,"l":43.1050228310502},"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"What factors do you consider when choosing a place to go for a dinner?","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.4,4.6],"tickmode":"array","ticktext":[" Location"," Price"," Ambience"," Customer Service"],"tickvals":[1,2,3,4],"categoryorder":"array","categoryarray":[" Location"," Price"," Ambience"," Customer Service"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-5,105],"tickmode":"array","ticktext":["0","25","50","75","100"],"tickvals":[0,25,50,75,100],"categoryorder":"array","categoryarray":["0","25","50","75","100"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Points","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":0.96751968503937},"annotations":[{"text":"Factor","x":1.02,"y":1,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"left","yanchor":"bottom","legendTitle":true}],"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"1748dbe2efc":{"x":{},"y":{},"fill":{},"type":"box"}},"cur_data":"1748dbe2efc","visdat":{"1748dbe2efc":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>With the data collected we are able to answer the question: what factor is the most important for our respondents when they go out for a dinner?</p>
<div class="sourceCode" id="cb893"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb893-1" title="1"><span class="kw">library</span>(robCompositions)</a>
<a class="sourceLine" id="cb893-2" title="2"><span class="kw">constSum</span>(constant.sum, <span class="dv">100</span>)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[" Location"],"name":[1],"type":["dbl"],"align":["right"]},{"label":[" Price"],"name":[2],"type":["dbl"],"align":["right"]},{"label":[" Ambience"],"name":[3],"type":["dbl"],"align":["right"]},{"label":[" Customer Service"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["id"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"31.683168","2":"22.772277","3":"31.683168","4":"12.871287","5":"0.990099"},{"1":"24.509804","2":"29.411765","3":"21.568627","4":"22.549020","5":"1.960784"},{"1":"18.446602","2":"20.388350","3":"29.126214","4":"29.126214","5":"2.912621"},{"1":"19.230769","2":"19.230769","3":"19.230769","4":"38.461538","5":"3.846154"},{"1":"28.571429","2":"28.571429","3":"9.523810","4":"28.571429","5":"4.761905"},{"1":"0.000000","2":"18.867925","3":"18.867925","4":"56.603774","5":"5.660377"},{"1":"0.000000","2":"28.037383","3":"37.383178","4":"28.037383","5":"6.542056"},{"1":"18.518519","2":"37.037037","3":"0.000000","4":"37.037037","5":"7.407407"},{"1":"0.000000","2":"36.697248","3":"27.522936","4":"27.522936","5":"8.256881"},{"1":"18.181818","2":"18.181818","3":"36.363636","4":"18.181818","5":"9.090909"},{"1":"18.018018","2":"18.018018","3":"18.018018","4":"36.036036","5":"9.909910"},{"1":"0.000000","2":"26.785714","3":"44.642857","4":"17.857143","5":"10.714286"},{"1":"0.000000","2":"35.398230","3":"53.097345","4":"0.000000","5":"11.504425"},{"1":"0.000000","2":"35.087719","3":"17.543860","4":"35.087719","5":"12.280702"},{"1":"17.391304","2":"26.086957","3":"17.391304","4":"26.086957","5":"13.043478"},{"1":"8.620690","2":"34.482759","3":"25.862069","4":"17.241379","5":"13.793103"},{"1":"17.094017","2":"17.094017","3":"25.641026","4":"25.641026","5":"14.529915"},{"1":"16.949153","2":"25.423729","3":"8.474576","4":"33.898305","5":"15.254237"},{"1":"25.210084","2":"25.210084","3":"16.806723","4":"16.806723","5":"15.966387"},{"1":"16.666667","2":"16.666667","3":"16.666667","4":"33.333333","5":"16.666667"},{"1":"16.528926","2":"8.264463","3":"33.057851","4":"24.793388","5":"17.355372"},{"1":"8.196721","2":"16.393443","3":"16.393443","4":"40.983607","5":"18.032787"},{"1":"16.260163","2":"24.390244","3":"8.130081","4":"32.520325","5":"18.699187"},{"1":"16.129032","2":"24.193548","3":"8.064516","4":"32.258065","5":"19.354839"},{"1":"0.000000","2":"48.000000","3":"24.000000","4":"8.000000","5":"20.000000"},{"1":"0.000000","2":"15.873016","3":"39.682540","4":"23.809524","5":"20.634921"},{"1":"7.874016","2":"23.622047","3":"23.622047","4":"23.622047","5":"21.259843"},{"1":"7.812500","2":"23.437500","3":"23.437500","4":"23.437500","5":"21.875000"},{"1":"7.751938","2":"15.503876","3":"23.255814","4":"31.007752","5":"22.480620"},{"1":"23.076923","2":"15.384615","3":"7.692308","4":"30.769231","5":"23.076923"},{"1":"0.000000","2":"15.267176","3":"30.534351","4":"30.534351","5":"23.664122"},{"1":"0.000000","2":"60.606061","3":"15.151515","4":"0.000000","5":"24.242424"},{"1":"15.037594","2":"30.075188","3":"15.037594","4":"15.037594","5":"24.812030"},{"1":"14.925373","2":"14.925373","3":"22.388060","4":"22.388060","5":"25.373134"},{"1":"7.407407","2":"22.222222","3":"14.814815","4":"29.629630","5":"25.925926"},{"1":"0.000000","2":"36.764706","3":"29.411765","4":"7.352941","5":"26.470588"},{"1":"14.598540","2":"43.795620","3":"7.299270","4":"7.299270","5":"27.007299"},{"1":"7.246377","2":"21.739130","3":"21.739130","4":"21.739130","5":"27.536232"},{"1":"7.194245","2":"21.582734","3":"28.776978","4":"14.388489","5":"28.057554"},{"1":"0.000000","2":"14.285714","3":"21.428571","4":"35.714286","5":"28.571429"},{"1":"14.184397","2":"14.184397","3":"7.092199","4":"35.460993","5":"29.078014"},{"1":"0.000000","2":"35.211268","3":"21.126761","4":"14.084507","5":"29.577465"},{"1":"6.993007","2":"20.979021","3":"6.993007","4":"34.965035","5":"30.069930"},{"1":"0.000000","2":"0.000000","3":"41.666667","4":"27.777778","5":"30.555556"},{"1":"0.000000","2":"27.586207","3":"27.586207","4":"13.793103","5":"31.034483"},{"1":"0.000000","2":"20.547945","3":"34.246575","4":"13.698630","5":"31.506849"},{"1":"6.802721","2":"13.605442","3":"13.605442","4":"34.013605","5":"31.972789"},{"1":"0.000000","2":"33.783784","3":"6.756757","4":"27.027027","5":"32.432432"},{"1":"26.845638","2":"26.845638","3":"13.422819","4":"0.000000","5":"32.885906"},{"1":"6.666667","2":"20.000000","3":"13.333333","4":"26.666667","5":"33.333333"},{"1":"13.245033","2":"13.245033","3":"6.622517","4":"33.112583","5":"33.774834"},{"1":"0.000000","2":"32.894737","3":"13.157895","4":"19.736842","5":"34.210526"},{"1":"0.000000","2":"19.607843","3":"13.071895","4":"32.679739","5":"34.640523"},{"1":"0.000000","2":"64.935065","3":"0.000000","4":"0.000000","5":"35.064935"},{"1":"6.451613","2":"19.354839","3":"25.806452","4":"12.903226","5":"35.483871"},{"1":"0.000000","2":"32.051282","3":"19.230769","4":"12.820513","5":"35.897436"},{"1":"12.738854","2":"19.108280","3":"19.108280","4":"12.738854","5":"36.305732"},{"1":"0.000000","2":"37.974684","3":"12.658228","4":"12.658228","5":"36.708861"},{"1":"0.000000","2":"37.735849","3":"12.578616","4":"12.578616","5":"37.106918"},{"1":"0.000000","2":"25.000000","3":"0.000000","4":"37.500000","5":"37.500000"},{"1":"0.000000","2":"31.055901","3":"12.422360","4":"18.633540","5":"37.888199"},{"1":"0.000000","2":"30.864198","3":"18.518519","4":"12.345679","5":"38.271605"},{"1":"0.000000","2":"6.134969","3":"30.674847","4":"24.539877","5":"38.650307"},{"1":"6.097561","2":"12.195122","3":"6.097561","4":"36.585366","5":"39.024390"},{"1":"6.060606","2":"18.181818","3":"12.121212","4":"24.242424","5":"39.393939"},{"1":"0.000000","2":"30.120482","3":"18.072289","4":"12.048193","5":"39.759036"},{"1":"17.964072","2":"17.964072","3":"5.988024","4":"17.964072","5":"40.119760"},{"1":"11.904762","2":"14.880952","3":"11.904762","4":"20.833333","5":"40.476190"},{"1":"5.917160","2":"17.751479","3":"11.834320","4":"23.668639","5":"40.828402"},{"1":"0.000000","2":"5.882353","3":"23.529412","4":"29.411765","5":"41.176471"},{"1":"5.847953","2":"11.695906","3":"5.847953","4":"35.087719","5":"41.520468"},{"1":"17.441860","2":"11.627907","3":"5.813953","4":"23.255814","5":"41.860465"},{"1":"11.560694","2":"17.341040","3":"11.560694","4":"17.341040","5":"42.196532"},{"1":"11.494253","2":"34.482759","3":"11.494253","4":"0.000000","5":"42.528736"},{"1":"11.428571","2":"11.428571","3":"11.428571","4":"22.857143","5":"42.857143"},{"1":"5.681818","2":"17.045455","3":"5.681818","4":"28.409091","5":"43.181818"},{"1":"11.299435","2":"5.649718","3":"16.949153","4":"22.598870","5":"43.502825"},{"1":"0.000000","2":"22.471910","3":"33.707865","4":"0.000000","5":"43.820225"},{"1":"11.173184","2":"16.759777","3":"5.586592","4":"22.346369","5":"44.134078"},{"1":"11.111111","2":"11.111111","3":"11.111111","4":"22.222222","5":"44.444444"},{"1":"5.524862","2":"11.049724","3":"16.574586","4":"22.099448","5":"44.751381"},{"1":"5.494505","2":"21.978022","3":"16.483516","4":"10.989011","5":"45.054945"},{"1":"0.000000","2":"27.322404","3":"27.322404","4":"0.000000","5":"45.355191"},{"1":"16.304348","2":"10.869565","3":"10.869565","4":"16.304348","5":"45.652174"},{"1":"10.810811","2":"10.810811","3":"10.810811","4":"21.621622","5":"45.945946"},{"1":"10.752688","2":"10.752688","3":"16.129032","4":"16.129032","5":"46.236559"},{"1":"10.695187","2":"16.042781","3":"16.042781","4":"10.695187","5":"46.524064"},{"1":"5.319149","2":"10.638298","3":"21.276596","4":"15.957447","5":"46.808511"},{"1":"0.000000","2":"26.455026","3":"0.000000","4":"26.455026","5":"47.089947"},{"1":"10.526316","2":"10.526316","3":"10.526316","4":"21.052632","5":"47.368421"},{"1":"5.235602","2":"10.471204","3":"15.706806","4":"20.942408","5":"47.643979"},{"1":"0.000000","2":"15.625000","3":"15.625000","4":"20.833333","5":"47.916667"},{"1":"15.544041","2":"15.544041","3":"10.362694","4":"10.362694","5":"48.186528"},{"1":"7.731959","2":"12.886598","3":"10.309278","4":"20.618557","5":"48.453608"},{"1":"0.000000","2":"15.384615","3":"25.641026","4":"10.256410","5":"48.717949"},{"1":"15.306122","2":"15.306122","3":"5.102041","4":"15.306122","5":"48.979592"},{"1":"10.152284","2":"10.152284","3":"5.076142","4":"25.380711","5":"49.238579"},{"1":"5.050505","2":"10.101010","3":"15.151515","4":"20.202020","5":"49.494949"},{"1":"0.000000","2":"50.251256","3":"0.000000","4":"0.000000","5":"49.748744"},{"1":"10.000000","2":"10.000000","3":"25.000000","4":"5.000000","5":"50.000000"},{"1":"9.950249","2":"9.950249","3":"9.950249","4":"19.900498","5":"50.248756"},{"1":"0.000000","2":"14.851485","3":"19.801980","4":"14.851485","5":"50.495050"},{"1":"0.000000","2":"24.630542","3":"14.778325","4":"9.852217","5":"50.738916"},{"1":"9.803922","2":"14.705882","3":"14.705882","4":"9.803922","5":"50.980392"},{"1":"14.634146","2":"19.512195","3":"9.756098","4":"4.878049","5":"51.219512"},{"1":"4.854369","2":"4.854369","3":"24.271845","4":"14.563107","5":"51.456311"},{"1":"0.000000","2":"19.323671","3":"19.323671","4":"9.661836","5":"51.690821"},{"1":"19.230769","2":"14.423077","3":"4.807692","4":"9.615385","5":"51.923077"},{"1":"0.000000","2":"23.923445","3":"23.923445","4":"0.000000","5":"52.153110"},{"1":"9.523810","2":"9.523810","3":"9.523810","4":"19.047619","5":"52.380952"},{"1":"0.000000","2":"14.218009","3":"18.957346","4":"14.218009","5":"52.606635"},{"1":"4.716981","2":"14.150943","3":"9.433962","4":"18.867925","5":"52.830189"},{"1":"9.389671","2":"9.389671","3":"18.779343","4":"9.389671","5":"53.051643"},{"1":"0.000000","2":"0.000000","3":"0.000000","4":"46.728972","5":"53.271028"},{"1":"9.302326","2":"13.953488","3":"13.953488","4":"9.302326","5":"53.488372"},{"1":"13.888889","2":"18.518519","3":"9.259259","4":"4.629630","5":"53.703704"},{"1":"4.608295","2":"4.608295","3":"23.041475","4":"13.824885","5":"53.917051"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>In order to anwser this question we need to conduct <strong>a repeated measures ANOVA</strong>.
This type of ANOVA is used for analyzing data where the same subjects are measured more than once. In our case we have every respondent measured on each of the factors (locations, price, ambience and customer service). Repeated measures ANOVA is an extension of the paired-samples t-test. This test is also referred to as a within-subjects ANOVA. In the within-subject experimental design the same individuals are measured on the same outcome variable under different time points or conditions.</p>
<p>We need to check all assumptions that need to be fulfilled in order to deploy this type of ANOVA. There are three assumputions that need to check. The first to check that each level of the independent variable is approximately normally distributed. Since we have more than 30 observations at each level, we do not need to proceed further due to the central limit theorem. Second assumption referrs to extreme outliers. Let’s have a look at potential outliers:</p>
<div class="sourceCode" id="cb894"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb894-1" title="1"><span class="co"># Outliers</span></a>
<a class="sourceLine" id="cb894-2" title="2">constant.sum.long <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(Factor) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">identify_outliers</span>(Points)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Factor"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["Points"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["is.outlier"],"name":[3],"type":["lgl"],"align":["right"]},{"label":["is.extreme"],"name":[4],"type":["lgl"],"align":["right"]}],"data":[{"1":"Price","2":"80","3":"TRUE","4":"FALSE"},{"1":"Price","2":"100","3":"TRUE","4":"FALSE"},{"1":"Price","2":"100","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"0","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"60","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"60","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"0","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"0","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"60","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"0","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"0","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"0","3":"TRUE","4":"FALSE"},{"1":"Ambience","2":"50","3":"TRUE","4":"FALSE"},{"1":"Customer Service","2":"100","3":"TRUE","4":"FALSE"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>As we cannot identify any extreme outliers, we can proceed with deploying repeated measures ANOVA.</p>
<div class="sourceCode" id="cb895"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb895-1" title="1"><span class="co"># Formatting data</span></a>
<a class="sourceLine" id="cb895-2" title="2">constant.sum.aov &lt;-<span class="st"> </span><span class="kw">gather</span>(constant.sum, <span class="dt">key =</span> <span class="st">&quot;Factor&quot;</span>, </a>
<a class="sourceLine" id="cb895-3" title="3">    <span class="dt">value =</span> <span class="st">&quot;Points&quot;</span>, <span class="st">`</span><span class="dt"> Location</span><span class="st">`</span>, <span class="st">`</span><span class="dt"> Price</span><span class="st">`</span>, <span class="st">`</span><span class="dt"> Ambience</span><span class="st">`</span>, </a>
<a class="sourceLine" id="cb895-4" title="4">    <span class="st">`</span><span class="dt"> Customer Service</span><span class="st">`</span>)</a>
<a class="sourceLine" id="cb895-5" title="5"></a>
<a class="sourceLine" id="cb895-6" title="6"><span class="co"># One-way repeated measures ANOVA</span></a>
<a class="sourceLine" id="cb895-7" title="7">res.aov &lt;-<span class="st"> </span><span class="kw">anova_test</span>(<span class="dt">data =</span> constant.sum.aov, <span class="dt">dv =</span> Points, </a>
<a class="sourceLine" id="cb895-8" title="8">    <span class="dt">wid =</span> id, <span class="dt">within =</span> Factor)</a>
<a class="sourceLine" id="cb895-9" title="9"><span class="kw">get_anova_table</span>(res.aov)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Effect"],"name":[1],"type":["chr"],"align":["left"]},{"label":["DFn"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["DFd"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["p<.05"],"name":[6],"type":["chr"],"align":["left"]},{"label":["ges"],"name":[7],"type":["dbl"],"align":["right"]}],"data":[{"1":"Factor","2":"2.56","3":"297.36","4":"33.668","5":"0.000000000000000106","6":"*","7":"0.225"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb896"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb896-1" title="1"><span class="co"># Post hoc test</span></a>
<a class="sourceLine" id="cb896-2" title="2"><span class="kw">pairwise.t.test</span>(constant.sum.long<span class="op">$</span>Points, constant.sum.long<span class="op">$</span>Factor, </a>
<a class="sourceLine" id="cb896-3" title="3">    <span class="dt">paired =</span> T, <span class="dt">p.adjust.method =</span> <span class="st">&quot;holm&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using paired t tests 
## 
## data:  constant.sum.long$Points and constant.sum.long$Factor 
## 
##                    Location             Price               Ambience          
##  Price            0.00000000000000371  -                   -                  
##  Ambience         0.00000000031537729  0.030               -                  
##  Customer Service &lt; 0.0000000000000002 0.742               0.079              
## id                &lt; 0.0000000000000002 0.00000000001208432 0.00000000000000031
##                    Customer Service  
##  Price            -                  
##  Ambience         -                  
##  Customer Service -                  
## id                0.00000000000298510
## 
## P value adjustment method: holm</code></pre>
<div class="sourceCode" id="cb898"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb898-1" title="1">ggstatsplot<span class="op">::</span><span class="kw">ggwithinstats</span>(</a>
<a class="sourceLine" id="cb898-2" title="2">  <span class="dt">data =</span> constant.sum.long <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Factor<span class="op">!=</span><span class="st">&quot;id&quot;</span>), <span class="co"># excluding &quot;id&quot; column from the data</span></a>
<a class="sourceLine" id="cb898-3" title="3">  <span class="dt">x =</span> Factor,</a>
<a class="sourceLine" id="cb898-4" title="4">  <span class="dt">y =</span> Points,</a>
<a class="sourceLine" id="cb898-5" title="5">  <span class="dt">type =</span> <span class="st">&quot;p&quot;</span>,</a>
<a class="sourceLine" id="cb898-6" title="6">  <span class="dt">pairwise.comparisons =</span> <span class="ot">TRUE</span>, <span class="co"># show pairwise comparison test results</span></a>
<a class="sourceLine" id="cb898-7" title="7">  <span class="dt">title =</span> <span class="st">&quot;What factors do you consider when choosing a place to go for a dinner?&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-552-1.png" width="672" /></p>

<p>References:</p>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/abs/1603.02754">XGBoost: A Scalable Tree Boosting System</a></li>
<li><a href="https://bradleyboehmke.github.io/HOML/gbm.html#xgboost">Hands on machine-learning with R</a></li>
<li><a href="https://towardsdatascience.com/predicting-marketing-performance-with-machine-learning-c8472bc7807">Predicting marketing performance with ML</a></li>
<li><a href="https://doi.org/10.1016/j.dss.2014.03.001">A data-driven approach to predict the success of bank telemarketing</a></li>
<li><a href="http://uc-r.github.io/gbm_regression">Extensive tutorial</a></li>
<li><a href="http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/139-gradient-boosting-essentials-in-r-using-xgboost/">Gradient Boosting Essentials in R Using XGBOOST</a></li>
<li><a href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/">A Gentle Introduction to XGBoost for Applied Machine Learning</a></li>
<li><a href="https://datascienceplus.com/gradient-boosting-in-r/">Gradient Boosting in R</a></li>
</ol>
</div>
</div>
</div>
<div id="gradient-boosting" class="section level2">
<h2><span class="header-section-number">10.4</span> Gradient Boosting</h2>
<p>When we talk about machine learning, there are two quite important factors that drive successful application: effective statistical models that are capable of capturing the complex data dependencies and how scalable are learning systems that learn the model from large datasets. Among the machine learning methods commonly used in practice are gradient tree boosting methods. Gradient boosting machines (GBMs) are a very popular machine learning method, and in this chapter we will introduce you R package “xgboost” and show how it can be used for marketing purposes. It is a scalable machine learning system for tree boosting and GMBs have proven successful across many domains and are one of the leading methods you can find across Kaggle competitions.</p>
<p>When it comes to marketing, it can be used in uplift modeling, i.e. can help a company to identify those who are likely to buy products as a result of receiving a discount or a personalized advertisement. Consequently, it helps a company to maximize profits by keeping advertising costs and overall efforts to the minimum. In the perspective of data analysis and marketing, performance of a marketing campaign can be predicted using algoritham such as GBMs. For instance, in the banking industry optimizing targeting for telemarketing used to be one of the main issues, especially under a growing pressure induced by financial crisis in 2008. A commercial bank from Portugal used data-driven model to predict the result of a telemarketing phone call to sell long term deposits is a valuable tool to support clientselection decisions of bank campaign managers. As a result, they identified that inbound calls and an increase in other attributes identified as a highly relevant (such as agent experience or duration of pre-vious calls) enhance the probability for a successful deposit sell.</p>
<div id="elements-of-supervised-machine-learning" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Elements of supervised machine learning</h3>
<p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" class="uri">https://xgboost.readthedocs.io/en/latest/tutorials/model.html</a>
<a href="https://bradleyboehmke.github.io/HOML/intro.html#supervised-learning" class="uri">https://bradleyboehmke.github.io/HOML/intro.html#supervised-learning</a></p>
<p>xGBoost is used in supervised machine learning. Let us decompose the meaning of supervised machine learning.</p>
<p>Supervised machine learning can be described as a process in which training data with multiple <strong>features </strong>(also called: predictor variables, independent variables, attributes, predictor) is used to predict a <strong>target variable</strong> (also called, dependent variable, response, outcome measurement).</p>
<p>The final outcome of supervised machine learning is a predictive model, so it is important to define what a model is. <strong>A model</strong>, in the context of supervised machine learning, contains a mathematical structure or algorithm by which the prediction of a target variable is made from the multiple features used as input. For instance, algorithm that helps you to predict the sale price of your house based on the house attributes.</p>
<p>Another important term in the context of machine learning are <strong>parameters</strong>. They denote undetermined part that we need to learn from data. For instance, in case of linear regression, the parameters are the <strong>coefficients </strong>.</p>
<p>Finally, as we said, models we build with supervised machine learning are predictive models. “Supervised” refers to a supervisory role of the target variable, which indicates the task that model needs to learn. More specifically, it means that the data which is used for training a model contains target variable. Given a set of training data, the learning algorithm attempts to find the combination of feature values that results in a predicted value as close to the actual target variable as possible.</p>
</div>
<div id="principle-behind-boosting" class="section level3">
<h3><span class="header-section-number">10.4.2</span> Principle behind boosting</h3>
<p>Boosting can be explained as a sequential process. That means that at each particular iteration, a new weak model is trained with respect to the error of the whole ensemble learnt by that time. A weak model is one whose predictions (error rate) are only slightly better than random guessing. In simple words, in each iteration a better model is created by adding a new weak model to the existing one, where the purpose of the weak model is to slightly improve the remaining errors of the existing model. This process slowly learns from data and tries to improve its prediction in subsequent iterations.</p>
<p>Among other, boosting is used for solving both regression and classification problems.
Below you can find an illustrative example and explanation for each of them.
<img src="bigd.png" width="276" style="display: block; margin: auto;" /></p>
<p>In the illustration above you can see 4 boxes with pluses and minuses within them representing observations. The ultimate goal of a model we need to develop is of classification nature, i.e. to classify pluses (“+”) and minuses (-) within a box as accurate as possible.</p>
<p>It is important to mention that at the begining all observations are assigned equal weights. However, weights are subject to change after each iteration as misclassified observations in one iteration will be assigned higher weight in the next one. Opposingly, observations that are correctly classified in one iteration will be assigned lower weight in the subsequent iteration.</p>
<p>In the box 1, the first weak learner identified “+” signs just on the left side of the box. It simply missclassified three “+” signs in the middle “-” upper part and recognized only the two on the left side. Consequently, it split the box in two parts (blue and light red), meaning that everything that appears in the blue “-” marked area is classified as “+”, while the rest is classified as “-”.</p>
<p>Although our prediction model at this point does not do great job, it contains information useful for the next weak lerner that is being added in the box 2. Next weak learner assigns more weight to three “+” signs that were previously missclassified. Similarly to the previous split, the weak learner split the box 2 again in blue “-” and red “-” marked area. Again, everything in the blue area (left from the splitting line) was classified as “+”, including three minus signs being missclassified. The rest was classified as -". Even though our predicting model looks a bit better, its classification is still incorrect.</p>
<p>In the box 3, our model is becoming even better in classifying. It splitted the box horizontally, so that everything below the line was classified as “-” and above the line as “+”. Despite the progress we still have some missclassified “-” in the blue-marked area as well as wrongly classified “+” below the splitting line (circled signs in the box 3).</p>
<p>Finally, in the box 4 we see the result from combining information obtained from numerous weak learners. It is a weighted combination of the weak classifiers resulting in a strong classifier.Each classifier individually proved pretty poor performance in predicting, as they all show certain misclassification error. However, after combining them, the ultimate goal to classify all points correctly is reached and strong classifier created.</p>
<div class="figure"><span id="fig:unnamed-chunk-555"></span>
<img src="boosted_stumps.gif" alt="Fig 1. Boosted regression tree predictions (courtesy of [Brandon Greenwell](https://github.com/bgreenwell))"  />
<p class="caption">
Figure 10.1: Fig 1. Boosted regression tree predictions (courtesy of <a href="https://github.com/bgreenwell">Brandon Greenwell</a>)
</p>
</div>
<p>To understand the whole concept easier, try to follow the image above. On the one hand, the blue curve depicts the real underlying function, while the points depict observations. Moreover, observations include some noise, i.e. errors. On the other hand, you can observe red curve representing constantly improving boosted prediction. More specifically, it illustrates the adjusted predictions after each additional sequential tree is added to the algorithm. At the beginning, you can observe large errors (.i.e. big deviation of the red curve from the blue one) which the boosted algorithm reduces pretty fast. However, as the predictions (.i.e. red curve) get closer to the true underlying function (i.e.blue curve), the contribution to model improvement of each additional tree is smaller and smaller. In the end, the predicted values nearly match to the true underlying function.</p>
</div>
</div>
<div id="xgboost-package" class="section level2">
<h2><span class="header-section-number">10.5</span> xgboost package</h2>
<p>There is an extensive list of packages with GBMs and its variations. However, the most popular implementations which we will cover here is certainly xgboost, which is quite fast and efficient.</p>
<div id="introduction-4" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Introduction</h3>
<p>XGboost stands for “Extreme Gradient Boosting”, which is an efficient implementation of gradient boosting framework. The xgboost package has been quite popular and successful on Kaggle for data mining competitions.</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb899-1" title="1"><span class="co"># Turn off scientific notation</span></a>
<a class="sourceLine" id="cb899-2" title="2"><span class="kw">options</span>(<span class="dt">scipen =</span> <span class="dv">9999</span>)</a>
<a class="sourceLine" id="cb899-3" title="3"></a>
<a class="sourceLine" id="cb899-4" title="4"><span class="co"># Helper packages</span></a>
<a class="sourceLine" id="cb899-5" title="5"><span class="kw">library</span>(dplyr)  <span class="co"># for general data wrangling needs</span></a>
<a class="sourceLine" id="cb899-6" title="6"></a>
<a class="sourceLine" id="cb899-7" title="7"><span class="co"># Modeling packages</span></a>
<a class="sourceLine" id="cb899-8" title="8"><span class="kw">library</span>(xgboost)  <span class="co"># for fitting extreme gradient boosting</span></a>
<a class="sourceLine" id="cb899-9" title="9"><span class="kw">library</span>(rsample)  <span class="co"># for split of data set in the training data and test data</span></a>
<a class="sourceLine" id="cb899-10" title="10"><span class="kw">library</span>(AmesHousing)  <span class="co"># data set</span></a>
<a class="sourceLine" id="cb899-11" title="11"><span class="kw">library</span>(caret)  <span class="co"># for resampling and model training</span></a>
<a class="sourceLine" id="cb899-12" title="12"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb899-13" title="13"><span class="kw">library</span>(recipes)</a>
<a class="sourceLine" id="cb899-14" title="14"><span class="kw">library</span>(pdp)</a>
<a class="sourceLine" id="cb899-15" title="15"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb899-16" title="16"><span class="kw">library</span>(gbm)</a>
<a class="sourceLine" id="cb899-17" title="17"><span class="kw">library</span>(mlr)</a>
<a class="sourceLine" id="cb899-18" title="18"><span class="kw">library</span>(ggplot2)</a></code></pre></div>
<p>To explain gradient boosting with xgboost, we will use typical Ames Iowa Housing data set and try to build a model that predict sale price for houses.</p>
<div class="sourceCode" id="cb900"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb900-1" title="1"><span class="co"># Ames housing data</span></a>
<a class="sourceLine" id="cb900-2" title="2">ames &lt;-<span class="st"> </span>AmesHousing<span class="op">::</span><span class="kw">make_ames</span>()</a>
<a class="sourceLine" id="cb900-3" title="3"></a>
<a class="sourceLine" id="cb900-4" title="4"><span class="co"># Ensure correct naming</span></a>
<a class="sourceLine" id="cb900-5" title="5"><span class="kw">library</span>(janitor)</a>
<a class="sourceLine" id="cb900-6" title="6">ames &lt;-<span class="st"> </span>ames <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">clean_names</span>()</a>
<a class="sourceLine" id="cb900-7" title="7"></a>
<a class="sourceLine" id="cb900-8" title="8"><span class="co"># Use mlr package to get an overview of your data</span></a>
<a class="sourceLine" id="cb900-9" title="9"><span class="kw">summarizeColumns</span>(ames)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["name"],"name":[1],"type":["chr"],"align":["left"]},{"label":["type"],"name":[2],"type":["chr"],"align":["left"]},{"label":["na"],"name":[3],"type":["int"],"align":["right"]},{"label":["mean"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["disp"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["median"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["mad"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["min"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["max"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["nlevs"],"name":[10],"type":["int"],"align":["right"]}],"data":[{"1":"ms_sub_class","2":"factor","3":"0","4":"NA","5":"0.631740614","6":"NA","7":"NA","8":"1.00000","9":"1079.00000","10":"16"},{"1":"ms_zoning","2":"factor","3":"0","4":"NA","5":"0.224232082","6":"NA","7":"NA","8":"2.00000","9":"2273.00000","10":"7"},{"1":"lot_frontage","2":"numeric","3":"0","4":"57.64778157","5":"33.499440794","6":"63.00000","7":"25.20420000","8":"0.00000","9":"313.00000","10":"0"},{"1":"lot_area","2":"integer","3":"0","4":"10147.92184300","5":"7880.017759439","6":"9436.50000","7":"3024.50400000","8":"1300.00000","9":"215245.00000","10":"0"},{"1":"street","2":"factor","3":"0","4":"NA","5":"0.004095563","6":"NA","7":"NA","8":"12.00000","9":"2918.00000","10":"2"},{"1":"alley","2":"factor","3":"0","4":"NA","5":"0.067576792","6":"NA","7":"NA","8":"78.00000","9":"2732.00000","10":"3"},{"1":"lot_shape","2":"factor","3":"0","4":"NA","5":"0.365529010","6":"NA","7":"NA","8":"16.00000","9":"1859.00000","10":"4"},{"1":"land_contour","2":"factor","3":"0","4":"NA","5":"0.101365188","6":"NA","7":"NA","8":"60.00000","9":"2633.00000","10":"4"},{"1":"utilities","2":"factor","3":"0","4":"NA","5":"0.001023891","6":"NA","7":"NA","8":"1.00000","9":"2927.00000","10":"3"},{"1":"lot_config","2":"factor","3":"0","4":"NA","5":"0.269624573","6":"NA","7":"NA","8":"14.00000","9":"2140.00000","10":"5"},{"1":"land_slope","2":"factor","3":"0","4":"NA","5":"0.048122867","6":"NA","7":"NA","8":"16.00000","9":"2789.00000","10":"3"},{"1":"neighborhood","2":"factor","3":"0","4":"NA","5":"0.848805461","6":"NA","7":"NA","8":"1.00000","9":"443.00000","10":"28"},{"1":"condition_1","2":"factor","3":"0","4":"NA","5":"0.139249147","6":"NA","7":"NA","8":"6.00000","9":"2522.00000","10":"9"},{"1":"condition_2","2":"factor","3":"0","4":"NA","5":"0.010238908","6":"NA","7":"NA","8":"1.00000","9":"2900.00000","10":"8"},{"1":"bldg_type","2":"factor","3":"0","4":"NA","5":"0.172354949","6":"NA","7":"NA","8":"62.00000","9":"2425.00000","10":"5"},{"1":"house_style","2":"factor","3":"0","4":"NA","5":"0.494539249","6":"NA","7":"NA","8":"8.00000","9":"1481.00000","10":"8"},{"1":"overall_qual","2":"factor","3":"0","4":"NA","5":"0.718430034","6":"NA","7":"NA","8":"4.00000","9":"825.00000","10":"10"},{"1":"overall_cond","2":"factor","3":"0","4":"NA","5":"0.435494881","6":"NA","7":"NA","8":"0.00000","9":"1654.00000","10":"9"},{"1":"year_built","2":"integer","3":"0","4":"1971.35631399","5":"30.245360629","6":"1973.00000","7":"37.06500000","8":"1872.00000","9":"2010.00000","10":"0"},{"1":"year_remod_add","2":"integer","3":"0","4":"1984.26655290","5":"20.860285877","6":"1993.00000","7":"20.75640000","8":"1950.00000","9":"2010.00000","10":"0"},{"1":"roof_style","2":"factor","3":"0","4":"NA","5":"0.207849829","6":"NA","7":"NA","8":"5.00000","9":"2321.00000","10":"6"},{"1":"roof_matl","2":"factor","3":"0","4":"NA","5":"0.014675768","6":"NA","7":"NA","8":"1.00000","9":"2887.00000","10":"8"},{"1":"exterior_1st","2":"factor","3":"0","4":"NA","5":"0.649829352","6":"NA","7":"NA","8":"1.00000","9":"1026.00000","10":"16"},{"1":"exterior_2nd","2":"factor","3":"0","4":"NA","5":"0.653583618","6":"NA","7":"NA","8":"1.00000","9":"1015.00000","10":"17"},{"1":"mas_vnr_type","2":"factor","3":"0","4":"NA","5":"0.394197952","6":"NA","7":"NA","8":"1.00000","9":"1775.00000","10":"5"},{"1":"mas_vnr_area","2":"numeric","3":"0","4":"101.09692833","5":"178.634544826","6":"0.00000","7":"0.00000000","8":"0.00000","9":"1600.00000","10":"0"},{"1":"exter_qual","2":"factor","3":"0","4":"NA","5":"0.386006826","6":"NA","7":"NA","8":"35.00000","9":"1799.00000","10":"4"},{"1":"exter_cond","2":"factor","3":"0","4":"NA","5":"0.130034130","6":"NA","7":"NA","8":"3.00000","9":"2549.00000","10":"5"},{"1":"foundation","2":"factor","3":"0","4":"NA","5":"0.552901024","6":"NA","7":"NA","8":"5.00000","9":"1310.00000","10":"6"},{"1":"bsmt_qual","2":"factor","3":"0","4":"NA","5":"0.562116041","6":"NA","7":"NA","8":"2.00000","9":"1283.00000","10":"6"},{"1":"bsmt_cond","2":"factor","3":"0","4":"NA","5":"0.107167235","6":"NA","7":"NA","8":"3.00000","9":"2616.00000","10":"6"},{"1":"bsmt_exposure","2":"factor","3":"0","4":"NA","5":"0.349488055","6":"NA","7":"NA","8":"83.00000","9":"1906.00000","10":"5"},{"1":"bsmt_fin_type_1","2":"factor","3":"0","4":"NA","5":"0.706825939","6":"NA","7":"NA","8":"80.00000","9":"859.00000","10":"7"},{"1":"bsmt_fin_sf_1","2":"numeric","3":"0","4":"4.17747440","5":"2.233372483","6":"3.00000","7":"2.96520000","8":"0.00000","9":"7.00000","10":"0"},{"1":"bsmt_fin_type_2","2":"factor","3":"0","4":"NA","5":"0.147098976","6":"NA","7":"NA","8":"34.00000","9":"2499.00000","10":"7"},{"1":"bsmt_fin_sf_2","2":"numeric","3":"0","4":"49.70546075","5":"169.142089287","6":"0.00000","7":"0.00000000","8":"0.00000","9":"1526.00000","10":"0"},{"1":"bsmt_unf_sf","2":"numeric","3":"0","4":"559.07167235","5":"439.540571057","6":"465.50000","7":"414.38670000","8":"0.00000","9":"2336.00000","10":"0"},{"1":"total_bsmt_sf","2":"numeric","3":"0","4":"1051.25563140","5":"440.968017664","6":"990.00000","7":"349.89360000","8":"0.00000","9":"6110.00000","10":"0"},{"1":"heating","2":"factor","3":"0","4":"NA","5":"0.015358362","6":"NA","7":"NA","8":"1.00000","9":"2885.00000","10":"6"},{"1":"heating_qc","2":"factor","3":"0","4":"NA","5":"0.489761092","6":"NA","7":"NA","8":"3.00000","9":"1495.00000","10":"5"},{"1":"central_air","2":"factor","3":"0","4":"NA","5":"0.066894198","6":"NA","7":"NA","8":"196.00000","9":"2734.00000","10":"2"},{"1":"electrical","2":"factor","3":"0","4":"NA","5":"0.084641638","6":"NA","7":"NA","8":"1.00000","9":"2682.00000","10":"6"},{"1":"first_flr_sf","2":"integer","3":"0","4":"1159.55767918","5":"391.890885253","6":"1084.00000","7":"349.89360000","8":"334.00000","9":"5095.00000","10":"0"},{"1":"second_flr_sf","2":"integer","3":"0","4":"335.45597270","5":"428.395715009","6":"0.00000","7":"0.00000000","8":"0.00000","9":"2065.00000","10":"0"},{"1":"low_qual_fin_sf","2":"integer","3":"0","4":"4.67679181","5":"46.310510034","6":"0.00000","7":"0.00000000","8":"0.00000","9":"1064.00000","10":"0"},{"1":"gr_liv_area","2":"integer","3":"0","4":"1499.69044369","5":"505.508887472","6":"1442.00000","7":"461.08860000","8":"334.00000","9":"5642.00000","10":"0"},{"1":"bsmt_full_bath","2":"numeric","3":"0","4":"0.43105802","5":"0.524761963","6":"0.00000","7":"0.00000000","8":"0.00000","9":"3.00000","10":"0"},{"1":"bsmt_half_bath","2":"numeric","3":"0","4":"0.06109215","5":"0.245175020","6":"0.00000","7":"0.00000000","8":"0.00000","9":"2.00000","10":"0"},{"1":"full_bath","2":"integer","3":"0","4":"1.56655290","5":"0.552940612","6":"2.00000","7":"0.00000000","8":"0.00000","9":"4.00000","10":"0"},{"1":"half_bath","2":"integer","3":"0","4":"0.37952218","5":"0.502629253","6":"0.00000","7":"0.00000000","8":"0.00000","9":"2.00000","10":"0"},{"1":"bedroom_abv_gr","2":"integer","3":"0","4":"2.85426621","5":"0.827731142","6":"3.00000","7":"0.00000000","8":"0.00000","9":"8.00000","10":"0"},{"1":"kitchen_abv_gr","2":"integer","3":"0","4":"1.04436860","5":"0.214076244","6":"1.00000","7":"0.00000000","8":"0.00000","9":"3.00000","10":"0"},{"1":"kitchen_qual","2":"factor","3":"0","4":"NA","5":"0.490102389","6":"NA","7":"NA","8":"1.00000","9":"1494.00000","10":"5"},{"1":"tot_rms_abv_grd","2":"integer","3":"0","4":"6.44300341","5":"1.572964396","6":"6.00000","7":"1.48260000","8":"2.00000","9":"15.00000","10":"0"},{"1":"functional","2":"factor","3":"0","4":"NA","5":"0.068941980","6":"NA","7":"NA","8":"2.00000","9":"2728.00000","10":"8"},{"1":"fireplaces","2":"integer","3":"0","4":"0.59931741","5":"0.647920917","6":"1.00000","7":"1.48260000","8":"0.00000","9":"4.00000","10":"0"},{"1":"fireplace_qu","2":"factor","3":"0","4":"NA","5":"0.514675768","6":"NA","7":"NA","8":"43.00000","9":"1422.00000","10":"6"},{"1":"garage_type","2":"factor","3":"0","4":"NA","5":"0.409215017","6":"NA","7":"NA","8":"15.00000","9":"1731.00000","10":"7"},{"1":"garage_finish","2":"factor","3":"0","4":"NA","5":"0.579863481","6":"NA","7":"NA","8":"159.00000","9":"1231.00000","10":"4"},{"1":"garage_cars","2":"numeric","3":"0","4":"1.76621160","5":"0.761136719","6":"2.00000","7":"0.00000000","8":"0.00000","9":"5.00000","10":"0"},{"1":"garage_area","2":"numeric","3":"0","4":"472.65836177","5":"215.187195710","6":"480.00000","7":"182.35980000","8":"0.00000","9":"1488.00000","10":"0"},{"1":"garage_qual","2":"factor","3":"0","4":"NA","5":"0.107508532","6":"NA","7":"NA","8":"3.00000","9":"2615.00000","10":"6"},{"1":"garage_cond","2":"factor","3":"0","4":"NA","5":"0.090443686","6":"NA","7":"NA","8":"3.00000","9":"2665.00000","10":"6"},{"1":"paved_drive","2":"factor","3":"0","4":"NA","5":"0.094880546","6":"NA","7":"NA","8":"62.00000","9":"2652.00000","10":"3"},{"1":"wood_deck_sf","2":"integer","3":"0","4":"93.75187713","5":"126.361561879","6":"0.00000","7":"0.00000000","8":"0.00000","9":"1424.00000","10":"0"},{"1":"open_porch_sf","2":"integer","3":"0","4":"47.53344710","5":"67.483400137","6":"27.00000","7":"40.03020000","8":"0.00000","9":"742.00000","10":"0"},{"1":"enclosed_porch","2":"integer","3":"0","4":"23.01160410","5":"64.139059209","6":"0.00000","7":"0.00000000","8":"0.00000","9":"1012.00000","10":"0"},{"1":"three_season_porch","2":"integer","3":"0","4":"2.59249147","5":"25.141331032","6":"0.00000","7":"0.00000000","8":"0.00000","9":"508.00000","10":"0"},{"1":"screen_porch","2":"integer","3":"0","4":"16.00204778","5":"56.087370229","6":"0.00000","7":"0.00000000","8":"0.00000","9":"576.00000","10":"0"},{"1":"pool_area","2":"integer","3":"0","4":"2.24334471","5":"35.597180615","6":"0.00000","7":"0.00000000","8":"0.00000","9":"800.00000","10":"0"},{"1":"pool_qc","2":"factor","3":"0","4":"NA","5":"0.004436860","6":"NA","7":"NA","8":"2.00000","9":"2917.00000","10":"5"},{"1":"fence","2":"factor","3":"0","4":"NA","5":"0.195221843","6":"NA","7":"NA","8":"12.00000","9":"2358.00000","10":"5"},{"1":"misc_feature","2":"factor","3":"0","4":"NA","5":"0.036177474","6":"NA","7":"NA","8":"1.00000","9":"2824.00000","10":"6"},{"1":"misc_val","2":"integer","3":"0","4":"50.63515358","5":"566.344288259","6":"0.00000","7":"0.00000000","8":"0.00000","9":"17000.00000","10":"0"},{"1":"mo_sold","2":"integer","3":"0","4":"6.21604096","5":"2.714492425","6":"6.00000","7":"2.96520000","8":"1.00000","9":"12.00000","10":"0"},{"1":"year_sold","2":"integer","3":"0","4":"2007.79044369","5":"1.316612923","6":"2008.00000","7":"1.48260000","8":"2006.00000","9":"2010.00000","10":"0"},{"1":"sale_type","2":"factor","3":"0","4":"NA","5":"0.134470990","6":"NA","7":"NA","8":"1.00000","9":"2536.00000","10":"10"},{"1":"sale_condition","2":"factor","3":"0","4":"NA","5":"0.176450512","6":"NA","7":"NA","8":"12.00000","9":"2413.00000","10":"6"},{"1":"sale_price","2":"integer","3":"0","4":"180796.06006826","5":"79886.692356665","6":"160000.00000","7":"54856.20000000","8":"12789.00000","9":"755000.00000","10":"0"},{"1":"longitude","2":"numeric","3":"0","4":"-93.64289690","5":"0.025699571","6":"-93.64181","7":"0.02852471","8":"-93.69315","9":"-93.57743","10":"0"},{"1":"latitude","2":"numeric","3":"0","4":"42.03448223","5":"0.018410072","6":"42.03466","7":"0.02007885","8":"41.98650","9":"42.06339","10":"0"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="data-preparation" class="section level3">
<h3><span class="header-section-number">10.5.2</span> Data preparation</h3>
<p>First, we need to deal with data preparation. When using xgboost package, it is necessary to convert the categorical variables into numeric using one hot encoding. What is one hot encoding?
Usually, when between several categories exist ordinal relationship (e.g. variable “place” can be “1st”, “2nd” and so on), all you need to do is so called the integer encoding. In the ordinal variable (such as “place”) the integer values have a natural ordered relationship between each other, so machine learning algorithms are able to understand this relationship. However, for categorical variables where no ordinal relationship exists (e.g. variable “pet” with “dog”, “cat” and “rabbit”), the integer encoding is not sufficient and you need one hot encoding. In the “pet” example, there are 3 categories, thus 3 binary variables are required. Therefore, “1” value is placed in the binary variable for the respective “color” and “0” values for the other colors.</p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb901-1" title="1"><span class="kw">cat</span>(tabl)</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th>dog</th>
<th>cat</th>
<th>rabbit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can apply one hot encoding to our data set by using R’s base function <code>model.matrix</code>. In the code below, ~.+0 leads to encoding of all categorical variables without producing an intercept.</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb902-1" title="1"><span class="kw">options</span>(<span class="st">&quot;contrasts&quot;</span>)</a></code></pre></div>
<pre><code>## $contrasts
##         unordered           ordered 
## &quot;contr.treatment&quot;      &quot;contr.poly&quot;</code></pre>
<div class="sourceCode" id="cb904"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb904-1" title="1"><span class="co"># One hot encoding (turning the test data into</span></a>
<a class="sourceLine" id="cb904-2" title="2"><span class="co"># matrix with all numerical values)</span></a>
<a class="sourceLine" id="cb904-3" title="3">ames.he &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>. <span class="op">+</span><span class="st"> </span><span class="dv">0</span>, <span class="dt">data =</span> ames)</a>
<a class="sourceLine" id="cb904-4" title="4"></a>
<a class="sourceLine" id="cb904-5" title="5"><span class="co"># Save variable names due to more practical</span></a>
<a class="sourceLine" id="cb904-6" title="6"><span class="co"># addressing columns later on</span></a>
<a class="sourceLine" id="cb904-7" title="7">setcol &lt;-<span class="st"> </span><span class="kw">colnames</span>(ames.he)</a>
<a class="sourceLine" id="cb904-8" title="8"><span class="kw">head</span>(ames.he[, <span class="dv">307</span>])  <span class="co"># Sale price column</span></a></code></pre></div>
<pre><code>##      1      2      3      4      5      6 
## 215000 105000 172000 244000 189900 195500</code></pre>
<p>We need to break our data set into training and test data, while ensuring we have consistent distributions between the training and test sets.</p>
<div class="sourceCode" id="cb906"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb906-1" title="1"><span class="co"># Data split on test and train data</span></a>
<a class="sourceLine" id="cb906-2" title="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb906-3" title="3"></a>
<a class="sourceLine" id="cb906-4" title="4"><span class="co"># Create partition library(caret)</span></a>
<a class="sourceLine" id="cb906-5" title="5">index &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">createDataPartition</span>(ames<span class="op">$</span>sale_price, </a>
<a class="sourceLine" id="cb906-6" title="6">    <span class="dt">p =</span> <span class="fl">0.7</span>, <span class="dt">list =</span> F)</a></code></pre></div>
<p><code>index</code> is a matrix with just one column that contains approximately 70% of rows from our original data set.</p>
<div class="sourceCode" id="cb907"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb907-1" title="1"><span class="kw">head</span>(index)</a></code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         2
## [3,]         5
## [4,]         7
## [5,]         8
## [6,]         9</code></pre>
<p>Now we use <code>index</code> to address columns we want to assign to our train data (<code>ames_train</code>), i.e. columns that we want to assign to our test data (<code>ames_test</code>).
Note that by using “-” in front of <code>index</code> we assign to <code>ames_test</code> all observations <strong>except</strong> those that are in <code>index</code>.</p>
<div class="sourceCode" id="cb909"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb909-1" title="1"><span class="co"># Split data set in train and test data</span></a>
<a class="sourceLine" id="cb909-2" title="2">ames_train &lt;-<span class="st"> </span>ames.he[index, ]</a>
<a class="sourceLine" id="cb909-3" title="3">ames_test &lt;-<span class="st"> </span>ames.he[<span class="op">-</span>index, ]</a></code></pre></div>
<p>If we take lake look at the dimensions of our test and train data, we can see that our split was successful.</p>
<div class="sourceCode" id="cb910"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb910-1" title="1"><span class="kw">dim</span>(ames_train)  <span class="co"># 2053 observations for training data</span></a></code></pre></div>
<pre><code>## [1] 2053  309</code></pre>
<div class="sourceCode" id="cb912"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb912-1" title="1"><span class="kw">dim</span>(ames_test)  <span class="co"># 877 observations for testing data</span></a></code></pre></div>
<pre><code>## [1] 877 309</code></pre>
<div class="sourceCode" id="cb914"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb914-1" title="1"><span class="co"># colnames(ames_train) colnames(ames_test)</span></a></code></pre></div>
<p>We are not done with preparation of data. Since our task is to build a predictive model for house pricing based on multiple features, our target variable (sale price) needs to be exluded from the test data. Newly created variable will be used at the end to test accuracy of our final model.</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb915-1" title="1"><span class="co"># Test data Matrix containing all columns from the</span></a>
<a class="sourceLine" id="cb915-2" title="2"><span class="co"># test data except dependent variable &#39;Sale_Price&#39;</span></a>
<a class="sourceLine" id="cb915-3" title="3">ames_x_test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ames_test[, <span class="dv">-307</span>])  <span class="co"># addressing &#39;Sale Price&#39; column in matrix and excluding it</span></a>
<a class="sourceLine" id="cb915-4" title="4"><span class="kw">colnames</span>(ames_x_test)[<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>]  <span class="co"># No &#39;Sale Price&#39; anymore here! It used to be among last columns.</span></a></code></pre></div>
<pre><code>## [1] &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;          &quot;sale_conditionAdjLand&quot;
## [4] &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot;  &quot;sale_conditionNormal&quot; 
## [7] &quot;sale_conditionPartial&quot; &quot;longitude&quot;             &quot;latitude&quot;</code></pre>
<p>For training purposes, target variable (sale price) needs to be excluded from the train data set as well.</p>
<div class="sourceCode" id="cb917"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb917-1" title="1"><span class="co"># Train data set Matrix containing all columns from</span></a>
<a class="sourceLine" id="cb917-2" title="2"><span class="co"># the train data except dependent variable</span></a>
<a class="sourceLine" id="cb917-3" title="3"><span class="co"># &#39;Sale_Price&#39;</span></a>
<a class="sourceLine" id="cb917-4" title="4">ames_x_train &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ames_train[, <span class="dv">-307</span>])</a></code></pre></div>
<p>However, the target variable is going to be stored seprately because the learning algorithm in a predictive model attempts to discover and model the relationships among the target variable and the other features.</p>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb918-1" title="1"><span class="co"># Dependent/Target variable &#39;Sales_Price&#39; from the</span></a>
<a class="sourceLine" id="cb918-2" title="2"><span class="co"># train data in a from of a vector</span></a>
<a class="sourceLine" id="cb918-3" title="3">ames_y_train &lt;-<span class="st"> </span>ames_train[, <span class="dv">307</span>]</a></code></pre></div>
</div>
<div id="engineering" class="section level3">
<h3><span class="header-section-number">10.5.3</span> Engineering</h3>
<p>In order to create a good predictive model, usually the most of time is spent optimizing parameters. Before we start training our model, let us take a closer look at what paramters we need to handle.</p>
<p>There are 3 categories XGBoost parameters can be divided into:</p>
<ol style="list-style-type: decimal">
<li>General parameters</li>
<li>Boosting parameters</li>
<li>Tree-specific paramters</li>
</ol>
<p>General parameters will not be discussed in further details, but it consists of 3 parameters:</p>
<ol style="list-style-type: decimal">
<li><code>booster</code> - determines the booster type (gbtree, gblinear or dart) to use. For regression, you can use any. By default it is gbtree (which we will use as well).<br />
</li>
<li><code>nthread</code> - refers to the number of cores activated when computing. By default it uses maximum cores available, which leads to the fastest computation.<br />
</li>
<li><code>silent</code> - refers to turning on (“1”) running messages in R console. By default “0” is set, so that console does not get flooded with messages.</li>
</ol>
<p>For general parameters we will be using default options.</p>
<p>Next, booster parameters control the performance of the selected booster(gbtree in our case). At this moment we will introduce just the main ones:</p>
<ol style="list-style-type: decimal">
<li><code>nrounds</code> - set the maximum number of iterations.<br />
</li>
<li><code>eta</code> - stands for the learning rate. It determines the rate at which our model learns patterns in data. After every iteration, it shrinks the feature weights to reach the best optimum. Smaller learning rates lead to longer computation time. It is important to note that smaller learning rates should be supported by increasing number of iterations. Otherwise, the risk of reaching the optimum is more likely. Usually, it lies between 0.01 - 0.3.</li>
<li><code>max_depth</code> - which determines the maximum depth of each tree. Generally, it is stands that larger the depth, more complex the model and consequently higher chances of overfitting.
4.<code>min_child_weight</code> - minimum number of observations required in each terminal node</li>
<li><code>subsample</code> - percent of training data to sample for each tree</li>
<li><code>colsample_bytrees</code> - percent of columns to sample from for each tree</li>
<li><code>early_stopping_rounds</code> - stopping the training model as soon as evaluation metric (for regression that is “RMSE”) does not improve for a given number of rounds</li>
</ol>
<p>Finally, learning task parameters define methods for the loss function and model evaluation:</p>
<ol style="list-style-type: decimal">
<li><code>objective</code>- for linear regression it should be set to “reg:linear”.</li>
<li><code>eval_metric</code> - this parameter depends on <code>objective</code>. Here we set metrics used to evaluate a model’s accuracy on validation data. When “reg:linear” set as objective, default metric is RMSE.</li>
</ol>
<p>A package with useful tools for parameter optimization is <code>mlr</code>. It includes extensive list of parameters for any type of algorithm. We can take a look at list with paramters for regression and check parameters we just dicussed.</p>
<div class="sourceCode" id="cb919"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb919-1" title="1"><span class="co"># Parameters for regression</span></a>
<a class="sourceLine" id="cb919-2" title="2"><span class="kw">getParamSet</span>(<span class="st">&quot;regr.xgboost&quot;</span>)</a></code></pre></div>
<pre><code>##                                 Type  len           Def
## booster                     discrete    -        gbtree
## watchlist                    untyped    -        &lt;NULL&gt;
## eta                          numeric    -           0.3
## gamma                        numeric    -             0
## max_depth                    integer    -             6
## min_child_weight             numeric    -             1
## subsample                    numeric    -             1
## colsample_bytree             numeric    -             1
## colsample_bylevel            numeric    -             1
## colsample_bynode             numeric    -             1
## num_parallel_tree            integer    -             1
## lambda                       numeric    -             1
## lambda_bias                  numeric    -             0
## alpha                        numeric    -             0
## objective                    untyped    -    reg:linear
## eval_metric                  untyped    -          rmse
## base_score                   numeric    -           0.5
## max_delta_step               numeric    -             0
## missing                      numeric    -              
## monotone_constraints   integervector &lt;NA&gt;             0
## tweedie_variance_power       numeric    -           1.5
## nthread                      integer    -             -
## nrounds                      integer    -             -
## feval                        untyped    -        &lt;NULL&gt;
## verbose                      integer    -             1
## print_every_n                integer    -             1
## early_stopping_rounds        integer    -        &lt;NULL&gt;
## maximize                     logical    -        &lt;NULL&gt;
## sample_type                 discrete    -       uniform
## normalize_type              discrete    -          tree
## rate_drop                    numeric    -             0
## skip_drop                    numeric    -             0
## scale_pos_weight             numeric    -             1
## refresh_leaf                 logical    -          TRUE
## feature_selector            discrete    -        cyclic
## top_k                        integer    -             0
## predictor                   discrete    - cpu_predictor
## updater                      untyped    -             -
## sketch_eps                   numeric    -          0.03
## one_drop                     logical    -         FALSE
## tree_method                 discrete    -          auto
## grow_policy                 discrete    -     depthwise
## max_leaves                   integer    -             0
## max_bin                      integer    -           256
## callbacks                    untyped    -              
##                                                      Constr Req Tunable Trafo
## booster                                gbtree,gblinear,dart   -    TRUE     -
## watchlist                                                 -   -   FALSE     -
## eta                                                  0 to 1   -    TRUE     -
## gamma                                              0 to Inf   -    TRUE     -
## max_depth                                          0 to Inf   -    TRUE     -
## min_child_weight                                   0 to Inf   -    TRUE     -
## subsample                                            0 to 1   -    TRUE     -
## colsample_bytree                                     0 to 1   -    TRUE     -
## colsample_bylevel                                    0 to 1   -    TRUE     -
## colsample_bynode                                     0 to 1   -    TRUE     -
## num_parallel_tree                                  1 to Inf   -    TRUE     -
## lambda                                             0 to Inf   -    TRUE     -
## lambda_bias                                        0 to Inf   -    TRUE     -
## alpha                                              0 to Inf   -    TRUE     -
## objective                                                 -   -   FALSE     -
## eval_metric                                               -   -   FALSE     -
## base_score                                      -Inf to Inf   -   FALSE     -
## max_delta_step                                     0 to Inf   -    TRUE     -
## missing                                         -Inf to Inf   -   FALSE     -
## monotone_constraints                                -1 to 1   -    TRUE     -
## tweedie_variance_power                               1 to 2   Y    TRUE     -
## nthread                                            1 to Inf   -   FALSE     -
## nrounds                                            1 to Inf   -    TRUE     -
## feval                                                     -   -   FALSE     -
## verbose                                              0 to 2   -   FALSE     -
## print_every_n                                      1 to Inf   Y   FALSE     -
## early_stopping_rounds                              1 to Inf   -   FALSE     -
## maximize                                                  -   -   FALSE     -
## sample_type                                uniform,weighted   Y    TRUE     -
## normalize_type                                  tree,forest   Y    TRUE     -
## rate_drop                                            0 to 1   Y    TRUE     -
## skip_drop                                            0 to 1   Y    TRUE     -
## scale_pos_weight                                -Inf to Inf   -    TRUE     -
## refresh_leaf                                              -   -    TRUE     -
## feature_selector       cyclic,shuffle,random,greedy,thrifty   -    TRUE     -
## top_k                                              0 to Inf   -    TRUE     -
## predictor                       cpu_predictor,gpu_predictor   -    TRUE     -
## updater                                                   -   -    TRUE     -
## sketch_eps                                           0 to 1   -    TRUE     -
## one_drop                                                  -   Y    TRUE     -
## tree_method                 auto,exact,approx,hist,gpu_hist   Y    TRUE     -
## grow_policy                             depthwise,lossguide   Y    TRUE     -
## max_leaves                                         0 to Inf   Y    TRUE     -
## max_bin                                            2 to Inf   Y    TRUE     -
## callbacks                                                 -   -   FALSE     -</code></pre>
<p>To build a well-performing predicitive model many iterations are necessary. Therefore, in order to determine how good or bad one model predicts the target variable, performance evaluation needs to be conducted. A technique that will be used to help us in evaluating performance of our future machine learning models is called <strong>k-fold cross-validation technique.</strong> K-fold cross-validation evaluates a model by training a couple of models on subsets of the available input data and evaluating them on the complementary subset of the data. In this process, training data is splitted into k groups (i.e. folds) of approximately equal size. Then the model is fit on k−1 folds and the remaining fold is used in computation of the model performance. This procedure is repeated k times, where each time, a different fold is treated as the validation set (i.e. used in computation of the model performance). Thus, the final cross-validation k-fold estimate is computed by averaging the k test errors. The final output provided is an approximation of the error we may expect on unseen data.</p>
<p>The first model to pass to the k-fold cross validation will be built using default parameters. As default for number of iterations (<code>nrounds</code>) is zero, we will set it on 200.</p>
<div class="sourceCode" id="cb921"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb921-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb921-2" title="2">ames_xgb &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</a>
<a class="sourceLine" id="cb921-3" title="3">  <span class="dt">booster =</span> <span class="st">&quot;gbtree&quot;</span>,</a>
<a class="sourceLine" id="cb921-4" title="4">  <span class="dt">data =</span> ames_x_train,      <span class="co"># matrix with train data without sale price</span></a>
<a class="sourceLine" id="cb921-5" title="5">  <span class="dt">label =</span> ames_y_train,     <span class="co"># numerical vector with sale price with train data </span></a>
<a class="sourceLine" id="cb921-6" title="6">  <span class="dt">nrounds =</span> <span class="dv">200</span>,            <span class="co"># numer of iterations </span></a>
<a class="sourceLine" id="cb921-7" title="7">  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>, <span class="co"># parameter refering to the function to be me minimised (RMSE)</span></a>
<a class="sourceLine" id="cb921-8" title="8">  <span class="dt">nfold =</span> <span class="dv">10</span>,                <span class="co"># data is randomly partitioned into nfold equal size subsamples.</span></a>
<a class="sourceLine" id="cb921-9" title="9">  <span class="dt">params =</span> <span class="kw">list</span>(            <span class="co"># defining the list of parameters</span></a>
<a class="sourceLine" id="cb921-10" title="10">    <span class="dt">eta =</span> <span class="fl">0.3</span>,              <span class="co"># learning rate </span></a>
<a class="sourceLine" id="cb921-11" title="11">    <span class="dt">max_depth =</span> <span class="dv">6</span>,          <span class="co"># maximal depth of tree</span></a>
<a class="sourceLine" id="cb921-12" title="12">    <span class="dt">min_child_weight =</span> <span class="dv">1</span>,   <span class="co"># minimum number of observations required in each terminal node</span></a>
<a class="sourceLine" id="cb921-13" title="13">    <span class="dt">subsample =</span> <span class="dv">1</span>,          <span class="co"># percent of training data to sample for each tree</span></a>
<a class="sourceLine" id="cb921-14" title="14">    <span class="dt">colsample_bytree =</span> <span class="dv">1</span>    <span class="co"># percent of columns to sample from for each tree</span></a>
<a class="sourceLine" id="cb921-15" title="15">    ),</a>
<a class="sourceLine" id="cb921-16" title="16">  <span class="dt">verbose =</span> <span class="dv">0</span>               <span class="co"># print the statistics during the process (1 or 0)</span></a>
<a class="sourceLine" id="cb921-17" title="17">  )  </a></code></pre></div>
<pre><code>## [12:35:57] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:00] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:03] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:05] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:06] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:06] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:06] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:06] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:06] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:06] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.</code></pre>
<div class="sourceCode" id="cb923"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb923-1" title="1">(eval &lt;-<span class="st"> </span>ames_xgb<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb923-2" title="2"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(</a>
<a class="sourceLine" id="cb923-3" title="3">    <span class="dt">ntrees.train =</span> <span class="kw">which</span>(train_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(train_rmse_mean))[<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb923-4" title="4">    <span class="dt">rmse.train   =</span> <span class="kw">min</span>(train_rmse_mean),</a>
<a class="sourceLine" id="cb923-5" title="5">    <span class="dt">ntrees.test  =</span> <span class="kw">which</span>(test_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(test_rmse_mean))[<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb923-6" title="6">    <span class="dt">rmse.test   =</span> <span class="kw">min</span>(test_rmse_mean)))</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["ntrees.train"],"name":[1],"type":["int"],"align":["right"]},{"label":["rmse.train"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["ntrees.test"],"name":[3],"type":["int"],"align":["right"]},{"label":["rmse.test"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"200","2":"596.9639","3":"180","4":"25324.12"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>After conducting the 10-fold cross validation, we sorted the output so that it shows us at what iteration (round) our model reached the lowest error (RMSE) when fitted to the seen part of training data (rmse.train) and unseen part of the training data (rmse.test).</p>
<p><em>Side note: unseen part of the training data (rmse.test) has nothing to do with test data from the initial split we did at the very begining of the chapter and named as <code>ames_test</code> . Here we talk about unseen data in the process of k-fold cross-validation.</em></p>
<p>Unsprisingly, our model performed very well when fitted to the seen data, suggesting the RMSE of <code>eval$rmse.train</code>. Here we see an evidance of overfitting. In other words, our model fits the training part of the training data very well, but is not generalizable, i.e. when confronted with unseen data, its predictions are not as good as for the trained part.</p>
<div class="sourceCode" id="cb924"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb924-1" title="1"><span class="co"># Plot error vs number trees</span></a>
<a class="sourceLine" id="cb924-2" title="2">pe &lt;-<span class="st"> </span><span class="kw">ggplot</span>(ames_xgb<span class="op">$</span>evaluation_log) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, </a>
<a class="sourceLine" id="cb924-3" title="3">    train_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, </a>
<a class="sourceLine" id="cb924-4" title="4">    test_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="dt">label =</span> <span class="st">&quot;Iteration (round)&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb924-5" title="5"><span class="st">    </span><span class="kw">ylab</span>(<span class="dt">label =</span> <span class="st">&quot;Root Mean Square Error&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="dt">label =</span> <span class="st">&quot;10-fold Cross-validation&quot;</span>)</a>
<a class="sourceLine" id="cb924-6" title="6"><span class="kw">ggplotly</span>(pe)</a></code></pre></div>
<div id="htmlwidget-31da950a69c1f1fe937a" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-31da950a69c1f1fe937a">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[140594.5875,101286.6867191,73807.3,54337.7457031,40785.1945314,31380.5322266,24898.7701174,20446.4482421,17473.6291017,15400.1719726,13967.5166992,12978.5578125,12269.121289,11734.381543,11249.5087889,10852.1914063,10512.896875,10225.8342773,9994.7419921,9751.9200196,9519.4708007,9315.4802735,9117.5376953,8910.4475586,8750.9584959,8593.2,8448.3758789,8278.2055175,8106.6850098,7939.7208984,7771.9758787,7613.2281738,7462.2565919,7337.0040038,7215.4793945,7085.5856446,6910.9421874,6796.2040037,6677.9723143,6574.5823733,6457.2341309,6338.7859863,6233.3975586,6160.2472657,6055.0294922,5962.9359374,5841.8112305,5740.5357911,5645.0680177,5540.2312012,5444.6583007,5364.8390137,5288.4353515,5201.0837403,5128.1606934,5069.8740233,4996.0522463,4919.8980958,4822.187793,4739.6396485,4670.4356935,4613.4903321,4552.1994141,4489.4088377,4403.4343262,4304.159204,4231.0445556,4165.1487548,4090.1139892,4008.9814942,3944.6739745,3891.482251,3829.4938966,3761.4834472,3726.1802246,3671.3132325,3630.2940919,3580.0728026,3520.8024168,3471.8132812,3411.6532714,3349.0892334,3293.5904052,3253.8236084,3192.9784913,3148.837671,3108.568164,3052.2455811,3007.3829834,2954.0198731,2911.3963135,2867.2989746,2819.4181153,2776.9638428,2727.821582,2693.3881348,2663.0775635,2617.4836426,2573.2193848,2532.8037842,2499.63125,2462.9624513,2432.0398438,2401.1086181,2370.0609131,2341.9923708,2304.8312989,2267.9302124,2229.6167969,2202.0569825,2165.8003053,2138.5861695,2106.6026976,2074.371228,2047.8173338,2014.0363891,1993.7470093,1967.0413451,1941.7842408,1918.6677857,1894.6833861,1864.8548583,1835.1145141,1795.4808105,1763.295166,1738.6042603,1710.9897583,1687.8432373,1661.0685181,1631.4690186,1613.6443483,1588.7129151,1563.7790528,1531.9783691,1506.9308228,1487.3664307,1460.6477783,1441.3762329,1419.9146241,1397.0803223,1378.6616455,1361.2245117,1344.8429687,1329.2414795,1308.6594483,1282.1494265,1259.7858643,1240.2073486,1221.02854,1205.4603698,1190.8673888,1177.3331053,1162.7586548,1151.5119934,1140.4366943,1125.1754701,1107.6707276,1092.8359192,1075.6617982,1061.7394958,1044.5710511,1027.7472351,1014.5498597,1002.2487854,983.644104,969.4414,952.2909913,936.2260805,922.0340517,907.7742554,898.2338135,886.1614807,872.0424987,862.011737,847.1053468,835.2450867,826.1704223,817.2422912,807.9752991,794.2547242,783.8483766,773.7636108,760.4451111,749.0615723,738.7089599,728.3018922,718.2620666,709.0963683,697.2377988,688.6634644,678.9270875,668.3063109,658.0195071,649.9507569,639.8951108,631.664569,622.9105988,612.3728911,602.315384,596.963913],"text":["iter:   1<br />train_rmse_mean: 140594.5875","iter:   2<br />train_rmse_mean: 101286.6867","iter:   3<br />train_rmse_mean:  73807.3000","iter:   4<br />train_rmse_mean:  54337.7457","iter:   5<br />train_rmse_mean:  40785.1945","iter:   6<br />train_rmse_mean:  31380.5322","iter:   7<br />train_rmse_mean:  24898.7701","iter:   8<br />train_rmse_mean:  20446.4482","iter:   9<br />train_rmse_mean:  17473.6291","iter:  10<br />train_rmse_mean:  15400.1720","iter:  11<br />train_rmse_mean:  13967.5167","iter:  12<br />train_rmse_mean:  12978.5578","iter:  13<br />train_rmse_mean:  12269.1213","iter:  14<br />train_rmse_mean:  11734.3815","iter:  15<br />train_rmse_mean:  11249.5088","iter:  16<br />train_rmse_mean:  10852.1914","iter:  17<br />train_rmse_mean:  10512.8969","iter:  18<br />train_rmse_mean:  10225.8343","iter:  19<br />train_rmse_mean:   9994.7420","iter:  20<br />train_rmse_mean:   9751.9200","iter:  21<br />train_rmse_mean:   9519.4708","iter:  22<br />train_rmse_mean:   9315.4803","iter:  23<br />train_rmse_mean:   9117.5377","iter:  24<br />train_rmse_mean:   8910.4476","iter:  25<br />train_rmse_mean:   8750.9585","iter:  26<br />train_rmse_mean:   8593.2000","iter:  27<br />train_rmse_mean:   8448.3759","iter:  28<br />train_rmse_mean:   8278.2055","iter:  29<br />train_rmse_mean:   8106.6850","iter:  30<br />train_rmse_mean:   7939.7209","iter:  31<br />train_rmse_mean:   7771.9759","iter:  32<br />train_rmse_mean:   7613.2282","iter:  33<br />train_rmse_mean:   7462.2566","iter:  34<br />train_rmse_mean:   7337.0040","iter:  35<br />train_rmse_mean:   7215.4794","iter:  36<br />train_rmse_mean:   7085.5856","iter:  37<br />train_rmse_mean:   6910.9422","iter:  38<br />train_rmse_mean:   6796.2040","iter:  39<br />train_rmse_mean:   6677.9723","iter:  40<br />train_rmse_mean:   6574.5824","iter:  41<br />train_rmse_mean:   6457.2341","iter:  42<br />train_rmse_mean:   6338.7860","iter:  43<br />train_rmse_mean:   6233.3976","iter:  44<br />train_rmse_mean:   6160.2473","iter:  45<br />train_rmse_mean:   6055.0295","iter:  46<br />train_rmse_mean:   5962.9359","iter:  47<br />train_rmse_mean:   5841.8112","iter:  48<br />train_rmse_mean:   5740.5358","iter:  49<br />train_rmse_mean:   5645.0680","iter:  50<br />train_rmse_mean:   5540.2312","iter:  51<br />train_rmse_mean:   5444.6583","iter:  52<br />train_rmse_mean:   5364.8390","iter:  53<br />train_rmse_mean:   5288.4354","iter:  54<br />train_rmse_mean:   5201.0837","iter:  55<br />train_rmse_mean:   5128.1607","iter:  56<br />train_rmse_mean:   5069.8740","iter:  57<br />train_rmse_mean:   4996.0522","iter:  58<br />train_rmse_mean:   4919.8981","iter:  59<br />train_rmse_mean:   4822.1878","iter:  60<br />train_rmse_mean:   4739.6396","iter:  61<br />train_rmse_mean:   4670.4357","iter:  62<br />train_rmse_mean:   4613.4903","iter:  63<br />train_rmse_mean:   4552.1994","iter:  64<br />train_rmse_mean:   4489.4088","iter:  65<br />train_rmse_mean:   4403.4343","iter:  66<br />train_rmse_mean:   4304.1592","iter:  67<br />train_rmse_mean:   4231.0446","iter:  68<br />train_rmse_mean:   4165.1488","iter:  69<br />train_rmse_mean:   4090.1140","iter:  70<br />train_rmse_mean:   4008.9815","iter:  71<br />train_rmse_mean:   3944.6740","iter:  72<br />train_rmse_mean:   3891.4823","iter:  73<br />train_rmse_mean:   3829.4939","iter:  74<br />train_rmse_mean:   3761.4834","iter:  75<br />train_rmse_mean:   3726.1802","iter:  76<br />train_rmse_mean:   3671.3132","iter:  77<br />train_rmse_mean:   3630.2941","iter:  78<br />train_rmse_mean:   3580.0728","iter:  79<br />train_rmse_mean:   3520.8024","iter:  80<br />train_rmse_mean:   3471.8133","iter:  81<br />train_rmse_mean:   3411.6533","iter:  82<br />train_rmse_mean:   3349.0892","iter:  83<br />train_rmse_mean:   3293.5904","iter:  84<br />train_rmse_mean:   3253.8236","iter:  85<br />train_rmse_mean:   3192.9785","iter:  86<br />train_rmse_mean:   3148.8377","iter:  87<br />train_rmse_mean:   3108.5682","iter:  88<br />train_rmse_mean:   3052.2456","iter:  89<br />train_rmse_mean:   3007.3830","iter:  90<br />train_rmse_mean:   2954.0199","iter:  91<br />train_rmse_mean:   2911.3963","iter:  92<br />train_rmse_mean:   2867.2990","iter:  93<br />train_rmse_mean:   2819.4181","iter:  94<br />train_rmse_mean:   2776.9638","iter:  95<br />train_rmse_mean:   2727.8216","iter:  96<br />train_rmse_mean:   2693.3881","iter:  97<br />train_rmse_mean:   2663.0776","iter:  98<br />train_rmse_mean:   2617.4836","iter:  99<br />train_rmse_mean:   2573.2194","iter: 100<br />train_rmse_mean:   2532.8038","iter: 101<br />train_rmse_mean:   2499.6312","iter: 102<br />train_rmse_mean:   2462.9625","iter: 103<br />train_rmse_mean:   2432.0398","iter: 104<br />train_rmse_mean:   2401.1086","iter: 105<br />train_rmse_mean:   2370.0609","iter: 106<br />train_rmse_mean:   2341.9924","iter: 107<br />train_rmse_mean:   2304.8313","iter: 108<br />train_rmse_mean:   2267.9302","iter: 109<br />train_rmse_mean:   2229.6168","iter: 110<br />train_rmse_mean:   2202.0570","iter: 111<br />train_rmse_mean:   2165.8003","iter: 112<br />train_rmse_mean:   2138.5862","iter: 113<br />train_rmse_mean:   2106.6027","iter: 114<br />train_rmse_mean:   2074.3712","iter: 115<br />train_rmse_mean:   2047.8173","iter: 116<br />train_rmse_mean:   2014.0364","iter: 117<br />train_rmse_mean:   1993.7470","iter: 118<br />train_rmse_mean:   1967.0413","iter: 119<br />train_rmse_mean:   1941.7842","iter: 120<br />train_rmse_mean:   1918.6678","iter: 121<br />train_rmse_mean:   1894.6834","iter: 122<br />train_rmse_mean:   1864.8549","iter: 123<br />train_rmse_mean:   1835.1145","iter: 124<br />train_rmse_mean:   1795.4808","iter: 125<br />train_rmse_mean:   1763.2952","iter: 126<br />train_rmse_mean:   1738.6043","iter: 127<br />train_rmse_mean:   1710.9898","iter: 128<br />train_rmse_mean:   1687.8432","iter: 129<br />train_rmse_mean:   1661.0685","iter: 130<br />train_rmse_mean:   1631.4690","iter: 131<br />train_rmse_mean:   1613.6443","iter: 132<br />train_rmse_mean:   1588.7129","iter: 133<br />train_rmse_mean:   1563.7791","iter: 134<br />train_rmse_mean:   1531.9784","iter: 135<br />train_rmse_mean:   1506.9308","iter: 136<br />train_rmse_mean:   1487.3664","iter: 137<br />train_rmse_mean:   1460.6478","iter: 138<br />train_rmse_mean:   1441.3762","iter: 139<br />train_rmse_mean:   1419.9146","iter: 140<br />train_rmse_mean:   1397.0803","iter: 141<br />train_rmse_mean:   1378.6616","iter: 142<br />train_rmse_mean:   1361.2245","iter: 143<br />train_rmse_mean:   1344.8430","iter: 144<br />train_rmse_mean:   1329.2415","iter: 145<br />train_rmse_mean:   1308.6594","iter: 146<br />train_rmse_mean:   1282.1494","iter: 147<br />train_rmse_mean:   1259.7859","iter: 148<br />train_rmse_mean:   1240.2073","iter: 149<br />train_rmse_mean:   1221.0285","iter: 150<br />train_rmse_mean:   1205.4604","iter: 151<br />train_rmse_mean:   1190.8674","iter: 152<br />train_rmse_mean:   1177.3331","iter: 153<br />train_rmse_mean:   1162.7587","iter: 154<br />train_rmse_mean:   1151.5120","iter: 155<br />train_rmse_mean:   1140.4367","iter: 156<br />train_rmse_mean:   1125.1755","iter: 157<br />train_rmse_mean:   1107.6707","iter: 158<br />train_rmse_mean:   1092.8359","iter: 159<br />train_rmse_mean:   1075.6618","iter: 160<br />train_rmse_mean:   1061.7395","iter: 161<br />train_rmse_mean:   1044.5711","iter: 162<br />train_rmse_mean:   1027.7472","iter: 163<br />train_rmse_mean:   1014.5499","iter: 164<br />train_rmse_mean:   1002.2488","iter: 165<br />train_rmse_mean:    983.6441","iter: 166<br />train_rmse_mean:    969.4414","iter: 167<br />train_rmse_mean:    952.2910","iter: 168<br />train_rmse_mean:    936.2261","iter: 169<br />train_rmse_mean:    922.0341","iter: 170<br />train_rmse_mean:    907.7743","iter: 171<br />train_rmse_mean:    898.2338","iter: 172<br />train_rmse_mean:    886.1615","iter: 173<br />train_rmse_mean:    872.0425","iter: 174<br />train_rmse_mean:    862.0117","iter: 175<br />train_rmse_mean:    847.1053","iter: 176<br />train_rmse_mean:    835.2451","iter: 177<br />train_rmse_mean:    826.1704","iter: 178<br />train_rmse_mean:    817.2423","iter: 179<br />train_rmse_mean:    807.9753","iter: 180<br />train_rmse_mean:    794.2547","iter: 181<br />train_rmse_mean:    783.8484","iter: 182<br />train_rmse_mean:    773.7636","iter: 183<br />train_rmse_mean:    760.4451","iter: 184<br />train_rmse_mean:    749.0616","iter: 185<br />train_rmse_mean:    738.7090","iter: 186<br />train_rmse_mean:    728.3019","iter: 187<br />train_rmse_mean:    718.2621","iter: 188<br />train_rmse_mean:    709.0964","iter: 189<br />train_rmse_mean:    697.2378","iter: 190<br />train_rmse_mean:    688.6635","iter: 191<br />train_rmse_mean:    678.9271","iter: 192<br />train_rmse_mean:    668.3063","iter: 193<br />train_rmse_mean:    658.0195","iter: 194<br />train_rmse_mean:    649.9508","iter: 195<br />train_rmse_mean:    639.8951","iter: 196<br />train_rmse_mean:    631.6646","iter: 197<br />train_rmse_mean:    622.9106","iter: 198<br />train_rmse_mean:    612.3729","iter: 199<br />train_rmse_mean:    602.3154","iter: 200<br />train_rmse_mean:    596.9639"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[140830.0953125,102345.2242188,75785.7109374,57672.9679688,45915.3250002,38311.3929687,33616.81875,30879.5783202,29185.3003906,28172.7208985,27607.0392578,27279.8892578,27061.8238281,26881.2216797,26793.7441406,26636.1140624,26526.2492185,26461.0822264,26375.2812501,26277.4503905,26193.3628907,26127.6947265,26053.3457031,26004.0568359,25962.8052734,25943.5228515,25885.6171876,25876.8292969,25862.4749998,25857.2537109,25851.8718751,25804.616992,25744.504492,25729.4595704,25712.5009763,25687.5533203,25652.7568361,25644.3392578,25622.3546876,25614.5439454,25587.1650391,25588.391797,25553.5650392,25563.8392578,25565.5976563,25558.894922,25559.4990234,25530.3472655,25533.0353515,25538.9937499,25521.7060546,25510.8697266,25501.2828125,25491.6111329,25494.9960938,25482.5375,25471.4607422,25468.3007812,25475.3955078,25477.7279297,25466.0449218,25463.3248048,25463.736914,25464.519336,25450.4017579,25440.8671874,25428.340039,25427.5576172,25416.8115233,25423.8158204,25421.641211,25434.1548829,25437.7242188,25419.9605468,25420.5507813,25410.667578,25407.8671876,25412.2046875,25400.1412109,25398.93125,25388.6585936,25387.0607422,25393.4564452,25380.0558592,25385.3869141,25393.2226562,25389.4341797,25386.0214846,25382.3292969,25379.0605469,25377.0248046,25374.8542967,25370.3861328,25361.2628907,25357.2740234,25356.1662108,25359.5531249,25354.3244142,25350.5220703,25350.6003907,25355.1398438,25350.8304687,25349.5009766,25350.7478516,25349.5787109,25351.8269532,25348.1710939,25344.3224608,25349.6767579,25346.9449221,25340.8808594,25343.3119139,25344.7367188,25345.3154296,25347.800586,25343.7220703,25344.9958983,25347.4572266,25347.4783203,25347.051953,25348.3916016,25350.9447264,25347.7277343,25349.5507813,25345.4978514,25342.9314454,25344.1292969,25343.3724609,25342.9529298,25336.3382812,25337.2623048,25336.9708984,25337.0501952,25335.7666016,25333.3785157,25332.696289,25332.6226562,25330.8494141,25330.9353516,25326.6519532,25326.8830078,25326.2708984,25327.1251953,25326.4398438,25326.8103516,25324.4328124,25327.0125001,25327.3517578,25328.8876954,25330.1875001,25331.8197265,25333.0693358,25332.1884766,25331.2546874,25330.8000001,25334.4599608,25334.1425781,25333.5736329,25334.2324219,25330.0503907,25328.7501951,25328.5691407,25328.2818359,25330.2097656,25329.8849611,25329.3031252,25331.8753905,25330.8140626,25329.659961,25329.7890625,25330.2810547,25330.6986328,25329.5765624,25329.8845702,25329.5523437,25327.6298828,25325.6216797,25324.7634766,25326.0646485,25324.1212892,25325.8197266,25325.7359375,25325.784375,25326.0166017,25326.1861327,25326.2892577,25327.3462891,25328.4289063,25329.7216797,25327.997656,25327.6833983,25329.2513673,25330.9572266,25330.2984375,25328.8724608,25328.0292969,25328.3859376,25328.9691408,25326.9431643,25327.2769531],"text":["iter:   1<br />test_rmse_mean: 140830.10","iter:   2<br />test_rmse_mean: 102345.22","iter:   3<br />test_rmse_mean:  75785.71","iter:   4<br />test_rmse_mean:  57672.97","iter:   5<br />test_rmse_mean:  45915.33","iter:   6<br />test_rmse_mean:  38311.39","iter:   7<br />test_rmse_mean:  33616.82","iter:   8<br />test_rmse_mean:  30879.58","iter:   9<br />test_rmse_mean:  29185.30","iter:  10<br />test_rmse_mean:  28172.72","iter:  11<br />test_rmse_mean:  27607.04","iter:  12<br />test_rmse_mean:  27279.89","iter:  13<br />test_rmse_mean:  27061.82","iter:  14<br />test_rmse_mean:  26881.22","iter:  15<br />test_rmse_mean:  26793.74","iter:  16<br />test_rmse_mean:  26636.11","iter:  17<br />test_rmse_mean:  26526.25","iter:  18<br />test_rmse_mean:  26461.08","iter:  19<br />test_rmse_mean:  26375.28","iter:  20<br />test_rmse_mean:  26277.45","iter:  21<br />test_rmse_mean:  26193.36","iter:  22<br />test_rmse_mean:  26127.69","iter:  23<br />test_rmse_mean:  26053.35","iter:  24<br />test_rmse_mean:  26004.06","iter:  25<br />test_rmse_mean:  25962.81","iter:  26<br />test_rmse_mean:  25943.52","iter:  27<br />test_rmse_mean:  25885.62","iter:  28<br />test_rmse_mean:  25876.83","iter:  29<br />test_rmse_mean:  25862.47","iter:  30<br />test_rmse_mean:  25857.25","iter:  31<br />test_rmse_mean:  25851.87","iter:  32<br />test_rmse_mean:  25804.62","iter:  33<br />test_rmse_mean:  25744.50","iter:  34<br />test_rmse_mean:  25729.46","iter:  35<br />test_rmse_mean:  25712.50","iter:  36<br />test_rmse_mean:  25687.55","iter:  37<br />test_rmse_mean:  25652.76","iter:  38<br />test_rmse_mean:  25644.34","iter:  39<br />test_rmse_mean:  25622.35","iter:  40<br />test_rmse_mean:  25614.54","iter:  41<br />test_rmse_mean:  25587.17","iter:  42<br />test_rmse_mean:  25588.39","iter:  43<br />test_rmse_mean:  25553.57","iter:  44<br />test_rmse_mean:  25563.84","iter:  45<br />test_rmse_mean:  25565.60","iter:  46<br />test_rmse_mean:  25558.89","iter:  47<br />test_rmse_mean:  25559.50","iter:  48<br />test_rmse_mean:  25530.35","iter:  49<br />test_rmse_mean:  25533.04","iter:  50<br />test_rmse_mean:  25538.99","iter:  51<br />test_rmse_mean:  25521.71","iter:  52<br />test_rmse_mean:  25510.87","iter:  53<br />test_rmse_mean:  25501.28","iter:  54<br />test_rmse_mean:  25491.61","iter:  55<br />test_rmse_mean:  25495.00","iter:  56<br />test_rmse_mean:  25482.54","iter:  57<br />test_rmse_mean:  25471.46","iter:  58<br />test_rmse_mean:  25468.30","iter:  59<br />test_rmse_mean:  25475.40","iter:  60<br />test_rmse_mean:  25477.73","iter:  61<br />test_rmse_mean:  25466.04","iter:  62<br />test_rmse_mean:  25463.32","iter:  63<br />test_rmse_mean:  25463.74","iter:  64<br />test_rmse_mean:  25464.52","iter:  65<br />test_rmse_mean:  25450.40","iter:  66<br />test_rmse_mean:  25440.87","iter:  67<br />test_rmse_mean:  25428.34","iter:  68<br />test_rmse_mean:  25427.56","iter:  69<br />test_rmse_mean:  25416.81","iter:  70<br />test_rmse_mean:  25423.82","iter:  71<br />test_rmse_mean:  25421.64","iter:  72<br />test_rmse_mean:  25434.15","iter:  73<br />test_rmse_mean:  25437.72","iter:  74<br />test_rmse_mean:  25419.96","iter:  75<br />test_rmse_mean:  25420.55","iter:  76<br />test_rmse_mean:  25410.67","iter:  77<br />test_rmse_mean:  25407.87","iter:  78<br />test_rmse_mean:  25412.20","iter:  79<br />test_rmse_mean:  25400.14","iter:  80<br />test_rmse_mean:  25398.93","iter:  81<br />test_rmse_mean:  25388.66","iter:  82<br />test_rmse_mean:  25387.06","iter:  83<br />test_rmse_mean:  25393.46","iter:  84<br />test_rmse_mean:  25380.06","iter:  85<br />test_rmse_mean:  25385.39","iter:  86<br />test_rmse_mean:  25393.22","iter:  87<br />test_rmse_mean:  25389.43","iter:  88<br />test_rmse_mean:  25386.02","iter:  89<br />test_rmse_mean:  25382.33","iter:  90<br />test_rmse_mean:  25379.06","iter:  91<br />test_rmse_mean:  25377.02","iter:  92<br />test_rmse_mean:  25374.85","iter:  93<br />test_rmse_mean:  25370.39","iter:  94<br />test_rmse_mean:  25361.26","iter:  95<br />test_rmse_mean:  25357.27","iter:  96<br />test_rmse_mean:  25356.17","iter:  97<br />test_rmse_mean:  25359.55","iter:  98<br />test_rmse_mean:  25354.32","iter:  99<br />test_rmse_mean:  25350.52","iter: 100<br />test_rmse_mean:  25350.60","iter: 101<br />test_rmse_mean:  25355.14","iter: 102<br />test_rmse_mean:  25350.83","iter: 103<br />test_rmse_mean:  25349.50","iter: 104<br />test_rmse_mean:  25350.75","iter: 105<br />test_rmse_mean:  25349.58","iter: 106<br />test_rmse_mean:  25351.83","iter: 107<br />test_rmse_mean:  25348.17","iter: 108<br />test_rmse_mean:  25344.32","iter: 109<br />test_rmse_mean:  25349.68","iter: 110<br />test_rmse_mean:  25346.94","iter: 111<br />test_rmse_mean:  25340.88","iter: 112<br />test_rmse_mean:  25343.31","iter: 113<br />test_rmse_mean:  25344.74","iter: 114<br />test_rmse_mean:  25345.32","iter: 115<br />test_rmse_mean:  25347.80","iter: 116<br />test_rmse_mean:  25343.72","iter: 117<br />test_rmse_mean:  25345.00","iter: 118<br />test_rmse_mean:  25347.46","iter: 119<br />test_rmse_mean:  25347.48","iter: 120<br />test_rmse_mean:  25347.05","iter: 121<br />test_rmse_mean:  25348.39","iter: 122<br />test_rmse_mean:  25350.94","iter: 123<br />test_rmse_mean:  25347.73","iter: 124<br />test_rmse_mean:  25349.55","iter: 125<br />test_rmse_mean:  25345.50","iter: 126<br />test_rmse_mean:  25342.93","iter: 127<br />test_rmse_mean:  25344.13","iter: 128<br />test_rmse_mean:  25343.37","iter: 129<br />test_rmse_mean:  25342.95","iter: 130<br />test_rmse_mean:  25336.34","iter: 131<br />test_rmse_mean:  25337.26","iter: 132<br />test_rmse_mean:  25336.97","iter: 133<br />test_rmse_mean:  25337.05","iter: 134<br />test_rmse_mean:  25335.77","iter: 135<br />test_rmse_mean:  25333.38","iter: 136<br />test_rmse_mean:  25332.70","iter: 137<br />test_rmse_mean:  25332.62","iter: 138<br />test_rmse_mean:  25330.85","iter: 139<br />test_rmse_mean:  25330.94","iter: 140<br />test_rmse_mean:  25326.65","iter: 141<br />test_rmse_mean:  25326.88","iter: 142<br />test_rmse_mean:  25326.27","iter: 143<br />test_rmse_mean:  25327.13","iter: 144<br />test_rmse_mean:  25326.44","iter: 145<br />test_rmse_mean:  25326.81","iter: 146<br />test_rmse_mean:  25324.43","iter: 147<br />test_rmse_mean:  25327.01","iter: 148<br />test_rmse_mean:  25327.35","iter: 149<br />test_rmse_mean:  25328.89","iter: 150<br />test_rmse_mean:  25330.19","iter: 151<br />test_rmse_mean:  25331.82","iter: 152<br />test_rmse_mean:  25333.07","iter: 153<br />test_rmse_mean:  25332.19","iter: 154<br />test_rmse_mean:  25331.25","iter: 155<br />test_rmse_mean:  25330.80","iter: 156<br />test_rmse_mean:  25334.46","iter: 157<br />test_rmse_mean:  25334.14","iter: 158<br />test_rmse_mean:  25333.57","iter: 159<br />test_rmse_mean:  25334.23","iter: 160<br />test_rmse_mean:  25330.05","iter: 161<br />test_rmse_mean:  25328.75","iter: 162<br />test_rmse_mean:  25328.57","iter: 163<br />test_rmse_mean:  25328.28","iter: 164<br />test_rmse_mean:  25330.21","iter: 165<br />test_rmse_mean:  25329.88","iter: 166<br />test_rmse_mean:  25329.30","iter: 167<br />test_rmse_mean:  25331.88","iter: 168<br />test_rmse_mean:  25330.81","iter: 169<br />test_rmse_mean:  25329.66","iter: 170<br />test_rmse_mean:  25329.79","iter: 171<br />test_rmse_mean:  25330.28","iter: 172<br />test_rmse_mean:  25330.70","iter: 173<br />test_rmse_mean:  25329.58","iter: 174<br />test_rmse_mean:  25329.88","iter: 175<br />test_rmse_mean:  25329.55","iter: 176<br />test_rmse_mean:  25327.63","iter: 177<br />test_rmse_mean:  25325.62","iter: 178<br />test_rmse_mean:  25324.76","iter: 179<br />test_rmse_mean:  25326.06","iter: 180<br />test_rmse_mean:  25324.12","iter: 181<br />test_rmse_mean:  25325.82","iter: 182<br />test_rmse_mean:  25325.74","iter: 183<br />test_rmse_mean:  25325.78","iter: 184<br />test_rmse_mean:  25326.02","iter: 185<br />test_rmse_mean:  25326.19","iter: 186<br />test_rmse_mean:  25326.29","iter: 187<br />test_rmse_mean:  25327.35","iter: 188<br />test_rmse_mean:  25328.43","iter: 189<br />test_rmse_mean:  25329.72","iter: 190<br />test_rmse_mean:  25328.00","iter: 191<br />test_rmse_mean:  25327.68","iter: 192<br />test_rmse_mean:  25329.25","iter: 193<br />test_rmse_mean:  25330.96","iter: 194<br />test_rmse_mean:  25330.30","iter: 195<br />test_rmse_mean:  25328.87","iter: 196<br />test_rmse_mean:  25328.03","iter: 197<br />test_rmse_mean:  25328.39","iter: 198<br />test_rmse_mean:  25328.97","iter: 199<br />test_rmse_mean:  25326.94","iter: 200<br />test_rmse_mean:  25327.28"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"10-fold Cross-validation","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-8.95,209.95],"tickmode":"array","ticktext":["0","50","100","150","200"],"tickvals":[0,50,100,150,200],"categoryorder":"array","categoryarray":["0","50","100","150","200"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Iteration (round)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-6414.692656975,147841.751882475],"tickmode":"array","ticktext":["0","50000","100000"],"tickvals":[0,50000,100000],"categoryorder":"array","categoryarray":["0","50000","100000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Root Mean Square Error","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"17488230a4":{"x":{},"y":{},"type":"scatter"},"174851c26ed5":{"x":{},"y":{}}},"cur_data":"17488230a4","visdat":{"17488230a4":["function (y) ","x"],"174851c26ed5":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The gap between the blue and the red line in the graph above depicts the performance difference of our model when fitted to the seen data (the red line) and when fitted to the unseen data (the blue line).
Our goal is to make our model perform with unseen data as good as possible, i.e. to minimize RMSE as much as possible.</p>
<p>To pursue that goal, parameters, that we dicussed earlier, need to be as optimally tuned as possible.
Therefore, some parameters should be adapted. For the following cross-validation process, we will slightly adapt parameters:</p>
<ol style="list-style-type: decimal">
<li>Decrease the learning rate <code>eta</code> from 0.3 to 0.03</li>
<li>Set <code>early_stopping_rounds</code> at 50</li>
<li>Reduce maximum tree depth <code>max_depth</code> to 3</li>
<li>Increase minimum number of observations required in each terminal node <code>min_child_weight</code> to 3</li>
<li>Reduce <code>subsample</code> to 0.5</li>
<li><code>colsample_bytree</code> reduced to 0.5</li>
</ol>
<div class="sourceCode" id="cb925"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb925-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb925-2" title="2">ames_xgb1 &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</a>
<a class="sourceLine" id="cb925-3" title="3">  <span class="dt">booster =</span> <span class="st">&quot;gbtree&quot;</span>,</a>
<a class="sourceLine" id="cb925-4" title="4">  <span class="dt">data =</span> ames_x_train,           <span class="co"># matrix with train data without sale price</span></a>
<a class="sourceLine" id="cb925-5" title="5">  <span class="dt">label =</span> ames_y_train,          <span class="co"># numerical vector with sale price with train data </span></a>
<a class="sourceLine" id="cb925-6" title="6">  <span class="dt">nrounds =</span> <span class="dv">2301</span>,                <span class="co"># numer of iterations </span></a>
<a class="sourceLine" id="cb925-7" title="7">  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,      <span class="co"># indicating regression model</span></a>
<a class="sourceLine" id="cb925-8" title="8">  <span class="dt">early_stopping_rounds =</span> <span class="dv">50</span>,    <span class="co"># stopping the training model as soon as evaluation metric (RMSE) does not improve for a given number of rounds</span></a>
<a class="sourceLine" id="cb925-9" title="9">  <span class="dt">nfold =</span> <span class="dv">10</span>,                     <span class="co"># data is randomly partitioned into nfold equal size subsamples</span></a>
<a class="sourceLine" id="cb925-10" title="10">  <span class="dt">params =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb925-11" title="11">    <span class="dt">eta =</span> <span class="fl">0.03</span>,                  <span class="co"># learning rate </span></a>
<a class="sourceLine" id="cb925-12" title="12">    <span class="dt">max_depth =</span> <span class="dv">3</span>,               <span class="co"># maximal depth of tree</span></a>
<a class="sourceLine" id="cb925-13" title="13">    <span class="dt">min_child_weight =</span> <span class="dv">3</span>,        <span class="co"># minimum number of observations required in each terminal node</span></a>
<a class="sourceLine" id="cb925-14" title="14">    <span class="dt">subsample =</span> <span class="fl">0.5</span>,             <span class="co"># percent of training data to sample for each tree</span></a>
<a class="sourceLine" id="cb925-15" title="15">    <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>       <span class="co"># percent of columns to sample from for each tree</span></a>
<a class="sourceLine" id="cb925-16" title="16">    ),</a>
<a class="sourceLine" id="cb925-17" title="17">  <span class="dt">verbose =</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb925-18" title="18">) </a></code></pre></div>
<pre><code>## [12:36:55] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:55] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:55] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:56] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:56] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:56] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:56] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:56] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:56] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:36:56] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.</code></pre>
<div class="sourceCode" id="cb927"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb927-1" title="1"><span class="co"># Checking results</span></a>
<a class="sourceLine" id="cb927-2" title="2">(eval1&lt;-ames_xgb1<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb927-3" title="3"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(</a>
<a class="sourceLine" id="cb927-4" title="4">    <span class="dt">ntrees.train =</span> <span class="kw">which</span>(train_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(train_rmse_mean))[<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb927-5" title="5">    <span class="dt">rmse.train   =</span> <span class="kw">min</span>(train_rmse_mean),</a>
<a class="sourceLine" id="cb927-6" title="6">    <span class="dt">ntrees.test  =</span> <span class="kw">which</span>(test_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(test_rmse_mean))[<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb927-7" title="7">    <span class="dt">rmse.test   =</span> <span class="kw">min</span>(test_rmse_mean),))</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["ntrees.train"],"name":[1],"type":["int"],"align":["right"]},{"label":["rmse.train"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["ntrees.test"],"name":[3],"type":["int"],"align":["right"]},{"label":["rmse.test"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"1543","2":"8590.856","3":"1493","4":"22234.75"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb928"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb928-1" title="1"><span class="co"># Plot error vs number trees</span></a>
<a class="sourceLine" id="cb928-2" title="2">pr1 &lt;-<span class="kw">ggplot</span>(ames_xgb1<span class="op">$</span>evaluation_log) <span class="op">+</span></a>
<a class="sourceLine" id="cb928-3" title="3"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, train_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb928-4" title="4"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, test_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb928-5" title="5"><span class="kw">ggplotly</span>(pr1)</a></code></pre></div>
<div id="htmlwidget-effa02e75c1ea5d42134" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-effa02e75c1ea5d42134">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543],"y":[191294.8015625,185961.0125,180770.8796875,175750.334375,170853.221875,166114.0859375,161539.753125,157081.2875,152774.1890625,148587.3734375,144523.0953125,140600.3515625,136796.69375,133102.5796875,129534.8984376,126069.28125,122686.5328126,119424.10625,116260.1617188,113185.8039063,110198.5492186,107319.7054688,104499.7289062,101781.2093749,99150.6804688,96598.428125,94122.2078125,91744.9671876,89440.1156251,87187.3007815,85017.8578126,82923.99375,80895.0664063,78920.7289062,76989.3187501,75129.7476562,73340.0624999,71600.4742188,69938.2195311,68317.2914062,66721.5109376,65192.841797,63727.825,62310.2640626,60925.9968751,59568.348828,58285.4039062,57046.9023437,55845.7582031,54666.1941406,53560.4839844,52484.3234375,51440.8800782,50449.8289061,49473.4093748,48530.3214843,47630.6488282,46735.4753907,45881.961328,45057.2257813,44243.8710938,43474.3925781,42742.1289063,42004.2488283,41297.0414062,40629.1367187,39992.1964844,39363.3851561,38770.7792968,38194.0535157,37634.0734375,37093.2921875,36565.3890624,36064.5984377,35569.8777344,35106.179297,34661.7140624,34226.6925783,33794.754297,33402.798828,33012.0341797,32639.5023437,32267.284961,31928.889453,31604.0960938,31267.3244141,30955.9505858,30651.5345703,30351.4457031,30064.3455079,29796.4916015,29525.3023437,29271.5853515,29023.8546876,28785.8523437,28551.863086,28328.1800781,28111.6845703,27893.4761718,27693.1521483,27497.4037109,27312.865039,27129.2339845,26950.9882811,26782.4419921,26611.8167967,26444.2347656,26290.9486327,26132.1765626,25988.5861328,25836.2925781,25697.1677733,25556.4521486,25423.4511719,25291.6849609,25149.0617188,25017.5625001,24888.9111328,24775.3478514,24656.9380859,24543.7652344,24428.0529298,24308.7529297,24204.8679687,24088.0966795,23990.5123046,23901.5583985,23802.3644531,23713.9515626,23624.598047,23536.2529297,23449.9351562,23371.8109376,23280.6457031,23192.8875,23110.1203126,23029.8283203,22944.7900391,22873.9183593,22797.5648437,22724.8103515,22657.2119143,22591.2,22518.6833984,22451.8949219,22387.5351563,22327.4167969,22258.0548827,22193.5251953,22127.0617188,22069.6785156,22008.6787108,21952.9195312,21905.503125,21855.1777343,21790.6056641,21733.1134765,21671.1955079,21620.9246095,21565.8337892,21509.9279296,21458.3484375,21407.7876953,21361.4718749,21308.7824219,21259.071289,21213.5285156,21169.8548827,21119.4601561,21064.3863281,21014.6091796,20963.1240233,20919.9916014,20870.0923829,20830.0115235,20783.9154296,20746.8398438,20704.3173829,20662.7873048,20625.8449219,20580.8058593,20537.7458985,20494.7398437,20454.0048827,20412.4271485,20371.4308594,20337.4052734,20295.7884765,20255.0458985,20210.6095705,20172.3603515,20133.4292967,20099.4574219,20064.5902345,20033.1421875,19996.0753905,19956.1607422,19921.3648437,19886.2228516,19849.4623046,19816.8789064,19778.8654296,19747.7205078,19713.4748047,19679.3234375,19647.1392579,19612.7039061,19575.5136719,19539.7945313,19507.1109375,19477.5021484,19445.8121094,19414.5455077,19384.0501953,19350.7572267,19320.4935546,19290.5689451,19266.4341799,19235.3982423,19199.3490234,19167.5355469,19139.6341795,19110.0267577,19079.7289063,19047.2179686,19020.4400391,18987.8273438,18956.9273437,18934.3931641,18903.5664063,18876.7810547,18852.6220702,18824.5521485,18796.1970703,18767.9410157,18739.3804688,18708.9886718,18681.5117187,18656.5173828,18634.7515626,18606.2394531,18584.7625,18558.6230469,18531.6300781,18506.3466797,18481.4097657,18457.4033203,18431.0531249,18401.9021484,18378.5167968,18352.0132815,18326.2341797,18295.6320313,18272.3757812,18249.7779297,18228.9412111,18204.8072268,18185.6689456,18164.7291016,18143.71875,18117.1150391,18093.7699219,18069.0986329,18050.3609376,18027.4873047,18002.4691405,17978.3005859,17955.1160158,17933.3539061,17909.9384766,17890.9494142,17873.703125,17851.2193358,17826.0755859,17805.9105468,17787.1611328,17767.3195312,17744.9777343,17719.7550781,17697.5945312,17673.5103517,17653.2226564,17635.9212891,17615.8884766,17596.9056641,17572.8064453,17553.9005859,17535.6683593,17511.6146485,17491.0160156,17472.8607422,17452.7488281,17435.3566407,17413.7917969,17393.6386719,17376.158203,17357.3970702,17340.8839845,17319.8132813,17302.9837892,17285.4541016,17262.6455077,17244.6134766,17228.9203125,17207.4236328,17188.7126952,17171.1097656,17151.9507814,17133.3873046,17115.3949218,17095.6460938,17076.215625,17054.4039061,17036.6529296,17016.8603515,17002.3154296,16984.4214844,16966.7490234,16947.7015625,16932.844336,16915.8835937,16900.4445311,16884.25625,16863.469336,16846.606836,16827.7373047,16808.3601562,16786.8576171,16767.8607422,16752.1023438,16736.0474609,16719.2802733,16701.3865235,16686.5947266,16672.7216797,16657.0457031,16642.9060547,16625.2214844,16606.0283204,16588.6513671,16571.3417967,16556.7813476,16542.0517578,16524.9926759,16510.1480468,16493.6381836,16478.5226563,16462.535254,16444.2535157,16429.2064454,16411.5376953,16397.2722657,16381.8916015,16366.9012695,16351.7348633,16336.9347657,16322.1988281,16307.4634766,16292.4909179,16275.091211,16261.3919923,16246.5427733,16231.1024411,16213.9438477,16195.230078,16181.141504,16165.1499023,16150.1512696,16135.0062501,16117.3119141,16101.2144532,16085.6480469,16073.5541993,16058.5094727,16044.7789063,16029.3337892,16015.6740234,16001.2018555,15985.921289,15973.6003906,15957.3056643,15943.9277345,15930.7331054,15915.2743164,15901.5453124,15889.3386718,15876.2331055,15860.4807617,15843.438379,15829.5122071,15816.1324219,15804.6830078,15789.4573243,15772.7398437,15756.8983399,15745.0339843,15730.8567382,15715.762793,15702.6845703,15690.3996094,15678.6833983,15664.6662108,15651.9063477,15640.5083985,15626.3000976,15613.0925782,15599.8353515,15587.8705077,15575.5129882,15559.7961913,15548.4674806,15538.502539,15526.761621,15515.2388673,15499.7798829,15487.1177734,15472.8640626,15458.5345703,15444.8174805,15431.0012697,15415.0592775,15402.6455079,15390.1493162,15377.2824219,15363.9868162,15350.4738282,15337.4889647,15325.3389648,15310.5776366,15299.7007812,15286.2146485,15273.6110351,15263.409082,15251.0680664,15235.5873047,15223.0032226,15208.140918,15194.6354493,15177.6206055,15167.1980469,15154.8549806,15142.4911133,15130.6714843,15118.9728516,15106.9779295,15094.2155274,15081.8463867,15070.6941406,15059.2316406,15047.0280273,15035.6704102,15022.7301759,15010.178125,14998.0140625,14984.6619141,14971.0799804,14959.3557616,14944.9948244,14931.8804687,14920.3166992,14909.5166015,14900.685547,14890.5547852,14876.5589844,14866.3148435,14853.9385742,14839.5845703,14827.3083008,14815.555469,14804.1783204,14794.2499023,14784.2141602,14773.4224612,14762.3048828,14751.4657226,14742.3570313,14730.6210937,14719.716211,14710.0490235,14697.9642579,14686.7802736,14674.9978515,14664.825879,14653.1288087,14642.4948243,14632.1963866,14623.2674804,14613.3136717,14602.8690429,14590.9708984,14580.3768553,14570.5557618,14559.1524413,14546.6663086,14537.6194334,14527.6539063,14516.8613283,14505.534668,14497.0871094,14487.6975585,14477.5063477,14468.8482423,14458.6787109,14447.8260743,14436.6581055,14426.5569338,14416.6762696,14406.3047851,14396.1097657,14386.2699219,14376.0128906,14365.1458008,14355.1595704,14343.9836914,14334.452832,14322.6139649,14312.8571289,14302.3420897,14291.1835938,14282.0164062,14271.837793,14262.1348634,14251.3614258,14242.8661132,14231.9113282,14224.0058593,14213.6858399,14200.7884767,14190.290625,14179.4528319,14168.2634766,14158.4632812,14148.3921874,14137.1424804,14126.9915038,14119.4704101,14108.6429687,14095.1923828,14085.0990235,14075.8311525,14066.5156249,14056.912207,14046.3808594,14033.887207,14024.2708984,14014.4995117,14005.5012694,13996.1983399,13985.9266603,13977.2456055,13967.669336,13958.0503907,13947.0227539,13938.3876953,13929.2837891,13919.6487303,13910.5216796,13900.4884766,13892.7043944,13882.8579099,13874.4839844,13864.7265625,13855.7155273,13846.0665038,13836.60459,13828.4004882,13818.8337889,13808.3830078,13799.3128906,13790.7416017,13782.0322266,13773.617578,13763.6140624,13753.8976563,13744.8883788,13737.2296876,13728.5781249,13720.19834,13710.9725587,13701.559668,13690.22334,13680.6519531,13670.9555663,13662.0017577,13651.747754,13642.4598633,13633.4203126,13625.4829102,13617.6753906,13607.5233398,13598.2543946,13588.9717773,13579.4206055,13572.4649413,13565.6723633,13556.8095705,13549.1841799,13539.7050782,13530.2496094,13522.3993163,13514.9629882,13505.2463866,13496.4894532,13488.1995117,13479.741211,13471.2960937,13464.555078,13454.2655273,13447.5799805,13439.2697267,13429.6587891,13420.5532226,13412.4221679,13402.987793,13393.5249023,13382.8380859,13374.1620117,13365.7183594,13359.1055663,13351.0001954,13342.6975586,13334.4097655,13325.5032227,13317.7467774,13310.8920898,13301.2228516,13293.3602538,13284.7588868,13276.9352539,13268.7887695,13260.4797852,13252.292578,13242.2781249,13235.6633788,13225.5715822,13216.7447266,13210.184668,13202.2039063,13194.465332,13187.7050782,13179.7738283,13171.393457,13162.2225587,13153.0666015,13143.9557617,13134.8168945,13125.8965818,13116.3687501,13108.3887694,13098.9286131,13090.5632812,13082.8192383,13075.6430663,13068.6296874,13062.6264649,13054.1711915,13045.2017578,13037.6106445,13030.8789063,13023.4117188,13016.5150391,13008.9079101,13000.7756836,12991.6388672,12984.4539064,12976.2876952,12967.6498048,12960.3503905,12953.1555663,12945.4820313,12937.0074219,12928.5600585,12919.7573242,12912.648047,12906.0972655,12899.2912109,12891.4458009,12884.665625,12874.3397461,12867.1615234,12860.7509767,12853.5265626,12846.2679688,12837.5914061,12828.7495116,12823.6558592,12815.7982422,12808.082422,12800.0897463,12794.7879882,12787.4484375,12778.5432618,12772.0766602,12763.9499998,12758.9624023,12750.3280273,12742.4552734,12735.1866211,12727.7662109,12720.0083007,12712.5796873,12705.7648438,12697.8913086,12691.3530273,12683.8921873,12676.9985352,12668.8076172,12659.8292969,12653.0984377,12644.7476562,12638.2576171,12631.0589842,12625.0593749,12618.2589843,12611.7545898,12604.930664,12597.4420899,12589.6270509,12583.1831054,12575.4301758,12567.5038084,12562.045703,12555.746582,12548.5204102,12539.3346681,12532.9294922,12526.4239259,12519.4027345,12511.6666992,12504.3757814,12497.5290038,12490.0841798,12483.9353515,12475.9555665,12467.3719726,12460.8762695,12452.2848634,12445.5418945,12439.0537108,12429.7710938,12423.2371093,12416.2394531,12411.3932619,12403.7120118,12397.3804688,12390.9422852,12383.4597656,12375.4929687,12369.0018556,12361.1859375,12351.9212889,12345.3584961,12338.8216796,12332.0288087,12325.0174805,12319.5575195,12311.4964844,12305.2368165,12298.5294922,12290.8811525,12286.3508789,12278.4034179,12271.9697266,12266.1438476,12259.4788086,12252.7337892,12246.3072266,12237.2490235,12229.2998049,12222.542871,12216.1853515,12208.1371094,12201.8206055,12195.9339843,12189.4768554,12182.6512695,12175.4628907,12168.9410157,12160.6093749,12154.0265624,12146.5052735,12140.7619141,12134.7257813,12129.3669921,12124.2406249,12117.2459961,12112.1835938,12105.1345704,12098.8525391,12092.7634765,12085.7829101,12079.4588868,12073.5600586,12067.7302734,12060.5261719,12053.13916,12046.6726564,12039.8665038,12031.3793944,12024.3925782,12017.6920898,12011.5796875,12005.2359373,11998.1330077,11991.3004884,11983.0200196,11976.3229491,11969.7154297,11963.0549804,11955.4620118,11949.0388671,11942.0282227,11934.7521484,11928.2236327,11921.0685547,11915.1985352,11909.1462889,11902.4192383,11895.0513672,11889.0916994,11883.2028319,11877.1206055,11871.1580078,11864.8100586,11859.6132813,11852.5211911,11846.260254,11839.9541993,11832.6464843,11825.4845702,11820.2775391,11812.1885743,11806.4560545,11799.5186524,11793.1864259,11788.0185547,11782.131543,11775.7446289,11769.1301758,11763.6936524,11757.9137696,11751.8954102,11746.1751952,11742.3615234,11736.5342774,11730.4437501,11725.076074,11718.4039062,11713.0496094,11707.538672,11701.7013671,11696.5086915,11690.0750977,11683.9690429,11677.313086,11671.2197267,11666.0061524,11659.8738282,11653.9988281,11648.1370119,11642.3985351,11635.6019531,11627.8581053,11622.9352538,11616.0223632,11610.8614259,11604.6135743,11598.4801758,11592.6220702,11585.9441407,11580.1148438,11574.7202148,11568.8479493,11562.6821289,11556.1995117,11550.8870116,11545.0243164,11538.744336,11533.6095705,11526.8128907,11520.9661133,11515.2800783,11507.6491212,11501.9021484,11496.4296876,11490.4875977,11485.737207,11479.9400391,11473.3524414,11467.9554686,11462.7390625,11456.3077148,11449.5430665,11443.2930664,11437.3122072,11431.6128904,11426.0463869,11420.5092773,11414.2678712,11409.455957,11403.4707032,11397.2635741,11392.3939453,11385.3225587,11381.1552735,11374.903418,11369.7576172,11362.9549803,11357.4430664,11352.3405273,11345.9922851,11338.9310547,11333.0213867,11327.8116211,11323.4279298,11317.8745117,11312.2313476,11307.672461,11301.9963867,11297.2703126,11292.2084961,11284.4526369,11279.2106445,11275.057617,11269.521875,11263.5721678,11258.2988279,11253.2998047,11247.0135743,11241.3888672,11235.0524415,11228.6809571,11223.1700194,11218.5130861,11211.800293,11206.2901365,11201.1516602,11194.0642576,11189.4398438,11183.3955077,11177.5713868,11172.0118162,11167.1222658,11163.1666993,11156.9351563,11152.0861328,11146.4180665,11140.3873047,11134.0009766,11128.7739258,11123.3382812,11119.2646483,11114.2835937,11107.5797852,11101.9723634,11095.6987305,11090.4982422,11084.1277343,11077.9322265,11072.8228517,11066.9724609,11061.5345703,11057.1885743,11050.768457,11044.7624024,11040.2904295,11035.7953126,11031.2997069,11026.0607422,11021.9921874,11016.3541016,11012.3650391,11008.037793,11002.5632812,10996.2970703,10989.7798828,10985.2504882,10979.2951172,10973.5801758,10968.9855467,10963.3520507,10958.0501953,10952.7434571,10947.7261719,10943.3482422,10938.795996,10933.0789062,10926.7201172,10921.5837891,10915.4272462,10911.8201174,10906.9190429,10902.8955078,10896.1751952,10890.5515626,10885.3382814,10879.7181642,10874.2320311,10868.7552735,10861.9232422,10857.0878907,10851.2225586,10844.5211915,10839.0469727,10832.8126953,10828.1530273,10822.5229492,10816.559375,10811.7232422,10805.7146483,10799.6696289,10794.2746094,10787.8143556,10781.6922852,10776.7660158,10771.8811523,10766.9446289,10762.2091795,10756.8404297,10750.8216795,10746.2601563,10742.27959,10737.7331055,10732.0385742,10728.1601563,10722.4842773,10717.3551757,10711.2143555,10706.588379,10701.8030273,10697.2180664,10692.2984375,10687.7142579,10683.0902343,10677.1268554,10670.8831056,10666.4128906,10661.9513672,10656.9351564,10651.690039,10646.8958986,10642.1515625,10636.9902343,10631.0612305,10626.9351562,10622.4267578,10616.7415039,10611.0285156,10605.5778321,10600.7270507,10595.7810547,10590.1231447,10585.2041017,10579.8789063,10574.8994141,10569.914258,10564.6506835,10559.3901367,10553.970996,10549.978711,10546.1812499,10541.7076171,10535.787793,10530.7662109,10525.8829102,10520.4134767,10515.1262696,10510.2703125,10505.2854491,10499.28418,10495.422461,10490.23916,10485.5639649,10479.4858399,10474.3049804,10469.7744142,10464.4275391,10459.8291016,10454.0433594,10449.4298828,10444.6888671,10439.4762695,10435.9250976,10431.1560547,10425.7019531,10421.6021485,10416.8373048,10411.3034179,10406.8159179,10401.5530273,10395.8297852,10390.0300782,10384.2126952,10379.9058594,10375.6304687,10370.4367187,10365.1785155,10360.603711,10355.4974609,10350.4749023,10345.4746094,10341.8722657,10336.874707,10331.3578126,10327.6566405,10321.5512696,10318.4717775,10313.3616213,10309.5706056,10305.3051758,10299.5041993,10294.8710938,10289.964746,10285.6943358,10282.4720703,10278.7754882,10273.8275391,10268.9741211,10264.2645508,10259.6915039,10255.8754883,10251.6422853,10246.5337891,10241.2328125,10236.8533204,10232.323828,10227.7041018,10222.5635743,10218.1854492,10214.5936524,10209.4106445,10205.8212891,10200.812207,10196.6246094,10191.627832,10186.7210937,10181.1222657,10177.4830077,10173.4626953,10168.5193361,10164.1390625,10159.5411131,10154.2549803,10148.817578,10144.1207029,10139.4563476,10135.5590819,10130.2404297,10125.7997071,10120.4644531,10115.249414,10110.8972656,10104.9092772,10100.4581055,10094.1827149,10089.1464843,10085.8980468,10080.7262695,10077.1523438,10073.0443358,10068.1912108,10063.9158202,10059.9328125,10055.9863282,10052.3138673,10047.5546876,10043.172754,10038.8505859,10034.1640624,10028.4096679,10023.5450194,10018.622754,10013.234668,10008.8241211,10004.8875978,9999.5352539,9994.4110349,9989.8283204,9985.2556641,9980.6204101,9975.7390625,9971.5538086,9967.7903321,9962.5562499,9957.6490233,9952.8592773,9948.2644532,9944.4571288,9939.7084962,9935.638086,9931.4341799,9927.0620117,9922.2349608,9919.165332,9915.290625,9912.1852538,9908.1450195,9904.359961,9899.6023437,9895.9228516,9891.3624024,9887.5837892,9883.8943358,9879.4662109,9874.2368164,9869.0048828,9863.7794922,9859.5027344,9855.0097655,9851.0485351,9846.2780273,9841.9242188,9838.9548828,9833.756543,9831.1029297,9827.0296876,9823.3666015,9818.8302735,9814.2557618,9809.6155272,9804.2824217,9800.0896484,9795.5047852,9790.840039,9786.4882814,9782.8132813,9778.5849611,9774.3510744,9769.4060547,9765.8089843,9762.4274413,9758.1600585,9755.5753904,9751.5452149,9747.6900392,9743.8111328,9739.4262695,9735.6172853,9731.6226562,9727.2587891,9723.5320312,9719.3688476,9715.6410156,9712.6759766,9708.6838867,9704.3727538,9699.6719726,9696.0134767,9692.3760742,9687.8035158,9682.7481446,9678.786035,9675.0776369,9670.3700197,9667.094336,9663.8262696,9659.418164,9655.6911132,9652.298047,9647.3941408,9642.6637696,9638.8960937,9634.261035,9629.7208983,9625.86875,9621.7947266,9617.1954101,9613.0858398,9607.8311523,9602.7813477,9598.3081055,9593.7158203,9588.5182617,9584.555078,9579.5610351,9575.0615234,9571.7110353,9567.3668945,9563.0327149,9558.8629883,9554.8335938,9550.0549806,9545.8928712,9541.803418,9537.8526368,9533.0903319,9529.9951171,9526.0550781,9521.3526368,9517.4733398,9514.1804687,9510.7470703,9505.5124026,9501.9001954,9498.3238281,9493.9596679,9488.7584962,9483.9140624,9479.2557615,9475.581543,9471.6672852,9468.4257812,9464.3142578,9460.8625,9456.4625977,9451.8782227,9448.2904299,9443.7633789,9439.3907226,9436.1896484,9431.5694335,9427.9083984,9423.4132814,9418.9499023,9415.0029299,9410.3242187,9407.048242,9403.4400389,9399.1702148,9394.2109375,9390.6597657,9387.0273437,9382.5385742,9378.9641601,9374.6880859,9370.4962892,9366.8137694,9363.0707031,9359.0075194,9354.1426758,9350.1825196,9345.8742188,9342.7693359,9337.9612304,9334.4786133,9330.2263671,9327.01875,9323.7878905,9320.6505858,9317.2804688,9313.5192384,9310.5364257,9305.7564454,9302.1639649,9297.8984375,9293.4890625,9288.9290038,9285.1464845,9280.5624023,9277.3041015,9272.7329101,9268.9992186,9265.529785,9261.6009764,9257.6889648,9253.940332,9249.9578126,9246.930957,9243.2372071,9239.3846678,9234.7210936,9230.650586,9226.1368164,9221.8942383,9218.9249999,9215.076953,9211.5325198,9208.0016603,9203.8299806,9200.3841795,9196.2486328,9193.143457,9188.7779297,9185.2993165,9181.3881836,9177.9759766,9173.6772461,9169.7972657,9165.8006836,9162.4297852,9159.0733399,9154.6814454,9150.2826172,9146.9510743,9142.5733399,9138.6428712,9134.3797853,9131.0523437,9127.1472657,9124.2101563,9120.7983399,9115.9259766,9112.3166016,9108.8449217,9105.2438479,9101.2423827,9097.4930665,9093.4230469,9090.7329101,9086.8568359,9083.2528319,9079.7914064,9076.5456055,9072.5198242,9069.2322266,9066.2603515,9062.1627929,9058.5999025,9055.0854492,9051.4167968,9047.6246094,9043.564258,9039.1033205,9035.7077149,9032.2703126,9028.8371095,9025.0934571,9022.1108399,9018.475879,9015.5491212,9010.9904297,9006.9255859,9003.7205078,8999.9472656,8997.096875,8993.6012695,8990.0602538,8985.7138672,8981.4614256,8978.1429688,8975.4515625,8970.9716797,8967.8686526,8964.4614258,8961.53584,8958.229004,8954.8114258,8951.667578,8948.5464843,8945.4916992,8942.0803711,8938.4837891,8934.3181641,8931.052832,8927.4897463,8923.1449218,8920.3299806,8916.1420899,8912.6354492,8908.7064453,8905.1059571,8902.0401368,8898.6136718,8894.2463867,8890.4015626,8886.603418,8883.5088868,8879.5308594,8875.6242186,8872.2913085,8869.1013671,8864.8760741,8860.9306641,8857.0994141,8853.7844726,8850.6826172,8847.316797,8844.3918947,8840.7620117,8837.5198242,8834.6561524,8830.8000979,8827.2533204,8823.8086915,8820.3525392,8816.1901367,8812.7970704,8808.7820314,8805.9290037,8802.8326173,8799.5336916,8795.6391601,8791.5717773,8787.5932617,8784.4555665,8781.3950196,8778.2344726,8774.8852539,8771.5706054,8768.4386718,8765.8918944,8762.9514648,8759.7741211,8755.7959962,8752.6802733,8749.5709961,8746.2838868,8742.1664062,8738.886914,8735.6878907,8732.7392577,8729.0914065,8724.7858399,8721.6275392,8718.0688477,8714.4824219,8711.8026368,8708.5963868,8703.9561523,8700.5147461,8697.0915038,8693.830957,8690.0045899,8687.6671875,8684.3728515,8680.4115235,8677.0438475,8673.5166992,8670.3525389,8666.8451171,8663.4471681,8660.4815429,8656.7791015,8652.6545897,8649.3177734,8645.726465,8642.5900391,8638.8928712,8635.5688475,8631.4196291,8627.5719726,8624.6899413,8621.2669922,8618.7685549,8615.5313479,8612.9213867,8609.3118162,8606.290918,8603.0641603,8600.3303711,8597.3025389,8593.7857424,8590.8559571],"text":["iter:    1<br />train_rmse_mean: 191294.802","iter:    2<br />train_rmse_mean: 185961.013","iter:    3<br />train_rmse_mean: 180770.880","iter:    4<br />train_rmse_mean: 175750.334","iter:    5<br />train_rmse_mean: 170853.222","iter:    6<br />train_rmse_mean: 166114.086","iter:    7<br />train_rmse_mean: 161539.753","iter:    8<br />train_rmse_mean: 157081.288","iter:    9<br />train_rmse_mean: 152774.189","iter:   10<br />train_rmse_mean: 148587.373","iter:   11<br />train_rmse_mean: 144523.095","iter:   12<br />train_rmse_mean: 140600.352","iter:   13<br />train_rmse_mean: 136796.694","iter:   14<br />train_rmse_mean: 133102.580","iter:   15<br />train_rmse_mean: 129534.898","iter:   16<br />train_rmse_mean: 126069.281","iter:   17<br />train_rmse_mean: 122686.533","iter:   18<br />train_rmse_mean: 119424.106","iter:   19<br />train_rmse_mean: 116260.162","iter:   20<br />train_rmse_mean: 113185.804","iter:   21<br />train_rmse_mean: 110198.549","iter:   22<br />train_rmse_mean: 107319.705","iter:   23<br />train_rmse_mean: 104499.729","iter:   24<br />train_rmse_mean: 101781.209","iter:   25<br />train_rmse_mean:  99150.680","iter:   26<br />train_rmse_mean:  96598.428","iter:   27<br />train_rmse_mean:  94122.208","iter:   28<br />train_rmse_mean:  91744.967","iter:   29<br />train_rmse_mean:  89440.116","iter:   30<br />train_rmse_mean:  87187.301","iter:   31<br />train_rmse_mean:  85017.858","iter:   32<br />train_rmse_mean:  82923.994","iter:   33<br />train_rmse_mean:  80895.066","iter:   34<br />train_rmse_mean:  78920.729","iter:   35<br />train_rmse_mean:  76989.319","iter:   36<br />train_rmse_mean:  75129.748","iter:   37<br />train_rmse_mean:  73340.062","iter:   38<br />train_rmse_mean:  71600.474","iter:   39<br />train_rmse_mean:  69938.220","iter:   40<br />train_rmse_mean:  68317.291","iter:   41<br />train_rmse_mean:  66721.511","iter:   42<br />train_rmse_mean:  65192.842","iter:   43<br />train_rmse_mean:  63727.825","iter:   44<br />train_rmse_mean:  62310.264","iter:   45<br />train_rmse_mean:  60925.997","iter:   46<br />train_rmse_mean:  59568.349","iter:   47<br />train_rmse_mean:  58285.404","iter:   48<br />train_rmse_mean:  57046.902","iter:   49<br />train_rmse_mean:  55845.758","iter:   50<br />train_rmse_mean:  54666.194","iter:   51<br />train_rmse_mean:  53560.484","iter:   52<br />train_rmse_mean:  52484.323","iter:   53<br />train_rmse_mean:  51440.880","iter:   54<br />train_rmse_mean:  50449.829","iter:   55<br />train_rmse_mean:  49473.409","iter:   56<br />train_rmse_mean:  48530.321","iter:   57<br />train_rmse_mean:  47630.649","iter:   58<br />train_rmse_mean:  46735.475","iter:   59<br />train_rmse_mean:  45881.961","iter:   60<br />train_rmse_mean:  45057.226","iter:   61<br />train_rmse_mean:  44243.871","iter:   62<br />train_rmse_mean:  43474.393","iter:   63<br />train_rmse_mean:  42742.129","iter:   64<br />train_rmse_mean:  42004.249","iter:   65<br />train_rmse_mean:  41297.041","iter:   66<br />train_rmse_mean:  40629.137","iter:   67<br />train_rmse_mean:  39992.196","iter:   68<br />train_rmse_mean:  39363.385","iter:   69<br />train_rmse_mean:  38770.779","iter:   70<br />train_rmse_mean:  38194.054","iter:   71<br />train_rmse_mean:  37634.073","iter:   72<br />train_rmse_mean:  37093.292","iter:   73<br />train_rmse_mean:  36565.389","iter:   74<br />train_rmse_mean:  36064.598","iter:   75<br />train_rmse_mean:  35569.878","iter:   76<br />train_rmse_mean:  35106.179","iter:   77<br />train_rmse_mean:  34661.714","iter:   78<br />train_rmse_mean:  34226.693","iter:   79<br />train_rmse_mean:  33794.754","iter:   80<br />train_rmse_mean:  33402.799","iter:   81<br />train_rmse_mean:  33012.034","iter:   82<br />train_rmse_mean:  32639.502","iter:   83<br />train_rmse_mean:  32267.285","iter:   84<br />train_rmse_mean:  31928.889","iter:   85<br />train_rmse_mean:  31604.096","iter:   86<br />train_rmse_mean:  31267.324","iter:   87<br />train_rmse_mean:  30955.951","iter:   88<br />train_rmse_mean:  30651.535","iter:   89<br />train_rmse_mean:  30351.446","iter:   90<br />train_rmse_mean:  30064.346","iter:   91<br />train_rmse_mean:  29796.492","iter:   92<br />train_rmse_mean:  29525.302","iter:   93<br />train_rmse_mean:  29271.585","iter:   94<br />train_rmse_mean:  29023.855","iter:   95<br />train_rmse_mean:  28785.852","iter:   96<br />train_rmse_mean:  28551.863","iter:   97<br />train_rmse_mean:  28328.180","iter:   98<br />train_rmse_mean:  28111.685","iter:   99<br />train_rmse_mean:  27893.476","iter:  100<br />train_rmse_mean:  27693.152","iter:  101<br />train_rmse_mean:  27497.404","iter:  102<br />train_rmse_mean:  27312.865","iter:  103<br />train_rmse_mean:  27129.234","iter:  104<br />train_rmse_mean:  26950.988","iter:  105<br />train_rmse_mean:  26782.442","iter:  106<br />train_rmse_mean:  26611.817","iter:  107<br />train_rmse_mean:  26444.235","iter:  108<br />train_rmse_mean:  26290.949","iter:  109<br />train_rmse_mean:  26132.177","iter:  110<br />train_rmse_mean:  25988.586","iter:  111<br />train_rmse_mean:  25836.293","iter:  112<br />train_rmse_mean:  25697.168","iter:  113<br />train_rmse_mean:  25556.452","iter:  114<br />train_rmse_mean:  25423.451","iter:  115<br />train_rmse_mean:  25291.685","iter:  116<br />train_rmse_mean:  25149.062","iter:  117<br />train_rmse_mean:  25017.563","iter:  118<br />train_rmse_mean:  24888.911","iter:  119<br />train_rmse_mean:  24775.348","iter:  120<br />train_rmse_mean:  24656.938","iter:  121<br />train_rmse_mean:  24543.765","iter:  122<br />train_rmse_mean:  24428.053","iter:  123<br />train_rmse_mean:  24308.753","iter:  124<br />train_rmse_mean:  24204.868","iter:  125<br />train_rmse_mean:  24088.097","iter:  126<br />train_rmse_mean:  23990.512","iter:  127<br />train_rmse_mean:  23901.558","iter:  128<br />train_rmse_mean:  23802.364","iter:  129<br />train_rmse_mean:  23713.952","iter:  130<br />train_rmse_mean:  23624.598","iter:  131<br />train_rmse_mean:  23536.253","iter:  132<br />train_rmse_mean:  23449.935","iter:  133<br />train_rmse_mean:  23371.811","iter:  134<br />train_rmse_mean:  23280.646","iter:  135<br />train_rmse_mean:  23192.888","iter:  136<br />train_rmse_mean:  23110.120","iter:  137<br />train_rmse_mean:  23029.828","iter:  138<br />train_rmse_mean:  22944.790","iter:  139<br />train_rmse_mean:  22873.918","iter:  140<br />train_rmse_mean:  22797.565","iter:  141<br />train_rmse_mean:  22724.810","iter:  142<br />train_rmse_mean:  22657.212","iter:  143<br />train_rmse_mean:  22591.200","iter:  144<br />train_rmse_mean:  22518.683","iter:  145<br />train_rmse_mean:  22451.895","iter:  146<br />train_rmse_mean:  22387.535","iter:  147<br />train_rmse_mean:  22327.417","iter:  148<br />train_rmse_mean:  22258.055","iter:  149<br />train_rmse_mean:  22193.525","iter:  150<br />train_rmse_mean:  22127.062","iter:  151<br />train_rmse_mean:  22069.679","iter:  152<br />train_rmse_mean:  22008.679","iter:  153<br />train_rmse_mean:  21952.920","iter:  154<br />train_rmse_mean:  21905.503","iter:  155<br />train_rmse_mean:  21855.178","iter:  156<br />train_rmse_mean:  21790.606","iter:  157<br />train_rmse_mean:  21733.113","iter:  158<br />train_rmse_mean:  21671.196","iter:  159<br />train_rmse_mean:  21620.925","iter:  160<br />train_rmse_mean:  21565.834","iter:  161<br />train_rmse_mean:  21509.928","iter:  162<br />train_rmse_mean:  21458.348","iter:  163<br />train_rmse_mean:  21407.788","iter:  164<br />train_rmse_mean:  21361.472","iter:  165<br />train_rmse_mean:  21308.782","iter:  166<br />train_rmse_mean:  21259.071","iter:  167<br />train_rmse_mean:  21213.529","iter:  168<br />train_rmse_mean:  21169.855","iter:  169<br />train_rmse_mean:  21119.460","iter:  170<br />train_rmse_mean:  21064.386","iter:  171<br />train_rmse_mean:  21014.609","iter:  172<br />train_rmse_mean:  20963.124","iter:  173<br />train_rmse_mean:  20919.992","iter:  174<br />train_rmse_mean:  20870.092","iter:  175<br />train_rmse_mean:  20830.012","iter:  176<br />train_rmse_mean:  20783.915","iter:  177<br />train_rmse_mean:  20746.840","iter:  178<br />train_rmse_mean:  20704.317","iter:  179<br />train_rmse_mean:  20662.787","iter:  180<br />train_rmse_mean:  20625.845","iter:  181<br />train_rmse_mean:  20580.806","iter:  182<br />train_rmse_mean:  20537.746","iter:  183<br />train_rmse_mean:  20494.740","iter:  184<br />train_rmse_mean:  20454.005","iter:  185<br />train_rmse_mean:  20412.427","iter:  186<br />train_rmse_mean:  20371.431","iter:  187<br />train_rmse_mean:  20337.405","iter:  188<br />train_rmse_mean:  20295.788","iter:  189<br />train_rmse_mean:  20255.046","iter:  190<br />train_rmse_mean:  20210.610","iter:  191<br />train_rmse_mean:  20172.360","iter:  192<br />train_rmse_mean:  20133.429","iter:  193<br />train_rmse_mean:  20099.457","iter:  194<br />train_rmse_mean:  20064.590","iter:  195<br />train_rmse_mean:  20033.142","iter:  196<br />train_rmse_mean:  19996.075","iter:  197<br />train_rmse_mean:  19956.161","iter:  198<br />train_rmse_mean:  19921.365","iter:  199<br />train_rmse_mean:  19886.223","iter:  200<br />train_rmse_mean:  19849.462","iter:  201<br />train_rmse_mean:  19816.879","iter:  202<br />train_rmse_mean:  19778.865","iter:  203<br />train_rmse_mean:  19747.721","iter:  204<br />train_rmse_mean:  19713.475","iter:  205<br />train_rmse_mean:  19679.323","iter:  206<br />train_rmse_mean:  19647.139","iter:  207<br />train_rmse_mean:  19612.704","iter:  208<br />train_rmse_mean:  19575.514","iter:  209<br />train_rmse_mean:  19539.795","iter:  210<br />train_rmse_mean:  19507.111","iter:  211<br />train_rmse_mean:  19477.502","iter:  212<br />train_rmse_mean:  19445.812","iter:  213<br />train_rmse_mean:  19414.546","iter:  214<br />train_rmse_mean:  19384.050","iter:  215<br />train_rmse_mean:  19350.757","iter:  216<br />train_rmse_mean:  19320.494","iter:  217<br />train_rmse_mean:  19290.569","iter:  218<br />train_rmse_mean:  19266.434","iter:  219<br />train_rmse_mean:  19235.398","iter:  220<br />train_rmse_mean:  19199.349","iter:  221<br />train_rmse_mean:  19167.536","iter:  222<br />train_rmse_mean:  19139.634","iter:  223<br />train_rmse_mean:  19110.027","iter:  224<br />train_rmse_mean:  19079.729","iter:  225<br />train_rmse_mean:  19047.218","iter:  226<br />train_rmse_mean:  19020.440","iter:  227<br />train_rmse_mean:  18987.827","iter:  228<br />train_rmse_mean:  18956.927","iter:  229<br />train_rmse_mean:  18934.393","iter:  230<br />train_rmse_mean:  18903.566","iter:  231<br />train_rmse_mean:  18876.781","iter:  232<br />train_rmse_mean:  18852.622","iter:  233<br />train_rmse_mean:  18824.552","iter:  234<br />train_rmse_mean:  18796.197","iter:  235<br />train_rmse_mean:  18767.941","iter:  236<br />train_rmse_mean:  18739.380","iter:  237<br />train_rmse_mean:  18708.989","iter:  238<br />train_rmse_mean:  18681.512","iter:  239<br />train_rmse_mean:  18656.517","iter:  240<br />train_rmse_mean:  18634.752","iter:  241<br />train_rmse_mean:  18606.239","iter:  242<br />train_rmse_mean:  18584.763","iter:  243<br />train_rmse_mean:  18558.623","iter:  244<br />train_rmse_mean:  18531.630","iter:  245<br />train_rmse_mean:  18506.347","iter:  246<br />train_rmse_mean:  18481.410","iter:  247<br />train_rmse_mean:  18457.403","iter:  248<br />train_rmse_mean:  18431.053","iter:  249<br />train_rmse_mean:  18401.902","iter:  250<br />train_rmse_mean:  18378.517","iter:  251<br />train_rmse_mean:  18352.013","iter:  252<br />train_rmse_mean:  18326.234","iter:  253<br />train_rmse_mean:  18295.632","iter:  254<br />train_rmse_mean:  18272.376","iter:  255<br />train_rmse_mean:  18249.778","iter:  256<br />train_rmse_mean:  18228.941","iter:  257<br />train_rmse_mean:  18204.807","iter:  258<br />train_rmse_mean:  18185.669","iter:  259<br />train_rmse_mean:  18164.729","iter:  260<br />train_rmse_mean:  18143.719","iter:  261<br />train_rmse_mean:  18117.115","iter:  262<br />train_rmse_mean:  18093.770","iter:  263<br />train_rmse_mean:  18069.099","iter:  264<br />train_rmse_mean:  18050.361","iter:  265<br />train_rmse_mean:  18027.487","iter:  266<br />train_rmse_mean:  18002.469","iter:  267<br />train_rmse_mean:  17978.301","iter:  268<br />train_rmse_mean:  17955.116","iter:  269<br />train_rmse_mean:  17933.354","iter:  270<br />train_rmse_mean:  17909.938","iter:  271<br />train_rmse_mean:  17890.949","iter:  272<br />train_rmse_mean:  17873.703","iter:  273<br />train_rmse_mean:  17851.219","iter:  274<br />train_rmse_mean:  17826.076","iter:  275<br />train_rmse_mean:  17805.911","iter:  276<br />train_rmse_mean:  17787.161","iter:  277<br />train_rmse_mean:  17767.320","iter:  278<br />train_rmse_mean:  17744.978","iter:  279<br />train_rmse_mean:  17719.755","iter:  280<br />train_rmse_mean:  17697.595","iter:  281<br />train_rmse_mean:  17673.510","iter:  282<br />train_rmse_mean:  17653.223","iter:  283<br />train_rmse_mean:  17635.921","iter:  284<br />train_rmse_mean:  17615.888","iter:  285<br />train_rmse_mean:  17596.906","iter:  286<br />train_rmse_mean:  17572.806","iter:  287<br />train_rmse_mean:  17553.901","iter:  288<br />train_rmse_mean:  17535.668","iter:  289<br />train_rmse_mean:  17511.615","iter:  290<br />train_rmse_mean:  17491.016","iter:  291<br />train_rmse_mean:  17472.861","iter:  292<br />train_rmse_mean:  17452.749","iter:  293<br />train_rmse_mean:  17435.357","iter:  294<br />train_rmse_mean:  17413.792","iter:  295<br />train_rmse_mean:  17393.639","iter:  296<br />train_rmse_mean:  17376.158","iter:  297<br />train_rmse_mean:  17357.397","iter:  298<br />train_rmse_mean:  17340.884","iter:  299<br />train_rmse_mean:  17319.813","iter:  300<br />train_rmse_mean:  17302.984","iter:  301<br />train_rmse_mean:  17285.454","iter:  302<br />train_rmse_mean:  17262.646","iter:  303<br />train_rmse_mean:  17244.613","iter:  304<br />train_rmse_mean:  17228.920","iter:  305<br />train_rmse_mean:  17207.424","iter:  306<br />train_rmse_mean:  17188.713","iter:  307<br />train_rmse_mean:  17171.110","iter:  308<br />train_rmse_mean:  17151.951","iter:  309<br />train_rmse_mean:  17133.387","iter:  310<br />train_rmse_mean:  17115.395","iter:  311<br />train_rmse_mean:  17095.646","iter:  312<br />train_rmse_mean:  17076.216","iter:  313<br />train_rmse_mean:  17054.404","iter:  314<br />train_rmse_mean:  17036.653","iter:  315<br />train_rmse_mean:  17016.860","iter:  316<br />train_rmse_mean:  17002.315","iter:  317<br />train_rmse_mean:  16984.421","iter:  318<br />train_rmse_mean:  16966.749","iter:  319<br />train_rmse_mean:  16947.702","iter:  320<br />train_rmse_mean:  16932.844","iter:  321<br />train_rmse_mean:  16915.884","iter:  322<br />train_rmse_mean:  16900.445","iter:  323<br />train_rmse_mean:  16884.256","iter:  324<br />train_rmse_mean:  16863.469","iter:  325<br />train_rmse_mean:  16846.607","iter:  326<br />train_rmse_mean:  16827.737","iter:  327<br />train_rmse_mean:  16808.360","iter:  328<br />train_rmse_mean:  16786.858","iter:  329<br />train_rmse_mean:  16767.861","iter:  330<br />train_rmse_mean:  16752.102","iter:  331<br />train_rmse_mean:  16736.047","iter:  332<br />train_rmse_mean:  16719.280","iter:  333<br />train_rmse_mean:  16701.387","iter:  334<br />train_rmse_mean:  16686.595","iter:  335<br />train_rmse_mean:  16672.722","iter:  336<br />train_rmse_mean:  16657.046","iter:  337<br />train_rmse_mean:  16642.906","iter:  338<br />train_rmse_mean:  16625.221","iter:  339<br />train_rmse_mean:  16606.028","iter:  340<br />train_rmse_mean:  16588.651","iter:  341<br />train_rmse_mean:  16571.342","iter:  342<br />train_rmse_mean:  16556.781","iter:  343<br />train_rmse_mean:  16542.052","iter:  344<br />train_rmse_mean:  16524.993","iter:  345<br />train_rmse_mean:  16510.148","iter:  346<br />train_rmse_mean:  16493.638","iter:  347<br />train_rmse_mean:  16478.523","iter:  348<br />train_rmse_mean:  16462.535","iter:  349<br />train_rmse_mean:  16444.254","iter:  350<br />train_rmse_mean:  16429.206","iter:  351<br />train_rmse_mean:  16411.538","iter:  352<br />train_rmse_mean:  16397.272","iter:  353<br />train_rmse_mean:  16381.892","iter:  354<br />train_rmse_mean:  16366.901","iter:  355<br />train_rmse_mean:  16351.735","iter:  356<br />train_rmse_mean:  16336.935","iter:  357<br />train_rmse_mean:  16322.199","iter:  358<br />train_rmse_mean:  16307.463","iter:  359<br />train_rmse_mean:  16292.491","iter:  360<br />train_rmse_mean:  16275.091","iter:  361<br />train_rmse_mean:  16261.392","iter:  362<br />train_rmse_mean:  16246.543","iter:  363<br />train_rmse_mean:  16231.102","iter:  364<br />train_rmse_mean:  16213.944","iter:  365<br />train_rmse_mean:  16195.230","iter:  366<br />train_rmse_mean:  16181.142","iter:  367<br />train_rmse_mean:  16165.150","iter:  368<br />train_rmse_mean:  16150.151","iter:  369<br />train_rmse_mean:  16135.006","iter:  370<br />train_rmse_mean:  16117.312","iter:  371<br />train_rmse_mean:  16101.214","iter:  372<br />train_rmse_mean:  16085.648","iter:  373<br />train_rmse_mean:  16073.554","iter:  374<br />train_rmse_mean:  16058.509","iter:  375<br />train_rmse_mean:  16044.779","iter:  376<br />train_rmse_mean:  16029.334","iter:  377<br />train_rmse_mean:  16015.674","iter:  378<br />train_rmse_mean:  16001.202","iter:  379<br />train_rmse_mean:  15985.921","iter:  380<br />train_rmse_mean:  15973.600","iter:  381<br />train_rmse_mean:  15957.306","iter:  382<br />train_rmse_mean:  15943.928","iter:  383<br />train_rmse_mean:  15930.733","iter:  384<br />train_rmse_mean:  15915.274","iter:  385<br />train_rmse_mean:  15901.545","iter:  386<br />train_rmse_mean:  15889.339","iter:  387<br />train_rmse_mean:  15876.233","iter:  388<br />train_rmse_mean:  15860.481","iter:  389<br />train_rmse_mean:  15843.438","iter:  390<br />train_rmse_mean:  15829.512","iter:  391<br />train_rmse_mean:  15816.132","iter:  392<br />train_rmse_mean:  15804.683","iter:  393<br />train_rmse_mean:  15789.457","iter:  394<br />train_rmse_mean:  15772.740","iter:  395<br />train_rmse_mean:  15756.898","iter:  396<br />train_rmse_mean:  15745.034","iter:  397<br />train_rmse_mean:  15730.857","iter:  398<br />train_rmse_mean:  15715.763","iter:  399<br />train_rmse_mean:  15702.685","iter:  400<br />train_rmse_mean:  15690.400","iter:  401<br />train_rmse_mean:  15678.683","iter:  402<br />train_rmse_mean:  15664.666","iter:  403<br />train_rmse_mean:  15651.906","iter:  404<br />train_rmse_mean:  15640.508","iter:  405<br />train_rmse_mean:  15626.300","iter:  406<br />train_rmse_mean:  15613.093","iter:  407<br />train_rmse_mean:  15599.835","iter:  408<br />train_rmse_mean:  15587.871","iter:  409<br />train_rmse_mean:  15575.513","iter:  410<br />train_rmse_mean:  15559.796","iter:  411<br />train_rmse_mean:  15548.467","iter:  412<br />train_rmse_mean:  15538.503","iter:  413<br />train_rmse_mean:  15526.762","iter:  414<br />train_rmse_mean:  15515.239","iter:  415<br />train_rmse_mean:  15499.780","iter:  416<br />train_rmse_mean:  15487.118","iter:  417<br />train_rmse_mean:  15472.864","iter:  418<br />train_rmse_mean:  15458.535","iter:  419<br />train_rmse_mean:  15444.817","iter:  420<br />train_rmse_mean:  15431.001","iter:  421<br />train_rmse_mean:  15415.059","iter:  422<br />train_rmse_mean:  15402.646","iter:  423<br />train_rmse_mean:  15390.149","iter:  424<br />train_rmse_mean:  15377.282","iter:  425<br />train_rmse_mean:  15363.987","iter:  426<br />train_rmse_mean:  15350.474","iter:  427<br />train_rmse_mean:  15337.489","iter:  428<br />train_rmse_mean:  15325.339","iter:  429<br />train_rmse_mean:  15310.578","iter:  430<br />train_rmse_mean:  15299.701","iter:  431<br />train_rmse_mean:  15286.215","iter:  432<br />train_rmse_mean:  15273.611","iter:  433<br />train_rmse_mean:  15263.409","iter:  434<br />train_rmse_mean:  15251.068","iter:  435<br />train_rmse_mean:  15235.587","iter:  436<br />train_rmse_mean:  15223.003","iter:  437<br />train_rmse_mean:  15208.141","iter:  438<br />train_rmse_mean:  15194.635","iter:  439<br />train_rmse_mean:  15177.621","iter:  440<br />train_rmse_mean:  15167.198","iter:  441<br />train_rmse_mean:  15154.855","iter:  442<br />train_rmse_mean:  15142.491","iter:  443<br />train_rmse_mean:  15130.671","iter:  444<br />train_rmse_mean:  15118.973","iter:  445<br />train_rmse_mean:  15106.978","iter:  446<br />train_rmse_mean:  15094.216","iter:  447<br />train_rmse_mean:  15081.846","iter:  448<br />train_rmse_mean:  15070.694","iter:  449<br />train_rmse_mean:  15059.232","iter:  450<br />train_rmse_mean:  15047.028","iter:  451<br />train_rmse_mean:  15035.670","iter:  452<br />train_rmse_mean:  15022.730","iter:  453<br />train_rmse_mean:  15010.178","iter:  454<br />train_rmse_mean:  14998.014","iter:  455<br />train_rmse_mean:  14984.662","iter:  456<br />train_rmse_mean:  14971.080","iter:  457<br />train_rmse_mean:  14959.356","iter:  458<br />train_rmse_mean:  14944.995","iter:  459<br />train_rmse_mean:  14931.880","iter:  460<br />train_rmse_mean:  14920.317","iter:  461<br />train_rmse_mean:  14909.517","iter:  462<br />train_rmse_mean:  14900.686","iter:  463<br />train_rmse_mean:  14890.555","iter:  464<br />train_rmse_mean:  14876.559","iter:  465<br />train_rmse_mean:  14866.315","iter:  466<br />train_rmse_mean:  14853.939","iter:  467<br />train_rmse_mean:  14839.585","iter:  468<br />train_rmse_mean:  14827.308","iter:  469<br />train_rmse_mean:  14815.555","iter:  470<br />train_rmse_mean:  14804.178","iter:  471<br />train_rmse_mean:  14794.250","iter:  472<br />train_rmse_mean:  14784.214","iter:  473<br />train_rmse_mean:  14773.422","iter:  474<br />train_rmse_mean:  14762.305","iter:  475<br />train_rmse_mean:  14751.466","iter:  476<br />train_rmse_mean:  14742.357","iter:  477<br />train_rmse_mean:  14730.621","iter:  478<br />train_rmse_mean:  14719.716","iter:  479<br />train_rmse_mean:  14710.049","iter:  480<br />train_rmse_mean:  14697.964","iter:  481<br />train_rmse_mean:  14686.780","iter:  482<br />train_rmse_mean:  14674.998","iter:  483<br />train_rmse_mean:  14664.826","iter:  484<br />train_rmse_mean:  14653.129","iter:  485<br />train_rmse_mean:  14642.495","iter:  486<br />train_rmse_mean:  14632.196","iter:  487<br />train_rmse_mean:  14623.267","iter:  488<br />train_rmse_mean:  14613.314","iter:  489<br />train_rmse_mean:  14602.869","iter:  490<br />train_rmse_mean:  14590.971","iter:  491<br />train_rmse_mean:  14580.377","iter:  492<br />train_rmse_mean:  14570.556","iter:  493<br />train_rmse_mean:  14559.152","iter:  494<br />train_rmse_mean:  14546.666","iter:  495<br />train_rmse_mean:  14537.619","iter:  496<br />train_rmse_mean:  14527.654","iter:  497<br />train_rmse_mean:  14516.861","iter:  498<br />train_rmse_mean:  14505.535","iter:  499<br />train_rmse_mean:  14497.087","iter:  500<br />train_rmse_mean:  14487.698","iter:  501<br />train_rmse_mean:  14477.506","iter:  502<br />train_rmse_mean:  14468.848","iter:  503<br />train_rmse_mean:  14458.679","iter:  504<br />train_rmse_mean:  14447.826","iter:  505<br />train_rmse_mean:  14436.658","iter:  506<br />train_rmse_mean:  14426.557","iter:  507<br />train_rmse_mean:  14416.676","iter:  508<br />train_rmse_mean:  14406.305","iter:  509<br />train_rmse_mean:  14396.110","iter:  510<br />train_rmse_mean:  14386.270","iter:  511<br />train_rmse_mean:  14376.013","iter:  512<br />train_rmse_mean:  14365.146","iter:  513<br />train_rmse_mean:  14355.160","iter:  514<br />train_rmse_mean:  14343.984","iter:  515<br />train_rmse_mean:  14334.453","iter:  516<br />train_rmse_mean:  14322.614","iter:  517<br />train_rmse_mean:  14312.857","iter:  518<br />train_rmse_mean:  14302.342","iter:  519<br />train_rmse_mean:  14291.184","iter:  520<br />train_rmse_mean:  14282.016","iter:  521<br />train_rmse_mean:  14271.838","iter:  522<br />train_rmse_mean:  14262.135","iter:  523<br />train_rmse_mean:  14251.361","iter:  524<br />train_rmse_mean:  14242.866","iter:  525<br />train_rmse_mean:  14231.911","iter:  526<br />train_rmse_mean:  14224.006","iter:  527<br />train_rmse_mean:  14213.686","iter:  528<br />train_rmse_mean:  14200.788","iter:  529<br />train_rmse_mean:  14190.291","iter:  530<br />train_rmse_mean:  14179.453","iter:  531<br />train_rmse_mean:  14168.263","iter:  532<br />train_rmse_mean:  14158.463","iter:  533<br />train_rmse_mean:  14148.392","iter:  534<br />train_rmse_mean:  14137.142","iter:  535<br />train_rmse_mean:  14126.992","iter:  536<br />train_rmse_mean:  14119.470","iter:  537<br />train_rmse_mean:  14108.643","iter:  538<br />train_rmse_mean:  14095.192","iter:  539<br />train_rmse_mean:  14085.099","iter:  540<br />train_rmse_mean:  14075.831","iter:  541<br />train_rmse_mean:  14066.516","iter:  542<br />train_rmse_mean:  14056.912","iter:  543<br />train_rmse_mean:  14046.381","iter:  544<br />train_rmse_mean:  14033.887","iter:  545<br />train_rmse_mean:  14024.271","iter:  546<br />train_rmse_mean:  14014.500","iter:  547<br />train_rmse_mean:  14005.501","iter:  548<br />train_rmse_mean:  13996.198","iter:  549<br />train_rmse_mean:  13985.927","iter:  550<br />train_rmse_mean:  13977.246","iter:  551<br />train_rmse_mean:  13967.669","iter:  552<br />train_rmse_mean:  13958.050","iter:  553<br />train_rmse_mean:  13947.023","iter:  554<br />train_rmse_mean:  13938.388","iter:  555<br />train_rmse_mean:  13929.284","iter:  556<br />train_rmse_mean:  13919.649","iter:  557<br />train_rmse_mean:  13910.522","iter:  558<br />train_rmse_mean:  13900.488","iter:  559<br />train_rmse_mean:  13892.704","iter:  560<br />train_rmse_mean:  13882.858","iter:  561<br />train_rmse_mean:  13874.484","iter:  562<br />train_rmse_mean:  13864.727","iter:  563<br />train_rmse_mean:  13855.716","iter:  564<br />train_rmse_mean:  13846.067","iter:  565<br />train_rmse_mean:  13836.605","iter:  566<br />train_rmse_mean:  13828.400","iter:  567<br />train_rmse_mean:  13818.834","iter:  568<br />train_rmse_mean:  13808.383","iter:  569<br />train_rmse_mean:  13799.313","iter:  570<br />train_rmse_mean:  13790.742","iter:  571<br />train_rmse_mean:  13782.032","iter:  572<br />train_rmse_mean:  13773.618","iter:  573<br />train_rmse_mean:  13763.614","iter:  574<br />train_rmse_mean:  13753.898","iter:  575<br />train_rmse_mean:  13744.888","iter:  576<br />train_rmse_mean:  13737.230","iter:  577<br />train_rmse_mean:  13728.578","iter:  578<br />train_rmse_mean:  13720.198","iter:  579<br />train_rmse_mean:  13710.973","iter:  580<br />train_rmse_mean:  13701.560","iter:  581<br />train_rmse_mean:  13690.223","iter:  582<br />train_rmse_mean:  13680.652","iter:  583<br />train_rmse_mean:  13670.956","iter:  584<br />train_rmse_mean:  13662.002","iter:  585<br />train_rmse_mean:  13651.748","iter:  586<br />train_rmse_mean:  13642.460","iter:  587<br />train_rmse_mean:  13633.420","iter:  588<br />train_rmse_mean:  13625.483","iter:  589<br />train_rmse_mean:  13617.675","iter:  590<br />train_rmse_mean:  13607.523","iter:  591<br />train_rmse_mean:  13598.254","iter:  592<br />train_rmse_mean:  13588.972","iter:  593<br />train_rmse_mean:  13579.421","iter:  594<br />train_rmse_mean:  13572.465","iter:  595<br />train_rmse_mean:  13565.672","iter:  596<br />train_rmse_mean:  13556.810","iter:  597<br />train_rmse_mean:  13549.184","iter:  598<br />train_rmse_mean:  13539.705","iter:  599<br />train_rmse_mean:  13530.250","iter:  600<br />train_rmse_mean:  13522.399","iter:  601<br />train_rmse_mean:  13514.963","iter:  602<br />train_rmse_mean:  13505.246","iter:  603<br />train_rmse_mean:  13496.489","iter:  604<br />train_rmse_mean:  13488.200","iter:  605<br />train_rmse_mean:  13479.741","iter:  606<br />train_rmse_mean:  13471.296","iter:  607<br />train_rmse_mean:  13464.555","iter:  608<br />train_rmse_mean:  13454.266","iter:  609<br />train_rmse_mean:  13447.580","iter:  610<br />train_rmse_mean:  13439.270","iter:  611<br />train_rmse_mean:  13429.659","iter:  612<br />train_rmse_mean:  13420.553","iter:  613<br />train_rmse_mean:  13412.422","iter:  614<br />train_rmse_mean:  13402.988","iter:  615<br />train_rmse_mean:  13393.525","iter:  616<br />train_rmse_mean:  13382.838","iter:  617<br />train_rmse_mean:  13374.162","iter:  618<br />train_rmse_mean:  13365.718","iter:  619<br />train_rmse_mean:  13359.106","iter:  620<br />train_rmse_mean:  13351.000","iter:  621<br />train_rmse_mean:  13342.698","iter:  622<br />train_rmse_mean:  13334.410","iter:  623<br />train_rmse_mean:  13325.503","iter:  624<br />train_rmse_mean:  13317.747","iter:  625<br />train_rmse_mean:  13310.892","iter:  626<br />train_rmse_mean:  13301.223","iter:  627<br />train_rmse_mean:  13293.360","iter:  628<br />train_rmse_mean:  13284.759","iter:  629<br />train_rmse_mean:  13276.935","iter:  630<br />train_rmse_mean:  13268.789","iter:  631<br />train_rmse_mean:  13260.480","iter:  632<br />train_rmse_mean:  13252.293","iter:  633<br />train_rmse_mean:  13242.278","iter:  634<br />train_rmse_mean:  13235.663","iter:  635<br />train_rmse_mean:  13225.572","iter:  636<br />train_rmse_mean:  13216.745","iter:  637<br />train_rmse_mean:  13210.185","iter:  638<br />train_rmse_mean:  13202.204","iter:  639<br />train_rmse_mean:  13194.465","iter:  640<br />train_rmse_mean:  13187.705","iter:  641<br />train_rmse_mean:  13179.774","iter:  642<br />train_rmse_mean:  13171.393","iter:  643<br />train_rmse_mean:  13162.223","iter:  644<br />train_rmse_mean:  13153.067","iter:  645<br />train_rmse_mean:  13143.956","iter:  646<br />train_rmse_mean:  13134.817","iter:  647<br />train_rmse_mean:  13125.897","iter:  648<br />train_rmse_mean:  13116.369","iter:  649<br />train_rmse_mean:  13108.389","iter:  650<br />train_rmse_mean:  13098.929","iter:  651<br />train_rmse_mean:  13090.563","iter:  652<br />train_rmse_mean:  13082.819","iter:  653<br />train_rmse_mean:  13075.643","iter:  654<br />train_rmse_mean:  13068.630","iter:  655<br />train_rmse_mean:  13062.626","iter:  656<br />train_rmse_mean:  13054.171","iter:  657<br />train_rmse_mean:  13045.202","iter:  658<br />train_rmse_mean:  13037.611","iter:  659<br />train_rmse_mean:  13030.879","iter:  660<br />train_rmse_mean:  13023.412","iter:  661<br />train_rmse_mean:  13016.515","iter:  662<br />train_rmse_mean:  13008.908","iter:  663<br />train_rmse_mean:  13000.776","iter:  664<br />train_rmse_mean:  12991.639","iter:  665<br />train_rmse_mean:  12984.454","iter:  666<br />train_rmse_mean:  12976.288","iter:  667<br />train_rmse_mean:  12967.650","iter:  668<br />train_rmse_mean:  12960.350","iter:  669<br />train_rmse_mean:  12953.156","iter:  670<br />train_rmse_mean:  12945.482","iter:  671<br />train_rmse_mean:  12937.007","iter:  672<br />train_rmse_mean:  12928.560","iter:  673<br />train_rmse_mean:  12919.757","iter:  674<br />train_rmse_mean:  12912.648","iter:  675<br />train_rmse_mean:  12906.097","iter:  676<br />train_rmse_mean:  12899.291","iter:  677<br />train_rmse_mean:  12891.446","iter:  678<br />train_rmse_mean:  12884.666","iter:  679<br />train_rmse_mean:  12874.340","iter:  680<br />train_rmse_mean:  12867.162","iter:  681<br />train_rmse_mean:  12860.751","iter:  682<br />train_rmse_mean:  12853.527","iter:  683<br />train_rmse_mean:  12846.268","iter:  684<br />train_rmse_mean:  12837.591","iter:  685<br />train_rmse_mean:  12828.750","iter:  686<br />train_rmse_mean:  12823.656","iter:  687<br />train_rmse_mean:  12815.798","iter:  688<br />train_rmse_mean:  12808.082","iter:  689<br />train_rmse_mean:  12800.090","iter:  690<br />train_rmse_mean:  12794.788","iter:  691<br />train_rmse_mean:  12787.448","iter:  692<br />train_rmse_mean:  12778.543","iter:  693<br />train_rmse_mean:  12772.077","iter:  694<br />train_rmse_mean:  12763.950","iter:  695<br />train_rmse_mean:  12758.962","iter:  696<br />train_rmse_mean:  12750.328","iter:  697<br />train_rmse_mean:  12742.455","iter:  698<br />train_rmse_mean:  12735.187","iter:  699<br />train_rmse_mean:  12727.766","iter:  700<br />train_rmse_mean:  12720.008","iter:  701<br />train_rmse_mean:  12712.580","iter:  702<br />train_rmse_mean:  12705.765","iter:  703<br />train_rmse_mean:  12697.891","iter:  704<br />train_rmse_mean:  12691.353","iter:  705<br />train_rmse_mean:  12683.892","iter:  706<br />train_rmse_mean:  12676.999","iter:  707<br />train_rmse_mean:  12668.808","iter:  708<br />train_rmse_mean:  12659.829","iter:  709<br />train_rmse_mean:  12653.098","iter:  710<br />train_rmse_mean:  12644.748","iter:  711<br />train_rmse_mean:  12638.258","iter:  712<br />train_rmse_mean:  12631.059","iter:  713<br />train_rmse_mean:  12625.059","iter:  714<br />train_rmse_mean:  12618.259","iter:  715<br />train_rmse_mean:  12611.755","iter:  716<br />train_rmse_mean:  12604.931","iter:  717<br />train_rmse_mean:  12597.442","iter:  718<br />train_rmse_mean:  12589.627","iter:  719<br />train_rmse_mean:  12583.183","iter:  720<br />train_rmse_mean:  12575.430","iter:  721<br />train_rmse_mean:  12567.504","iter:  722<br />train_rmse_mean:  12562.046","iter:  723<br />train_rmse_mean:  12555.747","iter:  724<br />train_rmse_mean:  12548.520","iter:  725<br />train_rmse_mean:  12539.335","iter:  726<br />train_rmse_mean:  12532.929","iter:  727<br />train_rmse_mean:  12526.424","iter:  728<br />train_rmse_mean:  12519.403","iter:  729<br />train_rmse_mean:  12511.667","iter:  730<br />train_rmse_mean:  12504.376","iter:  731<br />train_rmse_mean:  12497.529","iter:  732<br />train_rmse_mean:  12490.084","iter:  733<br />train_rmse_mean:  12483.935","iter:  734<br />train_rmse_mean:  12475.956","iter:  735<br />train_rmse_mean:  12467.372","iter:  736<br />train_rmse_mean:  12460.876","iter:  737<br />train_rmse_mean:  12452.285","iter:  738<br />train_rmse_mean:  12445.542","iter:  739<br />train_rmse_mean:  12439.054","iter:  740<br />train_rmse_mean:  12429.771","iter:  741<br />train_rmse_mean:  12423.237","iter:  742<br />train_rmse_mean:  12416.239","iter:  743<br />train_rmse_mean:  12411.393","iter:  744<br />train_rmse_mean:  12403.712","iter:  745<br />train_rmse_mean:  12397.380","iter:  746<br />train_rmse_mean:  12390.942","iter:  747<br />train_rmse_mean:  12383.460","iter:  748<br />train_rmse_mean:  12375.493","iter:  749<br />train_rmse_mean:  12369.002","iter:  750<br />train_rmse_mean:  12361.186","iter:  751<br />train_rmse_mean:  12351.921","iter:  752<br />train_rmse_mean:  12345.358","iter:  753<br />train_rmse_mean:  12338.822","iter:  754<br />train_rmse_mean:  12332.029","iter:  755<br />train_rmse_mean:  12325.017","iter:  756<br />train_rmse_mean:  12319.558","iter:  757<br />train_rmse_mean:  12311.496","iter:  758<br />train_rmse_mean:  12305.237","iter:  759<br />train_rmse_mean:  12298.529","iter:  760<br />train_rmse_mean:  12290.881","iter:  761<br />train_rmse_mean:  12286.351","iter:  762<br />train_rmse_mean:  12278.403","iter:  763<br />train_rmse_mean:  12271.970","iter:  764<br />train_rmse_mean:  12266.144","iter:  765<br />train_rmse_mean:  12259.479","iter:  766<br />train_rmse_mean:  12252.734","iter:  767<br />train_rmse_mean:  12246.307","iter:  768<br />train_rmse_mean:  12237.249","iter:  769<br />train_rmse_mean:  12229.300","iter:  770<br />train_rmse_mean:  12222.543","iter:  771<br />train_rmse_mean:  12216.185","iter:  772<br />train_rmse_mean:  12208.137","iter:  773<br />train_rmse_mean:  12201.821","iter:  774<br />train_rmse_mean:  12195.934","iter:  775<br />train_rmse_mean:  12189.477","iter:  776<br />train_rmse_mean:  12182.651","iter:  777<br />train_rmse_mean:  12175.463","iter:  778<br />train_rmse_mean:  12168.941","iter:  779<br />train_rmse_mean:  12160.609","iter:  780<br />train_rmse_mean:  12154.027","iter:  781<br />train_rmse_mean:  12146.505","iter:  782<br />train_rmse_mean:  12140.762","iter:  783<br />train_rmse_mean:  12134.726","iter:  784<br />train_rmse_mean:  12129.367","iter:  785<br />train_rmse_mean:  12124.241","iter:  786<br />train_rmse_mean:  12117.246","iter:  787<br />train_rmse_mean:  12112.184","iter:  788<br />train_rmse_mean:  12105.135","iter:  789<br />train_rmse_mean:  12098.853","iter:  790<br />train_rmse_mean:  12092.763","iter:  791<br />train_rmse_mean:  12085.783","iter:  792<br />train_rmse_mean:  12079.459","iter:  793<br />train_rmse_mean:  12073.560","iter:  794<br />train_rmse_mean:  12067.730","iter:  795<br />train_rmse_mean:  12060.526","iter:  796<br />train_rmse_mean:  12053.139","iter:  797<br />train_rmse_mean:  12046.673","iter:  798<br />train_rmse_mean:  12039.867","iter:  799<br />train_rmse_mean:  12031.379","iter:  800<br />train_rmse_mean:  12024.393","iter:  801<br />train_rmse_mean:  12017.692","iter:  802<br />train_rmse_mean:  12011.580","iter:  803<br />train_rmse_mean:  12005.236","iter:  804<br />train_rmse_mean:  11998.133","iter:  805<br />train_rmse_mean:  11991.300","iter:  806<br />train_rmse_mean:  11983.020","iter:  807<br />train_rmse_mean:  11976.323","iter:  808<br />train_rmse_mean:  11969.715","iter:  809<br />train_rmse_mean:  11963.055","iter:  810<br />train_rmse_mean:  11955.462","iter:  811<br />train_rmse_mean:  11949.039","iter:  812<br />train_rmse_mean:  11942.028","iter:  813<br />train_rmse_mean:  11934.752","iter:  814<br />train_rmse_mean:  11928.224","iter:  815<br />train_rmse_mean:  11921.069","iter:  816<br />train_rmse_mean:  11915.199","iter:  817<br />train_rmse_mean:  11909.146","iter:  818<br />train_rmse_mean:  11902.419","iter:  819<br />train_rmse_mean:  11895.051","iter:  820<br />train_rmse_mean:  11889.092","iter:  821<br />train_rmse_mean:  11883.203","iter:  822<br />train_rmse_mean:  11877.121","iter:  823<br />train_rmse_mean:  11871.158","iter:  824<br />train_rmse_mean:  11864.810","iter:  825<br />train_rmse_mean:  11859.613","iter:  826<br />train_rmse_mean:  11852.521","iter:  827<br />train_rmse_mean:  11846.260","iter:  828<br />train_rmse_mean:  11839.954","iter:  829<br />train_rmse_mean:  11832.646","iter:  830<br />train_rmse_mean:  11825.485","iter:  831<br />train_rmse_mean:  11820.278","iter:  832<br />train_rmse_mean:  11812.189","iter:  833<br />train_rmse_mean:  11806.456","iter:  834<br />train_rmse_mean:  11799.519","iter:  835<br />train_rmse_mean:  11793.186","iter:  836<br />train_rmse_mean:  11788.019","iter:  837<br />train_rmse_mean:  11782.132","iter:  838<br />train_rmse_mean:  11775.745","iter:  839<br />train_rmse_mean:  11769.130","iter:  840<br />train_rmse_mean:  11763.694","iter:  841<br />train_rmse_mean:  11757.914","iter:  842<br />train_rmse_mean:  11751.895","iter:  843<br />train_rmse_mean:  11746.175","iter:  844<br />train_rmse_mean:  11742.362","iter:  845<br />train_rmse_mean:  11736.534","iter:  846<br />train_rmse_mean:  11730.444","iter:  847<br />train_rmse_mean:  11725.076","iter:  848<br />train_rmse_mean:  11718.404","iter:  849<br />train_rmse_mean:  11713.050","iter:  850<br />train_rmse_mean:  11707.539","iter:  851<br />train_rmse_mean:  11701.701","iter:  852<br />train_rmse_mean:  11696.509","iter:  853<br />train_rmse_mean:  11690.075","iter:  854<br />train_rmse_mean:  11683.969","iter:  855<br />train_rmse_mean:  11677.313","iter:  856<br />train_rmse_mean:  11671.220","iter:  857<br />train_rmse_mean:  11666.006","iter:  858<br />train_rmse_mean:  11659.874","iter:  859<br />train_rmse_mean:  11653.999","iter:  860<br />train_rmse_mean:  11648.137","iter:  861<br />train_rmse_mean:  11642.399","iter:  862<br />train_rmse_mean:  11635.602","iter:  863<br />train_rmse_mean:  11627.858","iter:  864<br />train_rmse_mean:  11622.935","iter:  865<br />train_rmse_mean:  11616.022","iter:  866<br />train_rmse_mean:  11610.861","iter:  867<br />train_rmse_mean:  11604.614","iter:  868<br />train_rmse_mean:  11598.480","iter:  869<br />train_rmse_mean:  11592.622","iter:  870<br />train_rmse_mean:  11585.944","iter:  871<br />train_rmse_mean:  11580.115","iter:  872<br />train_rmse_mean:  11574.720","iter:  873<br />train_rmse_mean:  11568.848","iter:  874<br />train_rmse_mean:  11562.682","iter:  875<br />train_rmse_mean:  11556.200","iter:  876<br />train_rmse_mean:  11550.887","iter:  877<br />train_rmse_mean:  11545.024","iter:  878<br />train_rmse_mean:  11538.744","iter:  879<br />train_rmse_mean:  11533.610","iter:  880<br />train_rmse_mean:  11526.813","iter:  881<br />train_rmse_mean:  11520.966","iter:  882<br />train_rmse_mean:  11515.280","iter:  883<br />train_rmse_mean:  11507.649","iter:  884<br />train_rmse_mean:  11501.902","iter:  885<br />train_rmse_mean:  11496.430","iter:  886<br />train_rmse_mean:  11490.488","iter:  887<br />train_rmse_mean:  11485.737","iter:  888<br />train_rmse_mean:  11479.940","iter:  889<br />train_rmse_mean:  11473.352","iter:  890<br />train_rmse_mean:  11467.955","iter:  891<br />train_rmse_mean:  11462.739","iter:  892<br />train_rmse_mean:  11456.308","iter:  893<br />train_rmse_mean:  11449.543","iter:  894<br />train_rmse_mean:  11443.293","iter:  895<br />train_rmse_mean:  11437.312","iter:  896<br />train_rmse_mean:  11431.613","iter:  897<br />train_rmse_mean:  11426.046","iter:  898<br />train_rmse_mean:  11420.509","iter:  899<br />train_rmse_mean:  11414.268","iter:  900<br />train_rmse_mean:  11409.456","iter:  901<br />train_rmse_mean:  11403.471","iter:  902<br />train_rmse_mean:  11397.264","iter:  903<br />train_rmse_mean:  11392.394","iter:  904<br />train_rmse_mean:  11385.323","iter:  905<br />train_rmse_mean:  11381.155","iter:  906<br />train_rmse_mean:  11374.903","iter:  907<br />train_rmse_mean:  11369.758","iter:  908<br />train_rmse_mean:  11362.955","iter:  909<br />train_rmse_mean:  11357.443","iter:  910<br />train_rmse_mean:  11352.341","iter:  911<br />train_rmse_mean:  11345.992","iter:  912<br />train_rmse_mean:  11338.931","iter:  913<br />train_rmse_mean:  11333.021","iter:  914<br />train_rmse_mean:  11327.812","iter:  915<br />train_rmse_mean:  11323.428","iter:  916<br />train_rmse_mean:  11317.875","iter:  917<br />train_rmse_mean:  11312.231","iter:  918<br />train_rmse_mean:  11307.672","iter:  919<br />train_rmse_mean:  11301.996","iter:  920<br />train_rmse_mean:  11297.270","iter:  921<br />train_rmse_mean:  11292.208","iter:  922<br />train_rmse_mean:  11284.453","iter:  923<br />train_rmse_mean:  11279.211","iter:  924<br />train_rmse_mean:  11275.058","iter:  925<br />train_rmse_mean:  11269.522","iter:  926<br />train_rmse_mean:  11263.572","iter:  927<br />train_rmse_mean:  11258.299","iter:  928<br />train_rmse_mean:  11253.300","iter:  929<br />train_rmse_mean:  11247.014","iter:  930<br />train_rmse_mean:  11241.389","iter:  931<br />train_rmse_mean:  11235.052","iter:  932<br />train_rmse_mean:  11228.681","iter:  933<br />train_rmse_mean:  11223.170","iter:  934<br />train_rmse_mean:  11218.513","iter:  935<br />train_rmse_mean:  11211.800","iter:  936<br />train_rmse_mean:  11206.290","iter:  937<br />train_rmse_mean:  11201.152","iter:  938<br />train_rmse_mean:  11194.064","iter:  939<br />train_rmse_mean:  11189.440","iter:  940<br />train_rmse_mean:  11183.396","iter:  941<br />train_rmse_mean:  11177.571","iter:  942<br />train_rmse_mean:  11172.012","iter:  943<br />train_rmse_mean:  11167.122","iter:  944<br />train_rmse_mean:  11163.167","iter:  945<br />train_rmse_mean:  11156.935","iter:  946<br />train_rmse_mean:  11152.086","iter:  947<br />train_rmse_mean:  11146.418","iter:  948<br />train_rmse_mean:  11140.387","iter:  949<br />train_rmse_mean:  11134.001","iter:  950<br />train_rmse_mean:  11128.774","iter:  951<br />train_rmse_mean:  11123.338","iter:  952<br />train_rmse_mean:  11119.265","iter:  953<br />train_rmse_mean:  11114.284","iter:  954<br />train_rmse_mean:  11107.580","iter:  955<br />train_rmse_mean:  11101.972","iter:  956<br />train_rmse_mean:  11095.699","iter:  957<br />train_rmse_mean:  11090.498","iter:  958<br />train_rmse_mean:  11084.128","iter:  959<br />train_rmse_mean:  11077.932","iter:  960<br />train_rmse_mean:  11072.823","iter:  961<br />train_rmse_mean:  11066.972","iter:  962<br />train_rmse_mean:  11061.535","iter:  963<br />train_rmse_mean:  11057.189","iter:  964<br />train_rmse_mean:  11050.768","iter:  965<br />train_rmse_mean:  11044.762","iter:  966<br />train_rmse_mean:  11040.290","iter:  967<br />train_rmse_mean:  11035.795","iter:  968<br />train_rmse_mean:  11031.300","iter:  969<br />train_rmse_mean:  11026.061","iter:  970<br />train_rmse_mean:  11021.992","iter:  971<br />train_rmse_mean:  11016.354","iter:  972<br />train_rmse_mean:  11012.365","iter:  973<br />train_rmse_mean:  11008.038","iter:  974<br />train_rmse_mean:  11002.563","iter:  975<br />train_rmse_mean:  10996.297","iter:  976<br />train_rmse_mean:  10989.780","iter:  977<br />train_rmse_mean:  10985.250","iter:  978<br />train_rmse_mean:  10979.295","iter:  979<br />train_rmse_mean:  10973.580","iter:  980<br />train_rmse_mean:  10968.986","iter:  981<br />train_rmse_mean:  10963.352","iter:  982<br />train_rmse_mean:  10958.050","iter:  983<br />train_rmse_mean:  10952.743","iter:  984<br />train_rmse_mean:  10947.726","iter:  985<br />train_rmse_mean:  10943.348","iter:  986<br />train_rmse_mean:  10938.796","iter:  987<br />train_rmse_mean:  10933.079","iter:  988<br />train_rmse_mean:  10926.720","iter:  989<br />train_rmse_mean:  10921.584","iter:  990<br />train_rmse_mean:  10915.427","iter:  991<br />train_rmse_mean:  10911.820","iter:  992<br />train_rmse_mean:  10906.919","iter:  993<br />train_rmse_mean:  10902.896","iter:  994<br />train_rmse_mean:  10896.175","iter:  995<br />train_rmse_mean:  10890.552","iter:  996<br />train_rmse_mean:  10885.338","iter:  997<br />train_rmse_mean:  10879.718","iter:  998<br />train_rmse_mean:  10874.232","iter:  999<br />train_rmse_mean:  10868.755","iter: 1000<br />train_rmse_mean:  10861.923","iter: 1001<br />train_rmse_mean:  10857.088","iter: 1002<br />train_rmse_mean:  10851.223","iter: 1003<br />train_rmse_mean:  10844.521","iter: 1004<br />train_rmse_mean:  10839.047","iter: 1005<br />train_rmse_mean:  10832.813","iter: 1006<br />train_rmse_mean:  10828.153","iter: 1007<br />train_rmse_mean:  10822.523","iter: 1008<br />train_rmse_mean:  10816.559","iter: 1009<br />train_rmse_mean:  10811.723","iter: 1010<br />train_rmse_mean:  10805.715","iter: 1011<br />train_rmse_mean:  10799.670","iter: 1012<br />train_rmse_mean:  10794.275","iter: 1013<br />train_rmse_mean:  10787.814","iter: 1014<br />train_rmse_mean:  10781.692","iter: 1015<br />train_rmse_mean:  10776.766","iter: 1016<br />train_rmse_mean:  10771.881","iter: 1017<br />train_rmse_mean:  10766.945","iter: 1018<br />train_rmse_mean:  10762.209","iter: 1019<br />train_rmse_mean:  10756.840","iter: 1020<br />train_rmse_mean:  10750.822","iter: 1021<br />train_rmse_mean:  10746.260","iter: 1022<br />train_rmse_mean:  10742.280","iter: 1023<br />train_rmse_mean:  10737.733","iter: 1024<br />train_rmse_mean:  10732.039","iter: 1025<br />train_rmse_mean:  10728.160","iter: 1026<br />train_rmse_mean:  10722.484","iter: 1027<br />train_rmse_mean:  10717.355","iter: 1028<br />train_rmse_mean:  10711.214","iter: 1029<br />train_rmse_mean:  10706.588","iter: 1030<br />train_rmse_mean:  10701.803","iter: 1031<br />train_rmse_mean:  10697.218","iter: 1032<br />train_rmse_mean:  10692.298","iter: 1033<br />train_rmse_mean:  10687.714","iter: 1034<br />train_rmse_mean:  10683.090","iter: 1035<br />train_rmse_mean:  10677.127","iter: 1036<br />train_rmse_mean:  10670.883","iter: 1037<br />train_rmse_mean:  10666.413","iter: 1038<br />train_rmse_mean:  10661.951","iter: 1039<br />train_rmse_mean:  10656.935","iter: 1040<br />train_rmse_mean:  10651.690","iter: 1041<br />train_rmse_mean:  10646.896","iter: 1042<br />train_rmse_mean:  10642.152","iter: 1043<br />train_rmse_mean:  10636.990","iter: 1044<br />train_rmse_mean:  10631.061","iter: 1045<br />train_rmse_mean:  10626.935","iter: 1046<br />train_rmse_mean:  10622.427","iter: 1047<br />train_rmse_mean:  10616.742","iter: 1048<br />train_rmse_mean:  10611.029","iter: 1049<br />train_rmse_mean:  10605.578","iter: 1050<br />train_rmse_mean:  10600.727","iter: 1051<br />train_rmse_mean:  10595.781","iter: 1052<br />train_rmse_mean:  10590.123","iter: 1053<br />train_rmse_mean:  10585.204","iter: 1054<br />train_rmse_mean:  10579.879","iter: 1055<br />train_rmse_mean:  10574.899","iter: 1056<br />train_rmse_mean:  10569.914","iter: 1057<br />train_rmse_mean:  10564.651","iter: 1058<br />train_rmse_mean:  10559.390","iter: 1059<br />train_rmse_mean:  10553.971","iter: 1060<br />train_rmse_mean:  10549.979","iter: 1061<br />train_rmse_mean:  10546.181","iter: 1062<br />train_rmse_mean:  10541.708","iter: 1063<br />train_rmse_mean:  10535.788","iter: 1064<br />train_rmse_mean:  10530.766","iter: 1065<br />train_rmse_mean:  10525.883","iter: 1066<br />train_rmse_mean:  10520.413","iter: 1067<br />train_rmse_mean:  10515.126","iter: 1068<br />train_rmse_mean:  10510.270","iter: 1069<br />train_rmse_mean:  10505.285","iter: 1070<br />train_rmse_mean:  10499.284","iter: 1071<br />train_rmse_mean:  10495.422","iter: 1072<br />train_rmse_mean:  10490.239","iter: 1073<br />train_rmse_mean:  10485.564","iter: 1074<br />train_rmse_mean:  10479.486","iter: 1075<br />train_rmse_mean:  10474.305","iter: 1076<br />train_rmse_mean:  10469.774","iter: 1077<br />train_rmse_mean:  10464.428","iter: 1078<br />train_rmse_mean:  10459.829","iter: 1079<br />train_rmse_mean:  10454.043","iter: 1080<br />train_rmse_mean:  10449.430","iter: 1081<br />train_rmse_mean:  10444.689","iter: 1082<br />train_rmse_mean:  10439.476","iter: 1083<br />train_rmse_mean:  10435.925","iter: 1084<br />train_rmse_mean:  10431.156","iter: 1085<br />train_rmse_mean:  10425.702","iter: 1086<br />train_rmse_mean:  10421.602","iter: 1087<br />train_rmse_mean:  10416.837","iter: 1088<br />train_rmse_mean:  10411.303","iter: 1089<br />train_rmse_mean:  10406.816","iter: 1090<br />train_rmse_mean:  10401.553","iter: 1091<br />train_rmse_mean:  10395.830","iter: 1092<br />train_rmse_mean:  10390.030","iter: 1093<br />train_rmse_mean:  10384.213","iter: 1094<br />train_rmse_mean:  10379.906","iter: 1095<br />train_rmse_mean:  10375.630","iter: 1096<br />train_rmse_mean:  10370.437","iter: 1097<br />train_rmse_mean:  10365.179","iter: 1098<br />train_rmse_mean:  10360.604","iter: 1099<br />train_rmse_mean:  10355.497","iter: 1100<br />train_rmse_mean:  10350.475","iter: 1101<br />train_rmse_mean:  10345.475","iter: 1102<br />train_rmse_mean:  10341.872","iter: 1103<br />train_rmse_mean:  10336.875","iter: 1104<br />train_rmse_mean:  10331.358","iter: 1105<br />train_rmse_mean:  10327.657","iter: 1106<br />train_rmse_mean:  10321.551","iter: 1107<br />train_rmse_mean:  10318.472","iter: 1108<br />train_rmse_mean:  10313.362","iter: 1109<br />train_rmse_mean:  10309.571","iter: 1110<br />train_rmse_mean:  10305.305","iter: 1111<br />train_rmse_mean:  10299.504","iter: 1112<br />train_rmse_mean:  10294.871","iter: 1113<br />train_rmse_mean:  10289.965","iter: 1114<br />train_rmse_mean:  10285.694","iter: 1115<br />train_rmse_mean:  10282.472","iter: 1116<br />train_rmse_mean:  10278.775","iter: 1117<br />train_rmse_mean:  10273.828","iter: 1118<br />train_rmse_mean:  10268.974","iter: 1119<br />train_rmse_mean:  10264.265","iter: 1120<br />train_rmse_mean:  10259.692","iter: 1121<br />train_rmse_mean:  10255.875","iter: 1122<br />train_rmse_mean:  10251.642","iter: 1123<br />train_rmse_mean:  10246.534","iter: 1124<br />train_rmse_mean:  10241.233","iter: 1125<br />train_rmse_mean:  10236.853","iter: 1126<br />train_rmse_mean:  10232.324","iter: 1127<br />train_rmse_mean:  10227.704","iter: 1128<br />train_rmse_mean:  10222.564","iter: 1129<br />train_rmse_mean:  10218.185","iter: 1130<br />train_rmse_mean:  10214.594","iter: 1131<br />train_rmse_mean:  10209.411","iter: 1132<br />train_rmse_mean:  10205.821","iter: 1133<br />train_rmse_mean:  10200.812","iter: 1134<br />train_rmse_mean:  10196.625","iter: 1135<br />train_rmse_mean:  10191.628","iter: 1136<br />train_rmse_mean:  10186.721","iter: 1137<br />train_rmse_mean:  10181.122","iter: 1138<br />train_rmse_mean:  10177.483","iter: 1139<br />train_rmse_mean:  10173.463","iter: 1140<br />train_rmse_mean:  10168.519","iter: 1141<br />train_rmse_mean:  10164.139","iter: 1142<br />train_rmse_mean:  10159.541","iter: 1143<br />train_rmse_mean:  10154.255","iter: 1144<br />train_rmse_mean:  10148.818","iter: 1145<br />train_rmse_mean:  10144.121","iter: 1146<br />train_rmse_mean:  10139.456","iter: 1147<br />train_rmse_mean:  10135.559","iter: 1148<br />train_rmse_mean:  10130.240","iter: 1149<br />train_rmse_mean:  10125.800","iter: 1150<br />train_rmse_mean:  10120.464","iter: 1151<br />train_rmse_mean:  10115.249","iter: 1152<br />train_rmse_mean:  10110.897","iter: 1153<br />train_rmse_mean:  10104.909","iter: 1154<br />train_rmse_mean:  10100.458","iter: 1155<br />train_rmse_mean:  10094.183","iter: 1156<br />train_rmse_mean:  10089.146","iter: 1157<br />train_rmse_mean:  10085.898","iter: 1158<br />train_rmse_mean:  10080.726","iter: 1159<br />train_rmse_mean:  10077.152","iter: 1160<br />train_rmse_mean:  10073.044","iter: 1161<br />train_rmse_mean:  10068.191","iter: 1162<br />train_rmse_mean:  10063.916","iter: 1163<br />train_rmse_mean:  10059.933","iter: 1164<br />train_rmse_mean:  10055.986","iter: 1165<br />train_rmse_mean:  10052.314","iter: 1166<br />train_rmse_mean:  10047.555","iter: 1167<br />train_rmse_mean:  10043.173","iter: 1168<br />train_rmse_mean:  10038.851","iter: 1169<br />train_rmse_mean:  10034.164","iter: 1170<br />train_rmse_mean:  10028.410","iter: 1171<br />train_rmse_mean:  10023.545","iter: 1172<br />train_rmse_mean:  10018.623","iter: 1173<br />train_rmse_mean:  10013.235","iter: 1174<br />train_rmse_mean:  10008.824","iter: 1175<br />train_rmse_mean:  10004.888","iter: 1176<br />train_rmse_mean:   9999.535","iter: 1177<br />train_rmse_mean:   9994.411","iter: 1178<br />train_rmse_mean:   9989.828","iter: 1179<br />train_rmse_mean:   9985.256","iter: 1180<br />train_rmse_mean:   9980.620","iter: 1181<br />train_rmse_mean:   9975.739","iter: 1182<br />train_rmse_mean:   9971.554","iter: 1183<br />train_rmse_mean:   9967.790","iter: 1184<br />train_rmse_mean:   9962.556","iter: 1185<br />train_rmse_mean:   9957.649","iter: 1186<br />train_rmse_mean:   9952.859","iter: 1187<br />train_rmse_mean:   9948.264","iter: 1188<br />train_rmse_mean:   9944.457","iter: 1189<br />train_rmse_mean:   9939.708","iter: 1190<br />train_rmse_mean:   9935.638","iter: 1191<br />train_rmse_mean:   9931.434","iter: 1192<br />train_rmse_mean:   9927.062","iter: 1193<br />train_rmse_mean:   9922.235","iter: 1194<br />train_rmse_mean:   9919.165","iter: 1195<br />train_rmse_mean:   9915.291","iter: 1196<br />train_rmse_mean:   9912.185","iter: 1197<br />train_rmse_mean:   9908.145","iter: 1198<br />train_rmse_mean:   9904.360","iter: 1199<br />train_rmse_mean:   9899.602","iter: 1200<br />train_rmse_mean:   9895.923","iter: 1201<br />train_rmse_mean:   9891.362","iter: 1202<br />train_rmse_mean:   9887.584","iter: 1203<br />train_rmse_mean:   9883.894","iter: 1204<br />train_rmse_mean:   9879.466","iter: 1205<br />train_rmse_mean:   9874.237","iter: 1206<br />train_rmse_mean:   9869.005","iter: 1207<br />train_rmse_mean:   9863.779","iter: 1208<br />train_rmse_mean:   9859.503","iter: 1209<br />train_rmse_mean:   9855.010","iter: 1210<br />train_rmse_mean:   9851.049","iter: 1211<br />train_rmse_mean:   9846.278","iter: 1212<br />train_rmse_mean:   9841.924","iter: 1213<br />train_rmse_mean:   9838.955","iter: 1214<br />train_rmse_mean:   9833.757","iter: 1215<br />train_rmse_mean:   9831.103","iter: 1216<br />train_rmse_mean:   9827.030","iter: 1217<br />train_rmse_mean:   9823.367","iter: 1218<br />train_rmse_mean:   9818.830","iter: 1219<br />train_rmse_mean:   9814.256","iter: 1220<br />train_rmse_mean:   9809.616","iter: 1221<br />train_rmse_mean:   9804.282","iter: 1222<br />train_rmse_mean:   9800.090","iter: 1223<br />train_rmse_mean:   9795.505","iter: 1224<br />train_rmse_mean:   9790.840","iter: 1225<br />train_rmse_mean:   9786.488","iter: 1226<br />train_rmse_mean:   9782.813","iter: 1227<br />train_rmse_mean:   9778.585","iter: 1228<br />train_rmse_mean:   9774.351","iter: 1229<br />train_rmse_mean:   9769.406","iter: 1230<br />train_rmse_mean:   9765.809","iter: 1231<br />train_rmse_mean:   9762.427","iter: 1232<br />train_rmse_mean:   9758.160","iter: 1233<br />train_rmse_mean:   9755.575","iter: 1234<br />train_rmse_mean:   9751.545","iter: 1235<br />train_rmse_mean:   9747.690","iter: 1236<br />train_rmse_mean:   9743.811","iter: 1237<br />train_rmse_mean:   9739.426","iter: 1238<br />train_rmse_mean:   9735.617","iter: 1239<br />train_rmse_mean:   9731.623","iter: 1240<br />train_rmse_mean:   9727.259","iter: 1241<br />train_rmse_mean:   9723.532","iter: 1242<br />train_rmse_mean:   9719.369","iter: 1243<br />train_rmse_mean:   9715.641","iter: 1244<br />train_rmse_mean:   9712.676","iter: 1245<br />train_rmse_mean:   9708.684","iter: 1246<br />train_rmse_mean:   9704.373","iter: 1247<br />train_rmse_mean:   9699.672","iter: 1248<br />train_rmse_mean:   9696.013","iter: 1249<br />train_rmse_mean:   9692.376","iter: 1250<br />train_rmse_mean:   9687.804","iter: 1251<br />train_rmse_mean:   9682.748","iter: 1252<br />train_rmse_mean:   9678.786","iter: 1253<br />train_rmse_mean:   9675.078","iter: 1254<br />train_rmse_mean:   9670.370","iter: 1255<br />train_rmse_mean:   9667.094","iter: 1256<br />train_rmse_mean:   9663.826","iter: 1257<br />train_rmse_mean:   9659.418","iter: 1258<br />train_rmse_mean:   9655.691","iter: 1259<br />train_rmse_mean:   9652.298","iter: 1260<br />train_rmse_mean:   9647.394","iter: 1261<br />train_rmse_mean:   9642.664","iter: 1262<br />train_rmse_mean:   9638.896","iter: 1263<br />train_rmse_mean:   9634.261","iter: 1264<br />train_rmse_mean:   9629.721","iter: 1265<br />train_rmse_mean:   9625.869","iter: 1266<br />train_rmse_mean:   9621.795","iter: 1267<br />train_rmse_mean:   9617.195","iter: 1268<br />train_rmse_mean:   9613.086","iter: 1269<br />train_rmse_mean:   9607.831","iter: 1270<br />train_rmse_mean:   9602.781","iter: 1271<br />train_rmse_mean:   9598.308","iter: 1272<br />train_rmse_mean:   9593.716","iter: 1273<br />train_rmse_mean:   9588.518","iter: 1274<br />train_rmse_mean:   9584.555","iter: 1275<br />train_rmse_mean:   9579.561","iter: 1276<br />train_rmse_mean:   9575.062","iter: 1277<br />train_rmse_mean:   9571.711","iter: 1278<br />train_rmse_mean:   9567.367","iter: 1279<br />train_rmse_mean:   9563.033","iter: 1280<br />train_rmse_mean:   9558.863","iter: 1281<br />train_rmse_mean:   9554.834","iter: 1282<br />train_rmse_mean:   9550.055","iter: 1283<br />train_rmse_mean:   9545.893","iter: 1284<br />train_rmse_mean:   9541.803","iter: 1285<br />train_rmse_mean:   9537.853","iter: 1286<br />train_rmse_mean:   9533.090","iter: 1287<br />train_rmse_mean:   9529.995","iter: 1288<br />train_rmse_mean:   9526.055","iter: 1289<br />train_rmse_mean:   9521.353","iter: 1290<br />train_rmse_mean:   9517.473","iter: 1291<br />train_rmse_mean:   9514.180","iter: 1292<br />train_rmse_mean:   9510.747","iter: 1293<br />train_rmse_mean:   9505.512","iter: 1294<br />train_rmse_mean:   9501.900","iter: 1295<br />train_rmse_mean:   9498.324","iter: 1296<br />train_rmse_mean:   9493.960","iter: 1297<br />train_rmse_mean:   9488.758","iter: 1298<br />train_rmse_mean:   9483.914","iter: 1299<br />train_rmse_mean:   9479.256","iter: 1300<br />train_rmse_mean:   9475.582","iter: 1301<br />train_rmse_mean:   9471.667","iter: 1302<br />train_rmse_mean:   9468.426","iter: 1303<br />train_rmse_mean:   9464.314","iter: 1304<br />train_rmse_mean:   9460.862","iter: 1305<br />train_rmse_mean:   9456.463","iter: 1306<br />train_rmse_mean:   9451.878","iter: 1307<br />train_rmse_mean:   9448.290","iter: 1308<br />train_rmse_mean:   9443.763","iter: 1309<br />train_rmse_mean:   9439.391","iter: 1310<br />train_rmse_mean:   9436.190","iter: 1311<br />train_rmse_mean:   9431.569","iter: 1312<br />train_rmse_mean:   9427.908","iter: 1313<br />train_rmse_mean:   9423.413","iter: 1314<br />train_rmse_mean:   9418.950","iter: 1315<br />train_rmse_mean:   9415.003","iter: 1316<br />train_rmse_mean:   9410.324","iter: 1317<br />train_rmse_mean:   9407.048","iter: 1318<br />train_rmse_mean:   9403.440","iter: 1319<br />train_rmse_mean:   9399.170","iter: 1320<br />train_rmse_mean:   9394.211","iter: 1321<br />train_rmse_mean:   9390.660","iter: 1322<br />train_rmse_mean:   9387.027","iter: 1323<br />train_rmse_mean:   9382.539","iter: 1324<br />train_rmse_mean:   9378.964","iter: 1325<br />train_rmse_mean:   9374.688","iter: 1326<br />train_rmse_mean:   9370.496","iter: 1327<br />train_rmse_mean:   9366.814","iter: 1328<br />train_rmse_mean:   9363.071","iter: 1329<br />train_rmse_mean:   9359.008","iter: 1330<br />train_rmse_mean:   9354.143","iter: 1331<br />train_rmse_mean:   9350.183","iter: 1332<br />train_rmse_mean:   9345.874","iter: 1333<br />train_rmse_mean:   9342.769","iter: 1334<br />train_rmse_mean:   9337.961","iter: 1335<br />train_rmse_mean:   9334.479","iter: 1336<br />train_rmse_mean:   9330.226","iter: 1337<br />train_rmse_mean:   9327.019","iter: 1338<br />train_rmse_mean:   9323.788","iter: 1339<br />train_rmse_mean:   9320.651","iter: 1340<br />train_rmse_mean:   9317.280","iter: 1341<br />train_rmse_mean:   9313.519","iter: 1342<br />train_rmse_mean:   9310.536","iter: 1343<br />train_rmse_mean:   9305.756","iter: 1344<br />train_rmse_mean:   9302.164","iter: 1345<br />train_rmse_mean:   9297.898","iter: 1346<br />train_rmse_mean:   9293.489","iter: 1347<br />train_rmse_mean:   9288.929","iter: 1348<br />train_rmse_mean:   9285.146","iter: 1349<br />train_rmse_mean:   9280.562","iter: 1350<br />train_rmse_mean:   9277.304","iter: 1351<br />train_rmse_mean:   9272.733","iter: 1352<br />train_rmse_mean:   9268.999","iter: 1353<br />train_rmse_mean:   9265.530","iter: 1354<br />train_rmse_mean:   9261.601","iter: 1355<br />train_rmse_mean:   9257.689","iter: 1356<br />train_rmse_mean:   9253.940","iter: 1357<br />train_rmse_mean:   9249.958","iter: 1358<br />train_rmse_mean:   9246.931","iter: 1359<br />train_rmse_mean:   9243.237","iter: 1360<br />train_rmse_mean:   9239.385","iter: 1361<br />train_rmse_mean:   9234.721","iter: 1362<br />train_rmse_mean:   9230.651","iter: 1363<br />train_rmse_mean:   9226.137","iter: 1364<br />train_rmse_mean:   9221.894","iter: 1365<br />train_rmse_mean:   9218.925","iter: 1366<br />train_rmse_mean:   9215.077","iter: 1367<br />train_rmse_mean:   9211.533","iter: 1368<br />train_rmse_mean:   9208.002","iter: 1369<br />train_rmse_mean:   9203.830","iter: 1370<br />train_rmse_mean:   9200.384","iter: 1371<br />train_rmse_mean:   9196.249","iter: 1372<br />train_rmse_mean:   9193.143","iter: 1373<br />train_rmse_mean:   9188.778","iter: 1374<br />train_rmse_mean:   9185.299","iter: 1375<br />train_rmse_mean:   9181.388","iter: 1376<br />train_rmse_mean:   9177.976","iter: 1377<br />train_rmse_mean:   9173.677","iter: 1378<br />train_rmse_mean:   9169.797","iter: 1379<br />train_rmse_mean:   9165.801","iter: 1380<br />train_rmse_mean:   9162.430","iter: 1381<br />train_rmse_mean:   9159.073","iter: 1382<br />train_rmse_mean:   9154.681","iter: 1383<br />train_rmse_mean:   9150.283","iter: 1384<br />train_rmse_mean:   9146.951","iter: 1385<br />train_rmse_mean:   9142.573","iter: 1386<br />train_rmse_mean:   9138.643","iter: 1387<br />train_rmse_mean:   9134.380","iter: 1388<br />train_rmse_mean:   9131.052","iter: 1389<br />train_rmse_mean:   9127.147","iter: 1390<br />train_rmse_mean:   9124.210","iter: 1391<br />train_rmse_mean:   9120.798","iter: 1392<br />train_rmse_mean:   9115.926","iter: 1393<br />train_rmse_mean:   9112.317","iter: 1394<br />train_rmse_mean:   9108.845","iter: 1395<br />train_rmse_mean:   9105.244","iter: 1396<br />train_rmse_mean:   9101.242","iter: 1397<br />train_rmse_mean:   9097.493","iter: 1398<br />train_rmse_mean:   9093.423","iter: 1399<br />train_rmse_mean:   9090.733","iter: 1400<br />train_rmse_mean:   9086.857","iter: 1401<br />train_rmse_mean:   9083.253","iter: 1402<br />train_rmse_mean:   9079.791","iter: 1403<br />train_rmse_mean:   9076.546","iter: 1404<br />train_rmse_mean:   9072.520","iter: 1405<br />train_rmse_mean:   9069.232","iter: 1406<br />train_rmse_mean:   9066.260","iter: 1407<br />train_rmse_mean:   9062.163","iter: 1408<br />train_rmse_mean:   9058.600","iter: 1409<br />train_rmse_mean:   9055.085","iter: 1410<br />train_rmse_mean:   9051.417","iter: 1411<br />train_rmse_mean:   9047.625","iter: 1412<br />train_rmse_mean:   9043.564","iter: 1413<br />train_rmse_mean:   9039.103","iter: 1414<br />train_rmse_mean:   9035.708","iter: 1415<br />train_rmse_mean:   9032.270","iter: 1416<br />train_rmse_mean:   9028.837","iter: 1417<br />train_rmse_mean:   9025.093","iter: 1418<br />train_rmse_mean:   9022.111","iter: 1419<br />train_rmse_mean:   9018.476","iter: 1420<br />train_rmse_mean:   9015.549","iter: 1421<br />train_rmse_mean:   9010.990","iter: 1422<br />train_rmse_mean:   9006.926","iter: 1423<br />train_rmse_mean:   9003.721","iter: 1424<br />train_rmse_mean:   8999.947","iter: 1425<br />train_rmse_mean:   8997.097","iter: 1426<br />train_rmse_mean:   8993.601","iter: 1427<br />train_rmse_mean:   8990.060","iter: 1428<br />train_rmse_mean:   8985.714","iter: 1429<br />train_rmse_mean:   8981.461","iter: 1430<br />train_rmse_mean:   8978.143","iter: 1431<br />train_rmse_mean:   8975.452","iter: 1432<br />train_rmse_mean:   8970.972","iter: 1433<br />train_rmse_mean:   8967.869","iter: 1434<br />train_rmse_mean:   8964.461","iter: 1435<br />train_rmse_mean:   8961.536","iter: 1436<br />train_rmse_mean:   8958.229","iter: 1437<br />train_rmse_mean:   8954.811","iter: 1438<br />train_rmse_mean:   8951.668","iter: 1439<br />train_rmse_mean:   8948.546","iter: 1440<br />train_rmse_mean:   8945.492","iter: 1441<br />train_rmse_mean:   8942.080","iter: 1442<br />train_rmse_mean:   8938.484","iter: 1443<br />train_rmse_mean:   8934.318","iter: 1444<br />train_rmse_mean:   8931.053","iter: 1445<br />train_rmse_mean:   8927.490","iter: 1446<br />train_rmse_mean:   8923.145","iter: 1447<br />train_rmse_mean:   8920.330","iter: 1448<br />train_rmse_mean:   8916.142","iter: 1449<br />train_rmse_mean:   8912.635","iter: 1450<br />train_rmse_mean:   8908.706","iter: 1451<br />train_rmse_mean:   8905.106","iter: 1452<br />train_rmse_mean:   8902.040","iter: 1453<br />train_rmse_mean:   8898.614","iter: 1454<br />train_rmse_mean:   8894.246","iter: 1455<br />train_rmse_mean:   8890.402","iter: 1456<br />train_rmse_mean:   8886.603","iter: 1457<br />train_rmse_mean:   8883.509","iter: 1458<br />train_rmse_mean:   8879.531","iter: 1459<br />train_rmse_mean:   8875.624","iter: 1460<br />train_rmse_mean:   8872.291","iter: 1461<br />train_rmse_mean:   8869.101","iter: 1462<br />train_rmse_mean:   8864.876","iter: 1463<br />train_rmse_mean:   8860.931","iter: 1464<br />train_rmse_mean:   8857.099","iter: 1465<br />train_rmse_mean:   8853.784","iter: 1466<br />train_rmse_mean:   8850.683","iter: 1467<br />train_rmse_mean:   8847.317","iter: 1468<br />train_rmse_mean:   8844.392","iter: 1469<br />train_rmse_mean:   8840.762","iter: 1470<br />train_rmse_mean:   8837.520","iter: 1471<br />train_rmse_mean:   8834.656","iter: 1472<br />train_rmse_mean:   8830.800","iter: 1473<br />train_rmse_mean:   8827.253","iter: 1474<br />train_rmse_mean:   8823.809","iter: 1475<br />train_rmse_mean:   8820.353","iter: 1476<br />train_rmse_mean:   8816.190","iter: 1477<br />train_rmse_mean:   8812.797","iter: 1478<br />train_rmse_mean:   8808.782","iter: 1479<br />train_rmse_mean:   8805.929","iter: 1480<br />train_rmse_mean:   8802.833","iter: 1481<br />train_rmse_mean:   8799.534","iter: 1482<br />train_rmse_mean:   8795.639","iter: 1483<br />train_rmse_mean:   8791.572","iter: 1484<br />train_rmse_mean:   8787.593","iter: 1485<br />train_rmse_mean:   8784.456","iter: 1486<br />train_rmse_mean:   8781.395","iter: 1487<br />train_rmse_mean:   8778.234","iter: 1488<br />train_rmse_mean:   8774.885","iter: 1489<br />train_rmse_mean:   8771.571","iter: 1490<br />train_rmse_mean:   8768.439","iter: 1491<br />train_rmse_mean:   8765.892","iter: 1492<br />train_rmse_mean:   8762.951","iter: 1493<br />train_rmse_mean:   8759.774","iter: 1494<br />train_rmse_mean:   8755.796","iter: 1495<br />train_rmse_mean:   8752.680","iter: 1496<br />train_rmse_mean:   8749.571","iter: 1497<br />train_rmse_mean:   8746.284","iter: 1498<br />train_rmse_mean:   8742.166","iter: 1499<br />train_rmse_mean:   8738.887","iter: 1500<br />train_rmse_mean:   8735.688","iter: 1501<br />train_rmse_mean:   8732.739","iter: 1502<br />train_rmse_mean:   8729.091","iter: 1503<br />train_rmse_mean:   8724.786","iter: 1504<br />train_rmse_mean:   8721.628","iter: 1505<br />train_rmse_mean:   8718.069","iter: 1506<br />train_rmse_mean:   8714.482","iter: 1507<br />train_rmse_mean:   8711.803","iter: 1508<br />train_rmse_mean:   8708.596","iter: 1509<br />train_rmse_mean:   8703.956","iter: 1510<br />train_rmse_mean:   8700.515","iter: 1511<br />train_rmse_mean:   8697.092","iter: 1512<br />train_rmse_mean:   8693.831","iter: 1513<br />train_rmse_mean:   8690.005","iter: 1514<br />train_rmse_mean:   8687.667","iter: 1515<br />train_rmse_mean:   8684.373","iter: 1516<br />train_rmse_mean:   8680.412","iter: 1517<br />train_rmse_mean:   8677.044","iter: 1518<br />train_rmse_mean:   8673.517","iter: 1519<br />train_rmse_mean:   8670.353","iter: 1520<br />train_rmse_mean:   8666.845","iter: 1521<br />train_rmse_mean:   8663.447","iter: 1522<br />train_rmse_mean:   8660.482","iter: 1523<br />train_rmse_mean:   8656.779","iter: 1524<br />train_rmse_mean:   8652.655","iter: 1525<br />train_rmse_mean:   8649.318","iter: 1526<br />train_rmse_mean:   8645.726","iter: 1527<br />train_rmse_mean:   8642.590","iter: 1528<br />train_rmse_mean:   8638.893","iter: 1529<br />train_rmse_mean:   8635.569","iter: 1530<br />train_rmse_mean:   8631.420","iter: 1531<br />train_rmse_mean:   8627.572","iter: 1532<br />train_rmse_mean:   8624.690","iter: 1533<br />train_rmse_mean:   8621.267","iter: 1534<br />train_rmse_mean:   8618.769","iter: 1535<br />train_rmse_mean:   8615.531","iter: 1536<br />train_rmse_mean:   8612.921","iter: 1537<br />train_rmse_mean:   8609.312","iter: 1538<br />train_rmse_mean:   8606.291","iter: 1539<br />train_rmse_mean:   8603.064","iter: 1540<br />train_rmse_mean:   8600.330","iter: 1541<br />train_rmse_mean:   8597.303","iter: 1542<br />train_rmse_mean:   8593.786","iter: 1543<br />train_rmse_mean:   8590.856"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543],"y":[190839.01875,185545.0203125,180327.728125,175341.2921875,170447.9765625,165795.725,161247.5984375,156838.209375,152552.8984375,148376.5875,144281.7742188,140398.5109375,136638.3726563,132946.8546875,129395.0578125,125978.4570313,122614.1710936,119376.8882812,116209.0078124,113144.5960937,110222.7695314,107399.8703126,104581.6554689,101873.3257813,99281.8414062,96752.4499998,94306.7203124,91975.9773437,89693.1367187,87496.7640625,85374.522656,83297.9554688,81310.051172,79397.2457033,77512.4988281,75670.2480469,73893.4398439,72191.9363283,70622.7210938,69053.6507813,67499.2332029,66005.9507812,64570.7816407,63218.9859375,61906.5351562,60573.6285157,59300.795703,58119.4066405,56963.5062501,55815.8968751,54738.271875,53686.8007812,52713.5253906,51773.9429688,50848.0839843,49958.4058593,49120.3406251,48294.5066408,47502.5628905,46747.81875,45988.6523437,45263.3119142,44630.2982421,43914.6519533,43266.3041015,42673.136328,42133.0183593,41545.1087891,41033.9257813,40544.7937499,40052.5119141,39581.3291016,39143.4638673,38694.0199219,38279.7888672,37859.7888671,37457.465625,37096.9378905,36720.8400391,36397.0896484,36043.3496094,35739.5550783,35456.6365232,35156.521875,34891.3748045,34613.7095705,34365.9253906,34092.3308594,33875.2541016,33637.5753906,33439.0980468,33227.2505862,33017.1250001,32826.9240235,32651.6726562,32460.4632813,32292.2464844,32116.6580078,31945.4039062,31784.3529297,31628.0578125,31493.5390625,31340.090039,31208.5265625,31092.871875,31012.0914062,30894.6003905,30799.3076171,30710.4488282,30608.4060546,30495.2634764,30393.0451171,30318.4486328,30216.9236327,30135.4878908,30062.3058592,29939.3265626,29856.4865235,29751.0591798,29677.9468749,29607.2392577,29502.2808595,29434.1201173,29361.4992188,29294.3677733,29239.1509765,29194.570703,29154.0597656,29079.4375,29007.1000001,28945.1246094,28875.1240234,28825.6386718,28754.513867,28728.8509765,28715.0093751,28670.1580078,28593.045703,28581.7013672,28549.9773437,28504.5117189,28453.2638672,28403.3951173,28361.0908205,28304.2421875,28272.5701171,28232.786914,28189.859375,28139.8787109,28086.4734375,28054.4146484,28012.8103517,27982.1849611,27967.6251953,27915.8365234,27873.3166015,27804.0933593,27792.8621094,27757.0070313,27713.5208985,27669.6189453,27633.0197266,27603.9484375,27583.1722654,27548.9800782,27529.9226562,27506.1062501,27496.9033203,27492.2566405,27462.4515625,27433.6335937,27406.4402343,27397.2148437,27377.8091797,27364.7861327,27345.2736328,27337.0736327,27307.6181641,27282.4222656,27285.1843752,27264.478125,27262.9982421,27240.283789,27203.4148438,27185.4964845,27172.9025391,27172.8267579,27149.8773436,27129.2736328,27094.7917969,27083.636133,27075.584375,27059.9521483,27028.0917968,27007.0085939,26975.3464845,26934.8771486,26928.6873048,26921.4566407,26904.4109375,26857.5416015,26836.2894531,26827.2320314,26768.4365234,26750.2578125,26743.9232421,26692.9351563,26693.524414,26673.5339844,26655.4259766,26627.4296875,26610.5248049,26600.0935547,26574.5453124,26554.9607421,26539.2484375,26528.1105468,26540.2406251,26513.1417968,26499.5998048,26465.8984375,26455.6111328,26454.8582034,26442.6255859,26430.6501954,26409.0175781,26401.1703125,26374.80625,26372.1999999,26346.0958983,26328.8298829,26325.0966798,26312.4216796,26275.1107423,26247.5380861,26222.5505859,26204.486914,26180.3732423,26157.7283204,26145.1699219,26125.5144532,26119.8890625,26118.4421875,26111.5425782,26093.5105469,26049.4185547,26036.997461,26019.9888672,25988.6214843,25974.0267579,25961.3968751,25921.4298827,25912.0195313,25905.1753905,25888.8376953,25873.4541016,25870.4447266,25858.566406,25857.7228517,25844.0351562,25840.0386717,25812.1417968,25804.540625,25814.9886719,25810.4431642,25780.8580075,25765.6625,25753.4753906,25748.7875,25748.3839843,25724.5376953,25716.8535155,25702.0845703,25703.1031249,25691.0271485,25685.3064453,25669.8580077,25664.1896484,25644.9001951,25636.6078125,25621.7632812,25594.7802733,25586.165625,25579.6691408,25541.8974608,25534.6503908,25525.0400391,25516.9978516,25498.4609375,25491.3417969,25486.1314456,25466.4166017,25453.0003905,25431.5380859,25428.4009766,25418.4738282,25400.7841796,25370.9605469,25350.7560547,25354.9718749,25357.6990235,25342.5822265,25325.0511719,25322.1744142,25319.7402343,25333.8556639,25330.5873046,25307.8402344,25285.6197266,25281.440039,25269.8374999,25265.3001955,25267.513086,25266.064844,25251.7289062,25239.8882811,25230.2037109,25225.3017579,25228.6757814,25249.4498046,25247.7263671,25234.9253905,25231.9078126,25227.7435547,25211.1605468,25201.1890625,25178.9808595,25160.1128907,25157.2263672,25143.4923828,25138.0242189,25126.0802734,25123.8869139,25113.1691405,25106.6240234,25096.0316407,25091.6173827,25082.8779297,25070.8138671,25066.7177735,25072.956836,25053.4843751,25049.7724609,25038.490625,25039.3847656,25035.6609376,25023.6142578,25014.6339844,25011.8658205,25000.7232421,24994.9732422,24982.0414062,24993.3728515,24979.7748046,24980.8142579,24989.591797,24975.4792968,24973.9417969,24964.621875,24950.284375,24943.341211,24932.3998048,24921.0058594,24911.7630859,24899.2302734,24899.3595703,24887.8298829,24883.3314453,24882.6443361,24870.3152345,24872.1082031,24866.56875,24854.2126954,24845.6957031,24832.2718749,24825.7623047,24818.2683595,24813.6931642,24799.240039,24775,24773.4179688,24770.2298828,24760.0982422,24747.8087892,24736.0117188,24735.3554689,24738.4658204,24737.6115236,24730.6931641,24724.3310547,24725.1798827,24719.7091797,24714.6339844,24704.6771483,24699.7611326,24687.1999999,24672.0419921,24662.1232422,24639.6351562,24639.0326173,24627.2710937,24625.0802734,24621.3730467,24610.8480468,24605.1828124,24595.9468751,24577.9941406,24576.9720703,24567.5007812,24566.8912108,24569.0748047,24563.9982421,24559.4826171,24554.283789,24555.0291014,24553.6560545,24557.600586,24560.5148438,24557.0373046,24551.5734376,24545.9224608,24534.6947266,24531.5322266,24521.7878906,24517.6648436,24508.9173827,24504.3277343,24502.1136718,24494.6701171,24490.0267576,24466.3683595,24457.7654297,24454.5300782,24457.3607424,24460.8773441,24454.3390624,24453.3607422,24450.9744142,24436.45,24434.6302734,24427.8435548,24418.903711,24415.079492,24403.1374998,24405.2150392,24396.886328,24386.0658203,24373.2064453,24362.5302735,24352.2480468,24338.0535157,24331.2335938,24325.8748046,24324.0978514,24313.8476562,24298.7587891,24298.9958984,24282.9791017,24271.7537108,24270.2999999,24259.7248047,24257.5367187,24248.5998049,24252.9074218,24246.7181641,24230.4220703,24217.0777344,24217.2291016,24215.9429688,24208.6509764,24190.7964845,24181.1214844,24157.9615234,24149.8476563,24145.6595705,24143.5201172,24141.1275391,24130.2726563,24116.5232421,24096.2046874,24083.7287109,24078.6560547,24086.2251953,24065.0107423,24056.1783201,24047.2349609,24037.5492188,24033.8914065,24035.2498047,24021.224414,24017.210547,24012.5583984,24005.5921875,24000.4718751,24000.7443359,23985.4980469,23972.4767579,23962.4560546,23960.5570313,23961.0908203,23953.3957032,23958.2998047,23944.3507813,23935.3359375,23925.4583983,23923.4375001,23917.6445313,23900.9078126,23909.0490234,23905.2955078,23904.5296874,23897.7769531,23899.5853516,23895.2269529,23900.3820311,23888.3300779,23884.4476561,23879.6601561,23869.6210938,23862.1355469,23867.5302735,23863.6792969,23863.690625,23862.0488281,23853.2257814,23845.4792967,23840.6027344,23840.2517578,23838.172461,23835.3,23829.8367187,23833.896875,23833.7787111,23820.1232421,23803.064258,23806.0626953,23805.3085937,23804.7009765,23801.5406252,23797.0335937,23789.4095704,23784.5958985,23778.4427734,23776.6882814,23765.4541016,23771.6216796,23770.1037109,23769.5554686,23751.6033203,23753.6121094,23755.1623047,23743.3392579,23745.3134766,23739.5384766,23735.5728515,23736.0478517,23735.8394532,23735.3697265,23736.910547,23727.7298828,23720.1093751,23711.1486328,23707.863086,23702.4835937,23694.6123047,23686.5683595,23690.1085938,23685.4066406,23673.0484374,23673.6464844,23673.2101562,23664.9830079,23656.6746094,23645.6472657,23639.9703125,23630.9199218,23625.6730468,23615.7539063,23607.5619141,23602.617578,23599.1806641,23592.440039,23573.0736329,23566.0888671,23563.0173829,23556.2755861,23553.4449218,23547.9005861,23541.4460939,23541.3193359,23529.2269532,23529.9111329,23529.4228516,23529.9636719,23523.230078,23520.4203124,23513.8699219,23514.2714845,23501.6925779,23497.3882813,23502.9820314,23499.7568359,23496.8066406,23497.0052735,23499.5310548,23489.5757813,23487.4136717,23482.0042969,23470.0041016,23469.0472656,23465.4890624,23451.9798829,23448.9769531,23451.2644531,23454.905664,23451.5144531,23450.9544921,23447.2529296,23452.7503907,23446.6927735,23440.2310547,23437.2263671,23433.2595703,23431.4523438,23430.6607423,23417.9826173,23414.655664,23412.3978515,23418.9296876,23405.0449218,23398.4718751,23388.6576173,23387.2970704,23382.625,23372.3638672,23369.5876952,23373.2970703,23369.7189452,23360.6378906,23358.6951173,23360.1580077,23365.4046873,23358.1167969,23350.6056641,23348.5794922,23346.4376953,23338.508789,23325.4861329,23317.7566407,23319.4392577,23316.3167968,23313.2253907,23314.0580077,23314.8341797,23308.1531249,23304.3701172,23302.6263673,23300.804297,23299.0273437,23297.7142577,23293.6753905,23290.3414062,23280.326172,23280.4964844,23278.2613283,23274.1003907,23274.2285158,23265.4955078,23265.5021486,23256.5373047,23254.8236328,23250.5552735,23251.9863282,23241.1513671,23238.1333985,23233.9496093,23230.2726561,23225.868164,23220.0728515,23217.3683594,23215.1515624,23209.9873047,23211.6236328,23208.1117186,23205.6208983,23204.3480468,23205.0479491,23204.3077151,23202.8582033,23199.3907225,23198.5574218,23188.6514648,23189.0120117,23187.6774415,23177.4865235,23172.8982423,23163.3814452,23149.2550782,23142.0972657,23142.6674804,23139.309375,23141.5479494,23140.5266602,23140.8775391,23146.0550782,23143.6263673,23145.7714842,23138.8789063,23137.7640625,23131.036133,23134.0179689,23125.5544922,23123.6648439,23119.4990235,23117.4371094,23116.3812499,23114.3039064,23115.5566406,23115.8345704,23113.2419922,23109.361914,23111.3992188,23108.7099608,23111.1298828,23105.0894531,23097.5974611,23095.3291015,23097.8426758,23093.2277344,23089.1529296,23081.1237305,23078.8586916,23068.1139648,23065.319336,23066.3561523,23064.4781252,23058.8438476,23057.410742,23059.0514649,23057.5591796,23050.1424804,23040.6488282,23043.4926758,23041.2765626,23036.1305665,23030.1189452,23028.0026368,23029.549414,23030.2943359,23024.4177736,23019.4323243,23017.2184571,23018.343457,23015.0086914,23015.9309571,23011.9918947,23007.1652344,23000.7480469,23005.2831054,23003.1746094,23004.190625,22994.7458985,22997.1266601,22997.1788086,22995.7699218,22996.2342774,22993.8798829,22989.3597657,22986.2168945,22988.8651367,22987.1476562,22985.6535156,22986.3881835,22981.582422,22982.2501953,22981.7633788,22978.1436522,22974.0096681,22973.8956054,22976.677051,22975.2389649,22969.1183594,22964.5707032,22956.3697265,22958.3198243,22952.0563477,22944.4926757,22941.7505861,22941.630664,22942.8941404,22935.1384765,22937.1879882,22939.0526367,22939.3529297,22938.1364259,22936.0701171,22932.791211,22931.4724608,22920.8972658,22917.6522462,22917.0500976,22915.0219726,22918.677832,22915.0745118,22912.0682617,22909.8917968,22905.8007813,22906.0123047,22909.431543,22907.2345703,22905.4796875,22900.9657227,22905.1514649,22904.7631835,22902.9511718,22897.2836914,22888.6875,22883.7110351,22882.6388672,22883.7443359,22882.7434571,22875.5225587,22875.7214844,22877.3569336,22874,22872.8892579,22869.8241212,22862.8232422,22858.8686523,22855.3161133,22852.6583008,22848.649414,22846.6371095,22841.1350587,22837.2204099,22833.4198243,22825.8003906,22820.6664062,22820.3189454,22819.3035157,22818.948828,22822.9182617,22829.0964843,22832.4974609,22837.6052735,22838.429492,22834.0535156,22835.9907229,22836.7937498,22833.3897462,22827.8486328,22824.5136718,22822.2462889,22821.4827148,22820.4275391,22820.4118165,22820.2465822,22815.7006835,22814.7435546,22819.6880861,22822.1066407,22819.0324219,22816.3221679,22813.6106447,22811.0074217,22809.3564453,22806.8711914,22805.5937499,22806.8984375,22808.8179688,22811.1588866,22810.2154299,22804.2688475,22803.327539,22803.3470703,22802.8761718,22805.2467773,22791.2766601,22794.2749024,22791.2921876,22793.3148438,22789.6177733,22787.7479493,22781.3663087,22778.233203,22773.1468749,22773.3271485,22771.8699221,22770.2023437,22771.6901366,22771.982129,22769.8160156,22769.2257812,22771.2342773,22772.5569336,22769.6643555,22765.553418,22767.5143555,22760.7123046,22754.7958006,22755.2820313,22757.3211913,22758.108203,22756.7990233,22753.8527344,22750.0107421,22751.5204102,22745.6246095,22743.3977539,22746.5234376,22747.3933593,22745.4003906,22740.2789063,22742.4662108,22740.1291014,22738.3751953,22735.872754,22733.6878906,22726.1668946,22720.1424805,22719.9845702,22716.2571289,22709.6606445,22710.5097657,22708.7507812,22715.0963867,22718.2338866,22719.8399414,22715.8907227,22715.3873048,22712.6091797,22716.244043,22714.6819335,22714.7798828,22714.7599609,22713.043457,22715.6865236,22712.8369141,22711.4791991,22708.3271485,22707.9854493,22713.4887696,22713.1313477,22711.8119141,22705.6839844,22698.3105469,22700.0343748,22695.8820312,22692.0293944,22689.140332,22690.936621,22687.698535,22686.4785156,22684.8097655,22685.5305665,22681.8181641,22682.8919922,22682.461035,22683.4988281,22679.8577148,22679.3348634,22676.7480468,22676.9714843,22675.3612306,22675.3452147,22678.7064453,22681.3819334,22680.3640624,22677.9939451,22675.6629884,22676.2176759,22677.3913084,22678.6185547,22680.8011718,22678.3282228,22681.1209962,22682.7509765,22679.9773437,22678.1109375,22679.8320312,22678.849707,22677.4211914,22677.0062501,22675.6606446,22672.6135743,22664.2431642,22659.549707,22661.106836,22662.4177734,22660.7477538,22659.2352539,22660.562793,22658.1857422,22654.1566405,22652.8461913,22652.4703125,22649.4145507,22652.6683593,22650.2977539,22649.7126952,22649.5583985,22649.0395509,22645.9351562,22645.7708008,22645.4862306,22638.4257812,22637.4015625,22637.0870117,22637.783203,22639.7701172,22641.3607422,22634.322168,22631.3851563,22634.8549806,22640.9889649,22640.3070312,22639.4027346,22635.4504883,22640.3510743,22638.0609376,22636.4152344,22631.5151368,22632.7156248,22634.8092773,22632.6069336,22631.2945314,22628.4698241,22626.7801759,22626.5271483,22627.1327148,22624.7855468,22621.6416991,22621.3685547,22620.3649414,22620.0729493,22620.1316406,22622.7501952,22621.55166,22618.9750001,22617.6860352,22617.9164062,22617.4584959,22612.8351562,22613.0992187,22612.57666,22611.5865236,22607.050586,22611.1946289,22606.3638671,22605.8141602,22605.6842773,22603.325,22598.0914063,22596.2068359,22596.6448241,22594.594629,22592.6268555,22589.8812499,22593.7794921,22593.4807617,22597.900586,22596.8789062,22593.9188477,22595.7545899,22596.5804687,22596.240332,22590.7749023,22591.5804688,22591.4150391,22591.0230468,22586.1297852,22584.5362305,22584.5763672,22583.1430663,22581.0142579,22578.1311523,22570.7259766,22570.0351563,22568.2239258,22569.1437499,22569.3271483,22567.8542968,22569.0440429,22566.6368164,22562.9923828,22561.6892579,22561.2887696,22558.9808593,22557.3920899,22556.1729493,22555.5196289,22553.6674805,22550.6548828,22551.068457,22551.8021484,22549.0799806,22547.2880859,22547.1642578,22546.8461912,22549.2015624,22543.1916993,22542.2635743,22538.4699219,22536.8655272,22534.0958007,22528.331543,22523.7944334,22520.5920899,22519.6288087,22516.281836,22522.3012696,22523.7368163,22522.0799805,22523.8580078,22521.7196289,22518.3102538,22517.3576172,22515.1377929,22515.5094725,22512.7166993,22514.38125,22516.8566405,22513.8639648,22512.1532228,22513.4694335,22506.1290038,22502.153125,22502.2492187,22503.3088867,22500.8242186,22497.6998046,22498.9693358,22500.0643553,22498.065332,22495.3748048,22495.7292969,22492.115918,22494.2329099,22487.5890625,22489.2614258,22492.6604491,22490.3054688,22487.4975585,22484.7071288,22482.1183594,22482.7088867,22481.1593751,22480.3809571,22474.7789063,22476.5964844,22473.8642579,22476.1461914,22476.5494142,22475.6064453,22477.0947266,22476.1941406,22479.1766601,22480.9894533,22475.4750976,22470.9449218,22463.2370116,22464.7663086,22464.1094726,22466.3007812,22463.9728515,22463.0098632,22460.063867,22461.4115235,22460.8690428,22460.696875,22453.43916,22454.7034179,22451.3250001,22450.36875,22448.9719726,22443.3741212,22446.8952149,22446.0647463,22446.8875976,22445.4183594,22446.9267578,22451.6745118,22450.3842775,22446.649707,22442.8940431,22444.0842771,22440.9251955,22442.1655276,22436.3742186,22436.6121093,22436.1711914,22430.9954101,22431.4204102,22428.8918947,22427.839746,22426.3737306,22428.1775393,22430.5457031,22431.5981445,22426.7237305,22424.6814452,22423.993164,22423.760547,22425.4089844,22426.2472657,22425.7221682,22422.6644532,22424.1564454,22426.2676758,22424.3806639,22419.6414063,22420.7103516,22416.7385744,22419.5466798,22421.4644532,22418.4167968,22421.3339845,22420.8812499,22418.650586,22416.6640626,22416.7906251,22418.7680664,22419.4688477,22413.8997069,22412.6512695,22414.1081054,22412.206836,22410.8919922,22402.3216797,22399.3404296,22396.5977538,22394.0711914,22390.6496094,22389.519629,22393.1424805,22394.1770508,22395.2679688,22396.8281251,22396.1060546,22394.7482423,22392.175586,22390.0358399,22390.9652342,22396.386621,22395.3358398,22391.7497071,22391.5223632,22384.7032227,22382.3540038,22381.0165039,22378.469336,22376.060547,22374.851074,22373.204004,22371.0368165,22371.3492188,22368.5949219,22364.6865234,22364.008203,22360.4597656,22361.5705078,22361.5640625,22362.8605468,22363.7386721,22360.3916015,22363.7101562,22363.0875001,22364.5489257,22364.2167968,22362.0003907,22364.7235353,22365.0689454,22365.2208008,22367.2758789,22368.0905274,22367.3525389,22362.7394533,22361.1987305,22363.4312501,22367.2236328,22368.0190429,22364.4375,22364.6808593,22362.5320312,22360.7708984,22360.4403319,22358.2541015,22356.1782227,22354.3452147,22353.3705078,22351.615332,22350.6426758,22350.7586915,22353.9186525,22351.769043,22352.4699217,22353.3521484,22352.3147459,22350.0143555,22349.9170898,22349.8507813,22347.221875,22351.0990235,22352.2605468,22351.7638671,22353.5332032,22352.130664,22351.1325195,22350.437207,22349.8779298,22346.7672851,22345.2319335,22344.2027343,22343.2207031,22339.6348634,22334.2560547,22335.1387696,22334.3974609,22332.9061524,22330.1581054,22328.4835936,22327.1726563,22322.4306639,22321.1405272,22319.2173828,22321.3141601,22323.1467773,22323.8714844,22324.1048829,22320.2998047,22318.8787108,22317.3230469,22317.186035,22315.5109376,22315.2360352,22316.7323241,22316.9301757,22314.8851563,22312.0973632,22313.10791,22310.925879,22306.7033204,22304.8912109,22303.6776369,22304.5330078,22304.3211914,22306.0267579,22305.0452148,22305.5512696,22305.5729493,22305.7740235,22306.3333984,22299.2041991,22301.1289062,22303.2123047,22303.8132814,22304.7375001,22305.727539,22301.4499023,22299.8773438,22300.7429687,22299.2025389,22298.6219727,22299.8052736,22299.7711913,22298.4796876,22300.5252929,22301.9069336,22300.6490234,22298.4223635,22298.1602539,22296.3614258,22293.6466796,22293.0748046,22292.4129882,22295.4418945,22292.062793,22289.1800781,22286.8887694,22286.3103516,22284.5695311,22282.9737306,22284.1904297,22285.1320313,22284.5995117,22284.6239258,22284.5326172,22285.72334,22282.1388672,22280.1565431,22278.9749024,22279.840039,22281.3167967,22282.2923829,22280.146289,22277.6048828,22274.3535157,22269.7900391,22269.5619141,22269.373633,22269.4735353,22271.1914063,22275.9429687,22275.7776367,22274.4960938,22276.1493164,22273.5870118,22274.0914063,22275.8482422,22275.2685547,22274.5819337,22273.701172,22276.1216798,22277.2365233,22277.1339846,22274.8599608,22273.9406249,22271.8040039,22266.6755859,22265.0742186,22261.6125977,22263.3240234,22264.7283203,22264.5129882,22262.9865234,22264.5225587,22263.1434571,22264.6737305,22264.3955078,22264.5902344,22266.5190431,22268.4881835,22267.3387696,22265.4231445,22262.3739258,22262.0061524,22259.5424803,22259.1571288,22259.3894531,22260.7444337,22263.2677733,22267.8899415,22267.2708984,22263.9987305,22264.6063476,22264.6139648,22265.2081055,22266.9432617,22267.7671875,22264.8702148,22262.5539062,22263.4353518,22264.8079103,22262.8722655,22261.4865234,22260.583203,22260.3424805,22259.4729492,22259.8142578,22256.1628905,22253.419336,22254.9047851,22252.9703126,22252.015039,22253.4722655,22255.5747071,22254.9441406,22252.3916017,22249.7615234,22250.9311523,22250.388867,22249.1863281,22248.9019531,22250.4586914,22250.0291014,22251.0141603,22251.8691406,22247.288379,22248.4288086,22248.6763672,22249.7044922,22249.5726562,22249.5760742,22247.1889648,22247.9128906,22244.7078126,22242.007129,22242.0636719,22236.7360353,22235.7101562,22238.9269531,22237.2628905,22234.7459961,22236.0563476,22238.9063477,22239.6667969,22237.2046876,22239.8362305,22239.3960937,22240.750293,22239.8164062,22239.7957032,22238.4955078,22240.5988279,22238.5924804,22238.3892579,22239.1250976,22240.1601561,22238.4907227,22242.6038087,22243.2826171,22240.3458986,22240.4359375,22242.0353514,22242.9415038,22241.6032225,22240.9473633,22239.4318358,22240.3943359,22235.9513672,22236.8705077,22238.0272462,22237.3945313,22241.8433594,22242.4196288,22242.784961,22244.8032228,22244.7798829,22244.7725583,22243.3797851,22243.1036133,22242.695996,22243.2218749,22244.0237305,22245.1628907,22242.1385743,22243.6868163,22243.2199219,22242.959375,22240.3851564,22238.6379883,22238.904883,22237.4328126],"text":["iter:    1<br />test_rmse_mean: 190839.02","iter:    2<br />test_rmse_mean: 185545.02","iter:    3<br />test_rmse_mean: 180327.73","iter:    4<br />test_rmse_mean: 175341.29","iter:    5<br />test_rmse_mean: 170447.98","iter:    6<br />test_rmse_mean: 165795.73","iter:    7<br />test_rmse_mean: 161247.60","iter:    8<br />test_rmse_mean: 156838.21","iter:    9<br />test_rmse_mean: 152552.90","iter:   10<br />test_rmse_mean: 148376.59","iter:   11<br />test_rmse_mean: 144281.77","iter:   12<br />test_rmse_mean: 140398.51","iter:   13<br />test_rmse_mean: 136638.37","iter:   14<br />test_rmse_mean: 132946.85","iter:   15<br />test_rmse_mean: 129395.06","iter:   16<br />test_rmse_mean: 125978.46","iter:   17<br />test_rmse_mean: 122614.17","iter:   18<br />test_rmse_mean: 119376.89","iter:   19<br />test_rmse_mean: 116209.01","iter:   20<br />test_rmse_mean: 113144.60","iter:   21<br />test_rmse_mean: 110222.77","iter:   22<br />test_rmse_mean: 107399.87","iter:   23<br />test_rmse_mean: 104581.66","iter:   24<br />test_rmse_mean: 101873.33","iter:   25<br />test_rmse_mean:  99281.84","iter:   26<br />test_rmse_mean:  96752.45","iter:   27<br />test_rmse_mean:  94306.72","iter:   28<br />test_rmse_mean:  91975.98","iter:   29<br />test_rmse_mean:  89693.14","iter:   30<br />test_rmse_mean:  87496.76","iter:   31<br />test_rmse_mean:  85374.52","iter:   32<br />test_rmse_mean:  83297.96","iter:   33<br />test_rmse_mean:  81310.05","iter:   34<br />test_rmse_mean:  79397.25","iter:   35<br />test_rmse_mean:  77512.50","iter:   36<br />test_rmse_mean:  75670.25","iter:   37<br />test_rmse_mean:  73893.44","iter:   38<br />test_rmse_mean:  72191.94","iter:   39<br />test_rmse_mean:  70622.72","iter:   40<br />test_rmse_mean:  69053.65","iter:   41<br />test_rmse_mean:  67499.23","iter:   42<br />test_rmse_mean:  66005.95","iter:   43<br />test_rmse_mean:  64570.78","iter:   44<br />test_rmse_mean:  63218.99","iter:   45<br />test_rmse_mean:  61906.54","iter:   46<br />test_rmse_mean:  60573.63","iter:   47<br />test_rmse_mean:  59300.80","iter:   48<br />test_rmse_mean:  58119.41","iter:   49<br />test_rmse_mean:  56963.51","iter:   50<br />test_rmse_mean:  55815.90","iter:   51<br />test_rmse_mean:  54738.27","iter:   52<br />test_rmse_mean:  53686.80","iter:   53<br />test_rmse_mean:  52713.53","iter:   54<br />test_rmse_mean:  51773.94","iter:   55<br />test_rmse_mean:  50848.08","iter:   56<br />test_rmse_mean:  49958.41","iter:   57<br />test_rmse_mean:  49120.34","iter:   58<br />test_rmse_mean:  48294.51","iter:   59<br />test_rmse_mean:  47502.56","iter:   60<br />test_rmse_mean:  46747.82","iter:   61<br />test_rmse_mean:  45988.65","iter:   62<br />test_rmse_mean:  45263.31","iter:   63<br />test_rmse_mean:  44630.30","iter:   64<br />test_rmse_mean:  43914.65","iter:   65<br />test_rmse_mean:  43266.30","iter:   66<br />test_rmse_mean:  42673.14","iter:   67<br />test_rmse_mean:  42133.02","iter:   68<br />test_rmse_mean:  41545.11","iter:   69<br />test_rmse_mean:  41033.93","iter:   70<br />test_rmse_mean:  40544.79","iter:   71<br />test_rmse_mean:  40052.51","iter:   72<br />test_rmse_mean:  39581.33","iter:   73<br />test_rmse_mean:  39143.46","iter:   74<br />test_rmse_mean:  38694.02","iter:   75<br />test_rmse_mean:  38279.79","iter:   76<br />test_rmse_mean:  37859.79","iter:   77<br />test_rmse_mean:  37457.47","iter:   78<br />test_rmse_mean:  37096.94","iter:   79<br />test_rmse_mean:  36720.84","iter:   80<br />test_rmse_mean:  36397.09","iter:   81<br />test_rmse_mean:  36043.35","iter:   82<br />test_rmse_mean:  35739.56","iter:   83<br />test_rmse_mean:  35456.64","iter:   84<br />test_rmse_mean:  35156.52","iter:   85<br />test_rmse_mean:  34891.37","iter:   86<br />test_rmse_mean:  34613.71","iter:   87<br />test_rmse_mean:  34365.93","iter:   88<br />test_rmse_mean:  34092.33","iter:   89<br />test_rmse_mean:  33875.25","iter:   90<br />test_rmse_mean:  33637.58","iter:   91<br />test_rmse_mean:  33439.10","iter:   92<br />test_rmse_mean:  33227.25","iter:   93<br />test_rmse_mean:  33017.13","iter:   94<br />test_rmse_mean:  32826.92","iter:   95<br />test_rmse_mean:  32651.67","iter:   96<br />test_rmse_mean:  32460.46","iter:   97<br />test_rmse_mean:  32292.25","iter:   98<br />test_rmse_mean:  32116.66","iter:   99<br />test_rmse_mean:  31945.40","iter:  100<br />test_rmse_mean:  31784.35","iter:  101<br />test_rmse_mean:  31628.06","iter:  102<br />test_rmse_mean:  31493.54","iter:  103<br />test_rmse_mean:  31340.09","iter:  104<br />test_rmse_mean:  31208.53","iter:  105<br />test_rmse_mean:  31092.87","iter:  106<br />test_rmse_mean:  31012.09","iter:  107<br />test_rmse_mean:  30894.60","iter:  108<br />test_rmse_mean:  30799.31","iter:  109<br />test_rmse_mean:  30710.45","iter:  110<br />test_rmse_mean:  30608.41","iter:  111<br />test_rmse_mean:  30495.26","iter:  112<br />test_rmse_mean:  30393.05","iter:  113<br />test_rmse_mean:  30318.45","iter:  114<br />test_rmse_mean:  30216.92","iter:  115<br />test_rmse_mean:  30135.49","iter:  116<br />test_rmse_mean:  30062.31","iter:  117<br />test_rmse_mean:  29939.33","iter:  118<br />test_rmse_mean:  29856.49","iter:  119<br />test_rmse_mean:  29751.06","iter:  120<br />test_rmse_mean:  29677.95","iter:  121<br />test_rmse_mean:  29607.24","iter:  122<br />test_rmse_mean:  29502.28","iter:  123<br />test_rmse_mean:  29434.12","iter:  124<br />test_rmse_mean:  29361.50","iter:  125<br />test_rmse_mean:  29294.37","iter:  126<br />test_rmse_mean:  29239.15","iter:  127<br />test_rmse_mean:  29194.57","iter:  128<br />test_rmse_mean:  29154.06","iter:  129<br />test_rmse_mean:  29079.44","iter:  130<br />test_rmse_mean:  29007.10","iter:  131<br />test_rmse_mean:  28945.12","iter:  132<br />test_rmse_mean:  28875.12","iter:  133<br />test_rmse_mean:  28825.64","iter:  134<br />test_rmse_mean:  28754.51","iter:  135<br />test_rmse_mean:  28728.85","iter:  136<br />test_rmse_mean:  28715.01","iter:  137<br />test_rmse_mean:  28670.16","iter:  138<br />test_rmse_mean:  28593.05","iter:  139<br />test_rmse_mean:  28581.70","iter:  140<br />test_rmse_mean:  28549.98","iter:  141<br />test_rmse_mean:  28504.51","iter:  142<br />test_rmse_mean:  28453.26","iter:  143<br />test_rmse_mean:  28403.40","iter:  144<br />test_rmse_mean:  28361.09","iter:  145<br />test_rmse_mean:  28304.24","iter:  146<br />test_rmse_mean:  28272.57","iter:  147<br />test_rmse_mean:  28232.79","iter:  148<br />test_rmse_mean:  28189.86","iter:  149<br />test_rmse_mean:  28139.88","iter:  150<br />test_rmse_mean:  28086.47","iter:  151<br />test_rmse_mean:  28054.41","iter:  152<br />test_rmse_mean:  28012.81","iter:  153<br />test_rmse_mean:  27982.18","iter:  154<br />test_rmse_mean:  27967.63","iter:  155<br />test_rmse_mean:  27915.84","iter:  156<br />test_rmse_mean:  27873.32","iter:  157<br />test_rmse_mean:  27804.09","iter:  158<br />test_rmse_mean:  27792.86","iter:  159<br />test_rmse_mean:  27757.01","iter:  160<br />test_rmse_mean:  27713.52","iter:  161<br />test_rmse_mean:  27669.62","iter:  162<br />test_rmse_mean:  27633.02","iter:  163<br />test_rmse_mean:  27603.95","iter:  164<br />test_rmse_mean:  27583.17","iter:  165<br />test_rmse_mean:  27548.98","iter:  166<br />test_rmse_mean:  27529.92","iter:  167<br />test_rmse_mean:  27506.11","iter:  168<br />test_rmse_mean:  27496.90","iter:  169<br />test_rmse_mean:  27492.26","iter:  170<br />test_rmse_mean:  27462.45","iter:  171<br />test_rmse_mean:  27433.63","iter:  172<br />test_rmse_mean:  27406.44","iter:  173<br />test_rmse_mean:  27397.21","iter:  174<br />test_rmse_mean:  27377.81","iter:  175<br />test_rmse_mean:  27364.79","iter:  176<br />test_rmse_mean:  27345.27","iter:  177<br />test_rmse_mean:  27337.07","iter:  178<br />test_rmse_mean:  27307.62","iter:  179<br />test_rmse_mean:  27282.42","iter:  180<br />test_rmse_mean:  27285.18","iter:  181<br />test_rmse_mean:  27264.48","iter:  182<br />test_rmse_mean:  27263.00","iter:  183<br />test_rmse_mean:  27240.28","iter:  184<br />test_rmse_mean:  27203.41","iter:  185<br />test_rmse_mean:  27185.50","iter:  186<br />test_rmse_mean:  27172.90","iter:  187<br />test_rmse_mean:  27172.83","iter:  188<br />test_rmse_mean:  27149.88","iter:  189<br />test_rmse_mean:  27129.27","iter:  190<br />test_rmse_mean:  27094.79","iter:  191<br />test_rmse_mean:  27083.64","iter:  192<br />test_rmse_mean:  27075.58","iter:  193<br />test_rmse_mean:  27059.95","iter:  194<br />test_rmse_mean:  27028.09","iter:  195<br />test_rmse_mean:  27007.01","iter:  196<br />test_rmse_mean:  26975.35","iter:  197<br />test_rmse_mean:  26934.88","iter:  198<br />test_rmse_mean:  26928.69","iter:  199<br />test_rmse_mean:  26921.46","iter:  200<br />test_rmse_mean:  26904.41","iter:  201<br />test_rmse_mean:  26857.54","iter:  202<br />test_rmse_mean:  26836.29","iter:  203<br />test_rmse_mean:  26827.23","iter:  204<br />test_rmse_mean:  26768.44","iter:  205<br />test_rmse_mean:  26750.26","iter:  206<br />test_rmse_mean:  26743.92","iter:  207<br />test_rmse_mean:  26692.94","iter:  208<br />test_rmse_mean:  26693.52","iter:  209<br />test_rmse_mean:  26673.53","iter:  210<br />test_rmse_mean:  26655.43","iter:  211<br />test_rmse_mean:  26627.43","iter:  212<br />test_rmse_mean:  26610.52","iter:  213<br />test_rmse_mean:  26600.09","iter:  214<br />test_rmse_mean:  26574.55","iter:  215<br />test_rmse_mean:  26554.96","iter:  216<br />test_rmse_mean:  26539.25","iter:  217<br />test_rmse_mean:  26528.11","iter:  218<br />test_rmse_mean:  26540.24","iter:  219<br />test_rmse_mean:  26513.14","iter:  220<br />test_rmse_mean:  26499.60","iter:  221<br />test_rmse_mean:  26465.90","iter:  222<br />test_rmse_mean:  26455.61","iter:  223<br />test_rmse_mean:  26454.86","iter:  224<br />test_rmse_mean:  26442.63","iter:  225<br />test_rmse_mean:  26430.65","iter:  226<br />test_rmse_mean:  26409.02","iter:  227<br />test_rmse_mean:  26401.17","iter:  228<br />test_rmse_mean:  26374.81","iter:  229<br />test_rmse_mean:  26372.20","iter:  230<br />test_rmse_mean:  26346.10","iter:  231<br />test_rmse_mean:  26328.83","iter:  232<br />test_rmse_mean:  26325.10","iter:  233<br />test_rmse_mean:  26312.42","iter:  234<br />test_rmse_mean:  26275.11","iter:  235<br />test_rmse_mean:  26247.54","iter:  236<br />test_rmse_mean:  26222.55","iter:  237<br />test_rmse_mean:  26204.49","iter:  238<br />test_rmse_mean:  26180.37","iter:  239<br />test_rmse_mean:  26157.73","iter:  240<br />test_rmse_mean:  26145.17","iter:  241<br />test_rmse_mean:  26125.51","iter:  242<br />test_rmse_mean:  26119.89","iter:  243<br />test_rmse_mean:  26118.44","iter:  244<br />test_rmse_mean:  26111.54","iter:  245<br />test_rmse_mean:  26093.51","iter:  246<br />test_rmse_mean:  26049.42","iter:  247<br />test_rmse_mean:  26037.00","iter:  248<br />test_rmse_mean:  26019.99","iter:  249<br />test_rmse_mean:  25988.62","iter:  250<br />test_rmse_mean:  25974.03","iter:  251<br />test_rmse_mean:  25961.40","iter:  252<br />test_rmse_mean:  25921.43","iter:  253<br />test_rmse_mean:  25912.02","iter:  254<br />test_rmse_mean:  25905.18","iter:  255<br />test_rmse_mean:  25888.84","iter:  256<br />test_rmse_mean:  25873.45","iter:  257<br />test_rmse_mean:  25870.44","iter:  258<br />test_rmse_mean:  25858.57","iter:  259<br />test_rmse_mean:  25857.72","iter:  260<br />test_rmse_mean:  25844.04","iter:  261<br />test_rmse_mean:  25840.04","iter:  262<br />test_rmse_mean:  25812.14","iter:  263<br />test_rmse_mean:  25804.54","iter:  264<br />test_rmse_mean:  25814.99","iter:  265<br />test_rmse_mean:  25810.44","iter:  266<br />test_rmse_mean:  25780.86","iter:  267<br />test_rmse_mean:  25765.66","iter:  268<br />test_rmse_mean:  25753.48","iter:  269<br />test_rmse_mean:  25748.79","iter:  270<br />test_rmse_mean:  25748.38","iter:  271<br />test_rmse_mean:  25724.54","iter:  272<br />test_rmse_mean:  25716.85","iter:  273<br />test_rmse_mean:  25702.08","iter:  274<br />test_rmse_mean:  25703.10","iter:  275<br />test_rmse_mean:  25691.03","iter:  276<br />test_rmse_mean:  25685.31","iter:  277<br />test_rmse_mean:  25669.86","iter:  278<br />test_rmse_mean:  25664.19","iter:  279<br />test_rmse_mean:  25644.90","iter:  280<br />test_rmse_mean:  25636.61","iter:  281<br />test_rmse_mean:  25621.76","iter:  282<br />test_rmse_mean:  25594.78","iter:  283<br />test_rmse_mean:  25586.17","iter:  284<br />test_rmse_mean:  25579.67","iter:  285<br />test_rmse_mean:  25541.90","iter:  286<br />test_rmse_mean:  25534.65","iter:  287<br />test_rmse_mean:  25525.04","iter:  288<br />test_rmse_mean:  25517.00","iter:  289<br />test_rmse_mean:  25498.46","iter:  290<br />test_rmse_mean:  25491.34","iter:  291<br />test_rmse_mean:  25486.13","iter:  292<br />test_rmse_mean:  25466.42","iter:  293<br />test_rmse_mean:  25453.00","iter:  294<br />test_rmse_mean:  25431.54","iter:  295<br />test_rmse_mean:  25428.40","iter:  296<br />test_rmse_mean:  25418.47","iter:  297<br />test_rmse_mean:  25400.78","iter:  298<br />test_rmse_mean:  25370.96","iter:  299<br />test_rmse_mean:  25350.76","iter:  300<br />test_rmse_mean:  25354.97","iter:  301<br />test_rmse_mean:  25357.70","iter:  302<br />test_rmse_mean:  25342.58","iter:  303<br />test_rmse_mean:  25325.05","iter:  304<br />test_rmse_mean:  25322.17","iter:  305<br />test_rmse_mean:  25319.74","iter:  306<br />test_rmse_mean:  25333.86","iter:  307<br />test_rmse_mean:  25330.59","iter:  308<br />test_rmse_mean:  25307.84","iter:  309<br />test_rmse_mean:  25285.62","iter:  310<br />test_rmse_mean:  25281.44","iter:  311<br />test_rmse_mean:  25269.84","iter:  312<br />test_rmse_mean:  25265.30","iter:  313<br />test_rmse_mean:  25267.51","iter:  314<br />test_rmse_mean:  25266.06","iter:  315<br />test_rmse_mean:  25251.73","iter:  316<br />test_rmse_mean:  25239.89","iter:  317<br />test_rmse_mean:  25230.20","iter:  318<br />test_rmse_mean:  25225.30","iter:  319<br />test_rmse_mean:  25228.68","iter:  320<br />test_rmse_mean:  25249.45","iter:  321<br />test_rmse_mean:  25247.73","iter:  322<br />test_rmse_mean:  25234.93","iter:  323<br />test_rmse_mean:  25231.91","iter:  324<br />test_rmse_mean:  25227.74","iter:  325<br />test_rmse_mean:  25211.16","iter:  326<br />test_rmse_mean:  25201.19","iter:  327<br />test_rmse_mean:  25178.98","iter:  328<br />test_rmse_mean:  25160.11","iter:  329<br />test_rmse_mean:  25157.23","iter:  330<br />test_rmse_mean:  25143.49","iter:  331<br />test_rmse_mean:  25138.02","iter:  332<br />test_rmse_mean:  25126.08","iter:  333<br />test_rmse_mean:  25123.89","iter:  334<br />test_rmse_mean:  25113.17","iter:  335<br />test_rmse_mean:  25106.62","iter:  336<br />test_rmse_mean:  25096.03","iter:  337<br />test_rmse_mean:  25091.62","iter:  338<br />test_rmse_mean:  25082.88","iter:  339<br />test_rmse_mean:  25070.81","iter:  340<br />test_rmse_mean:  25066.72","iter:  341<br />test_rmse_mean:  25072.96","iter:  342<br />test_rmse_mean:  25053.48","iter:  343<br />test_rmse_mean:  25049.77","iter:  344<br />test_rmse_mean:  25038.49","iter:  345<br />test_rmse_mean:  25039.38","iter:  346<br />test_rmse_mean:  25035.66","iter:  347<br />test_rmse_mean:  25023.61","iter:  348<br />test_rmse_mean:  25014.63","iter:  349<br />test_rmse_mean:  25011.87","iter:  350<br />test_rmse_mean:  25000.72","iter:  351<br />test_rmse_mean:  24994.97","iter:  352<br />test_rmse_mean:  24982.04","iter:  353<br />test_rmse_mean:  24993.37","iter:  354<br />test_rmse_mean:  24979.77","iter:  355<br />test_rmse_mean:  24980.81","iter:  356<br />test_rmse_mean:  24989.59","iter:  357<br />test_rmse_mean:  24975.48","iter:  358<br />test_rmse_mean:  24973.94","iter:  359<br />test_rmse_mean:  24964.62","iter:  360<br />test_rmse_mean:  24950.28","iter:  361<br />test_rmse_mean:  24943.34","iter:  362<br />test_rmse_mean:  24932.40","iter:  363<br />test_rmse_mean:  24921.01","iter:  364<br />test_rmse_mean:  24911.76","iter:  365<br />test_rmse_mean:  24899.23","iter:  366<br />test_rmse_mean:  24899.36","iter:  367<br />test_rmse_mean:  24887.83","iter:  368<br />test_rmse_mean:  24883.33","iter:  369<br />test_rmse_mean:  24882.64","iter:  370<br />test_rmse_mean:  24870.32","iter:  371<br />test_rmse_mean:  24872.11","iter:  372<br />test_rmse_mean:  24866.57","iter:  373<br />test_rmse_mean:  24854.21","iter:  374<br />test_rmse_mean:  24845.70","iter:  375<br />test_rmse_mean:  24832.27","iter:  376<br />test_rmse_mean:  24825.76","iter:  377<br />test_rmse_mean:  24818.27","iter:  378<br />test_rmse_mean:  24813.69","iter:  379<br />test_rmse_mean:  24799.24","iter:  380<br />test_rmse_mean:  24775.00","iter:  381<br />test_rmse_mean:  24773.42","iter:  382<br />test_rmse_mean:  24770.23","iter:  383<br />test_rmse_mean:  24760.10","iter:  384<br />test_rmse_mean:  24747.81","iter:  385<br />test_rmse_mean:  24736.01","iter:  386<br />test_rmse_mean:  24735.36","iter:  387<br />test_rmse_mean:  24738.47","iter:  388<br />test_rmse_mean:  24737.61","iter:  389<br />test_rmse_mean:  24730.69","iter:  390<br />test_rmse_mean:  24724.33","iter:  391<br />test_rmse_mean:  24725.18","iter:  392<br />test_rmse_mean:  24719.71","iter:  393<br />test_rmse_mean:  24714.63","iter:  394<br />test_rmse_mean:  24704.68","iter:  395<br />test_rmse_mean:  24699.76","iter:  396<br />test_rmse_mean:  24687.20","iter:  397<br />test_rmse_mean:  24672.04","iter:  398<br />test_rmse_mean:  24662.12","iter:  399<br />test_rmse_mean:  24639.64","iter:  400<br />test_rmse_mean:  24639.03","iter:  401<br />test_rmse_mean:  24627.27","iter:  402<br />test_rmse_mean:  24625.08","iter:  403<br />test_rmse_mean:  24621.37","iter:  404<br />test_rmse_mean:  24610.85","iter:  405<br />test_rmse_mean:  24605.18","iter:  406<br />test_rmse_mean:  24595.95","iter:  407<br />test_rmse_mean:  24577.99","iter:  408<br />test_rmse_mean:  24576.97","iter:  409<br />test_rmse_mean:  24567.50","iter:  410<br />test_rmse_mean:  24566.89","iter:  411<br />test_rmse_mean:  24569.07","iter:  412<br />test_rmse_mean:  24564.00","iter:  413<br />test_rmse_mean:  24559.48","iter:  414<br />test_rmse_mean:  24554.28","iter:  415<br />test_rmse_mean:  24555.03","iter:  416<br />test_rmse_mean:  24553.66","iter:  417<br />test_rmse_mean:  24557.60","iter:  418<br />test_rmse_mean:  24560.51","iter:  419<br />test_rmse_mean:  24557.04","iter:  420<br />test_rmse_mean:  24551.57","iter:  421<br />test_rmse_mean:  24545.92","iter:  422<br />test_rmse_mean:  24534.69","iter:  423<br />test_rmse_mean:  24531.53","iter:  424<br />test_rmse_mean:  24521.79","iter:  425<br />test_rmse_mean:  24517.66","iter:  426<br />test_rmse_mean:  24508.92","iter:  427<br />test_rmse_mean:  24504.33","iter:  428<br />test_rmse_mean:  24502.11","iter:  429<br />test_rmse_mean:  24494.67","iter:  430<br />test_rmse_mean:  24490.03","iter:  431<br />test_rmse_mean:  24466.37","iter:  432<br />test_rmse_mean:  24457.77","iter:  433<br />test_rmse_mean:  24454.53","iter:  434<br />test_rmse_mean:  24457.36","iter:  435<br />test_rmse_mean:  24460.88","iter:  436<br />test_rmse_mean:  24454.34","iter:  437<br />test_rmse_mean:  24453.36","iter:  438<br />test_rmse_mean:  24450.97","iter:  439<br />test_rmse_mean:  24436.45","iter:  440<br />test_rmse_mean:  24434.63","iter:  441<br />test_rmse_mean:  24427.84","iter:  442<br />test_rmse_mean:  24418.90","iter:  443<br />test_rmse_mean:  24415.08","iter:  444<br />test_rmse_mean:  24403.14","iter:  445<br />test_rmse_mean:  24405.22","iter:  446<br />test_rmse_mean:  24396.89","iter:  447<br />test_rmse_mean:  24386.07","iter:  448<br />test_rmse_mean:  24373.21","iter:  449<br />test_rmse_mean:  24362.53","iter:  450<br />test_rmse_mean:  24352.25","iter:  451<br />test_rmse_mean:  24338.05","iter:  452<br />test_rmse_mean:  24331.23","iter:  453<br />test_rmse_mean:  24325.87","iter:  454<br />test_rmse_mean:  24324.10","iter:  455<br />test_rmse_mean:  24313.85","iter:  456<br />test_rmse_mean:  24298.76","iter:  457<br />test_rmse_mean:  24299.00","iter:  458<br />test_rmse_mean:  24282.98","iter:  459<br />test_rmse_mean:  24271.75","iter:  460<br />test_rmse_mean:  24270.30","iter:  461<br />test_rmse_mean:  24259.72","iter:  462<br />test_rmse_mean:  24257.54","iter:  463<br />test_rmse_mean:  24248.60","iter:  464<br />test_rmse_mean:  24252.91","iter:  465<br />test_rmse_mean:  24246.72","iter:  466<br />test_rmse_mean:  24230.42","iter:  467<br />test_rmse_mean:  24217.08","iter:  468<br />test_rmse_mean:  24217.23","iter:  469<br />test_rmse_mean:  24215.94","iter:  470<br />test_rmse_mean:  24208.65","iter:  471<br />test_rmse_mean:  24190.80","iter:  472<br />test_rmse_mean:  24181.12","iter:  473<br />test_rmse_mean:  24157.96","iter:  474<br />test_rmse_mean:  24149.85","iter:  475<br />test_rmse_mean:  24145.66","iter:  476<br />test_rmse_mean:  24143.52","iter:  477<br />test_rmse_mean:  24141.13","iter:  478<br />test_rmse_mean:  24130.27","iter:  479<br />test_rmse_mean:  24116.52","iter:  480<br />test_rmse_mean:  24096.20","iter:  481<br />test_rmse_mean:  24083.73","iter:  482<br />test_rmse_mean:  24078.66","iter:  483<br />test_rmse_mean:  24086.23","iter:  484<br />test_rmse_mean:  24065.01","iter:  485<br />test_rmse_mean:  24056.18","iter:  486<br />test_rmse_mean:  24047.23","iter:  487<br />test_rmse_mean:  24037.55","iter:  488<br />test_rmse_mean:  24033.89","iter:  489<br />test_rmse_mean:  24035.25","iter:  490<br />test_rmse_mean:  24021.22","iter:  491<br />test_rmse_mean:  24017.21","iter:  492<br />test_rmse_mean:  24012.56","iter:  493<br />test_rmse_mean:  24005.59","iter:  494<br />test_rmse_mean:  24000.47","iter:  495<br />test_rmse_mean:  24000.74","iter:  496<br />test_rmse_mean:  23985.50","iter:  497<br />test_rmse_mean:  23972.48","iter:  498<br />test_rmse_mean:  23962.46","iter:  499<br />test_rmse_mean:  23960.56","iter:  500<br />test_rmse_mean:  23961.09","iter:  501<br />test_rmse_mean:  23953.40","iter:  502<br />test_rmse_mean:  23958.30","iter:  503<br />test_rmse_mean:  23944.35","iter:  504<br />test_rmse_mean:  23935.34","iter:  505<br />test_rmse_mean:  23925.46","iter:  506<br />test_rmse_mean:  23923.44","iter:  507<br />test_rmse_mean:  23917.64","iter:  508<br />test_rmse_mean:  23900.91","iter:  509<br />test_rmse_mean:  23909.05","iter:  510<br />test_rmse_mean:  23905.30","iter:  511<br />test_rmse_mean:  23904.53","iter:  512<br />test_rmse_mean:  23897.78","iter:  513<br />test_rmse_mean:  23899.59","iter:  514<br />test_rmse_mean:  23895.23","iter:  515<br />test_rmse_mean:  23900.38","iter:  516<br />test_rmse_mean:  23888.33","iter:  517<br />test_rmse_mean:  23884.45","iter:  518<br />test_rmse_mean:  23879.66","iter:  519<br />test_rmse_mean:  23869.62","iter:  520<br />test_rmse_mean:  23862.14","iter:  521<br />test_rmse_mean:  23867.53","iter:  522<br />test_rmse_mean:  23863.68","iter:  523<br />test_rmse_mean:  23863.69","iter:  524<br />test_rmse_mean:  23862.05","iter:  525<br />test_rmse_mean:  23853.23","iter:  526<br />test_rmse_mean:  23845.48","iter:  527<br />test_rmse_mean:  23840.60","iter:  528<br />test_rmse_mean:  23840.25","iter:  529<br />test_rmse_mean:  23838.17","iter:  530<br />test_rmse_mean:  23835.30","iter:  531<br />test_rmse_mean:  23829.84","iter:  532<br />test_rmse_mean:  23833.90","iter:  533<br />test_rmse_mean:  23833.78","iter:  534<br />test_rmse_mean:  23820.12","iter:  535<br />test_rmse_mean:  23803.06","iter:  536<br />test_rmse_mean:  23806.06","iter:  537<br />test_rmse_mean:  23805.31","iter:  538<br />test_rmse_mean:  23804.70","iter:  539<br />test_rmse_mean:  23801.54","iter:  540<br />test_rmse_mean:  23797.03","iter:  541<br />test_rmse_mean:  23789.41","iter:  542<br />test_rmse_mean:  23784.60","iter:  543<br />test_rmse_mean:  23778.44","iter:  544<br />test_rmse_mean:  23776.69","iter:  545<br />test_rmse_mean:  23765.45","iter:  546<br />test_rmse_mean:  23771.62","iter:  547<br />test_rmse_mean:  23770.10","iter:  548<br />test_rmse_mean:  23769.56","iter:  549<br />test_rmse_mean:  23751.60","iter:  550<br />test_rmse_mean:  23753.61","iter:  551<br />test_rmse_mean:  23755.16","iter:  552<br />test_rmse_mean:  23743.34","iter:  553<br />test_rmse_mean:  23745.31","iter:  554<br />test_rmse_mean:  23739.54","iter:  555<br />test_rmse_mean:  23735.57","iter:  556<br />test_rmse_mean:  23736.05","iter:  557<br />test_rmse_mean:  23735.84","iter:  558<br />test_rmse_mean:  23735.37","iter:  559<br />test_rmse_mean:  23736.91","iter:  560<br />test_rmse_mean:  23727.73","iter:  561<br />test_rmse_mean:  23720.11","iter:  562<br />test_rmse_mean:  23711.15","iter:  563<br />test_rmse_mean:  23707.86","iter:  564<br />test_rmse_mean:  23702.48","iter:  565<br />test_rmse_mean:  23694.61","iter:  566<br />test_rmse_mean:  23686.57","iter:  567<br />test_rmse_mean:  23690.11","iter:  568<br />test_rmse_mean:  23685.41","iter:  569<br />test_rmse_mean:  23673.05","iter:  570<br />test_rmse_mean:  23673.65","iter:  571<br />test_rmse_mean:  23673.21","iter:  572<br />test_rmse_mean:  23664.98","iter:  573<br />test_rmse_mean:  23656.67","iter:  574<br />test_rmse_mean:  23645.65","iter:  575<br />test_rmse_mean:  23639.97","iter:  576<br />test_rmse_mean:  23630.92","iter:  577<br />test_rmse_mean:  23625.67","iter:  578<br />test_rmse_mean:  23615.75","iter:  579<br />test_rmse_mean:  23607.56","iter:  580<br />test_rmse_mean:  23602.62","iter:  581<br />test_rmse_mean:  23599.18","iter:  582<br />test_rmse_mean:  23592.44","iter:  583<br />test_rmse_mean:  23573.07","iter:  584<br />test_rmse_mean:  23566.09","iter:  585<br />test_rmse_mean:  23563.02","iter:  586<br />test_rmse_mean:  23556.28","iter:  587<br />test_rmse_mean:  23553.44","iter:  588<br />test_rmse_mean:  23547.90","iter:  589<br />test_rmse_mean:  23541.45","iter:  590<br />test_rmse_mean:  23541.32","iter:  591<br />test_rmse_mean:  23529.23","iter:  592<br />test_rmse_mean:  23529.91","iter:  593<br />test_rmse_mean:  23529.42","iter:  594<br />test_rmse_mean:  23529.96","iter:  595<br />test_rmse_mean:  23523.23","iter:  596<br />test_rmse_mean:  23520.42","iter:  597<br />test_rmse_mean:  23513.87","iter:  598<br />test_rmse_mean:  23514.27","iter:  599<br />test_rmse_mean:  23501.69","iter:  600<br />test_rmse_mean:  23497.39","iter:  601<br />test_rmse_mean:  23502.98","iter:  602<br />test_rmse_mean:  23499.76","iter:  603<br />test_rmse_mean:  23496.81","iter:  604<br />test_rmse_mean:  23497.01","iter:  605<br />test_rmse_mean:  23499.53","iter:  606<br />test_rmse_mean:  23489.58","iter:  607<br />test_rmse_mean:  23487.41","iter:  608<br />test_rmse_mean:  23482.00","iter:  609<br />test_rmse_mean:  23470.00","iter:  610<br />test_rmse_mean:  23469.05","iter:  611<br />test_rmse_mean:  23465.49","iter:  612<br />test_rmse_mean:  23451.98","iter:  613<br />test_rmse_mean:  23448.98","iter:  614<br />test_rmse_mean:  23451.26","iter:  615<br />test_rmse_mean:  23454.91","iter:  616<br />test_rmse_mean:  23451.51","iter:  617<br />test_rmse_mean:  23450.95","iter:  618<br />test_rmse_mean:  23447.25","iter:  619<br />test_rmse_mean:  23452.75","iter:  620<br />test_rmse_mean:  23446.69","iter:  621<br />test_rmse_mean:  23440.23","iter:  622<br />test_rmse_mean:  23437.23","iter:  623<br />test_rmse_mean:  23433.26","iter:  624<br />test_rmse_mean:  23431.45","iter:  625<br />test_rmse_mean:  23430.66","iter:  626<br />test_rmse_mean:  23417.98","iter:  627<br />test_rmse_mean:  23414.66","iter:  628<br />test_rmse_mean:  23412.40","iter:  629<br />test_rmse_mean:  23418.93","iter:  630<br />test_rmse_mean:  23405.04","iter:  631<br />test_rmse_mean:  23398.47","iter:  632<br />test_rmse_mean:  23388.66","iter:  633<br />test_rmse_mean:  23387.30","iter:  634<br />test_rmse_mean:  23382.62","iter:  635<br />test_rmse_mean:  23372.36","iter:  636<br />test_rmse_mean:  23369.59","iter:  637<br />test_rmse_mean:  23373.30","iter:  638<br />test_rmse_mean:  23369.72","iter:  639<br />test_rmse_mean:  23360.64","iter:  640<br />test_rmse_mean:  23358.70","iter:  641<br />test_rmse_mean:  23360.16","iter:  642<br />test_rmse_mean:  23365.40","iter:  643<br />test_rmse_mean:  23358.12","iter:  644<br />test_rmse_mean:  23350.61","iter:  645<br />test_rmse_mean:  23348.58","iter:  646<br />test_rmse_mean:  23346.44","iter:  647<br />test_rmse_mean:  23338.51","iter:  648<br />test_rmse_mean:  23325.49","iter:  649<br />test_rmse_mean:  23317.76","iter:  650<br />test_rmse_mean:  23319.44","iter:  651<br />test_rmse_mean:  23316.32","iter:  652<br />test_rmse_mean:  23313.23","iter:  653<br />test_rmse_mean:  23314.06","iter:  654<br />test_rmse_mean:  23314.83","iter:  655<br />test_rmse_mean:  23308.15","iter:  656<br />test_rmse_mean:  23304.37","iter:  657<br />test_rmse_mean:  23302.63","iter:  658<br />test_rmse_mean:  23300.80","iter:  659<br />test_rmse_mean:  23299.03","iter:  660<br />test_rmse_mean:  23297.71","iter:  661<br />test_rmse_mean:  23293.68","iter:  662<br />test_rmse_mean:  23290.34","iter:  663<br />test_rmse_mean:  23280.33","iter:  664<br />test_rmse_mean:  23280.50","iter:  665<br />test_rmse_mean:  23278.26","iter:  666<br />test_rmse_mean:  23274.10","iter:  667<br />test_rmse_mean:  23274.23","iter:  668<br />test_rmse_mean:  23265.50","iter:  669<br />test_rmse_mean:  23265.50","iter:  670<br />test_rmse_mean:  23256.54","iter:  671<br />test_rmse_mean:  23254.82","iter:  672<br />test_rmse_mean:  23250.56","iter:  673<br />test_rmse_mean:  23251.99","iter:  674<br />test_rmse_mean:  23241.15","iter:  675<br />test_rmse_mean:  23238.13","iter:  676<br />test_rmse_mean:  23233.95","iter:  677<br />test_rmse_mean:  23230.27","iter:  678<br />test_rmse_mean:  23225.87","iter:  679<br />test_rmse_mean:  23220.07","iter:  680<br />test_rmse_mean:  23217.37","iter:  681<br />test_rmse_mean:  23215.15","iter:  682<br />test_rmse_mean:  23209.99","iter:  683<br />test_rmse_mean:  23211.62","iter:  684<br />test_rmse_mean:  23208.11","iter:  685<br />test_rmse_mean:  23205.62","iter:  686<br />test_rmse_mean:  23204.35","iter:  687<br />test_rmse_mean:  23205.05","iter:  688<br />test_rmse_mean:  23204.31","iter:  689<br />test_rmse_mean:  23202.86","iter:  690<br />test_rmse_mean:  23199.39","iter:  691<br />test_rmse_mean:  23198.56","iter:  692<br />test_rmse_mean:  23188.65","iter:  693<br />test_rmse_mean:  23189.01","iter:  694<br />test_rmse_mean:  23187.68","iter:  695<br />test_rmse_mean:  23177.49","iter:  696<br />test_rmse_mean:  23172.90","iter:  697<br />test_rmse_mean:  23163.38","iter:  698<br />test_rmse_mean:  23149.26","iter:  699<br />test_rmse_mean:  23142.10","iter:  700<br />test_rmse_mean:  23142.67","iter:  701<br />test_rmse_mean:  23139.31","iter:  702<br />test_rmse_mean:  23141.55","iter:  703<br />test_rmse_mean:  23140.53","iter:  704<br />test_rmse_mean:  23140.88","iter:  705<br />test_rmse_mean:  23146.06","iter:  706<br />test_rmse_mean:  23143.63","iter:  707<br />test_rmse_mean:  23145.77","iter:  708<br />test_rmse_mean:  23138.88","iter:  709<br />test_rmse_mean:  23137.76","iter:  710<br />test_rmse_mean:  23131.04","iter:  711<br />test_rmse_mean:  23134.02","iter:  712<br />test_rmse_mean:  23125.55","iter:  713<br />test_rmse_mean:  23123.66","iter:  714<br />test_rmse_mean:  23119.50","iter:  715<br />test_rmse_mean:  23117.44","iter:  716<br />test_rmse_mean:  23116.38","iter:  717<br />test_rmse_mean:  23114.30","iter:  718<br />test_rmse_mean:  23115.56","iter:  719<br />test_rmse_mean:  23115.83","iter:  720<br />test_rmse_mean:  23113.24","iter:  721<br />test_rmse_mean:  23109.36","iter:  722<br />test_rmse_mean:  23111.40","iter:  723<br />test_rmse_mean:  23108.71","iter:  724<br />test_rmse_mean:  23111.13","iter:  725<br />test_rmse_mean:  23105.09","iter:  726<br />test_rmse_mean:  23097.60","iter:  727<br />test_rmse_mean:  23095.33","iter:  728<br />test_rmse_mean:  23097.84","iter:  729<br />test_rmse_mean:  23093.23","iter:  730<br />test_rmse_mean:  23089.15","iter:  731<br />test_rmse_mean:  23081.12","iter:  732<br />test_rmse_mean:  23078.86","iter:  733<br />test_rmse_mean:  23068.11","iter:  734<br />test_rmse_mean:  23065.32","iter:  735<br />test_rmse_mean:  23066.36","iter:  736<br />test_rmse_mean:  23064.48","iter:  737<br />test_rmse_mean:  23058.84","iter:  738<br />test_rmse_mean:  23057.41","iter:  739<br />test_rmse_mean:  23059.05","iter:  740<br />test_rmse_mean:  23057.56","iter:  741<br />test_rmse_mean:  23050.14","iter:  742<br />test_rmse_mean:  23040.65","iter:  743<br />test_rmse_mean:  23043.49","iter:  744<br />test_rmse_mean:  23041.28","iter:  745<br />test_rmse_mean:  23036.13","iter:  746<br />test_rmse_mean:  23030.12","iter:  747<br />test_rmse_mean:  23028.00","iter:  748<br />test_rmse_mean:  23029.55","iter:  749<br />test_rmse_mean:  23030.29","iter:  750<br />test_rmse_mean:  23024.42","iter:  751<br />test_rmse_mean:  23019.43","iter:  752<br />test_rmse_mean:  23017.22","iter:  753<br />test_rmse_mean:  23018.34","iter:  754<br />test_rmse_mean:  23015.01","iter:  755<br />test_rmse_mean:  23015.93","iter:  756<br />test_rmse_mean:  23011.99","iter:  757<br />test_rmse_mean:  23007.17","iter:  758<br />test_rmse_mean:  23000.75","iter:  759<br />test_rmse_mean:  23005.28","iter:  760<br />test_rmse_mean:  23003.17","iter:  761<br />test_rmse_mean:  23004.19","iter:  762<br />test_rmse_mean:  22994.75","iter:  763<br />test_rmse_mean:  22997.13","iter:  764<br />test_rmse_mean:  22997.18","iter:  765<br />test_rmse_mean:  22995.77","iter:  766<br />test_rmse_mean:  22996.23","iter:  767<br />test_rmse_mean:  22993.88","iter:  768<br />test_rmse_mean:  22989.36","iter:  769<br />test_rmse_mean:  22986.22","iter:  770<br />test_rmse_mean:  22988.87","iter:  771<br />test_rmse_mean:  22987.15","iter:  772<br />test_rmse_mean:  22985.65","iter:  773<br />test_rmse_mean:  22986.39","iter:  774<br />test_rmse_mean:  22981.58","iter:  775<br />test_rmse_mean:  22982.25","iter:  776<br />test_rmse_mean:  22981.76","iter:  777<br />test_rmse_mean:  22978.14","iter:  778<br />test_rmse_mean:  22974.01","iter:  779<br />test_rmse_mean:  22973.90","iter:  780<br />test_rmse_mean:  22976.68","iter:  781<br />test_rmse_mean:  22975.24","iter:  782<br />test_rmse_mean:  22969.12","iter:  783<br />test_rmse_mean:  22964.57","iter:  784<br />test_rmse_mean:  22956.37","iter:  785<br />test_rmse_mean:  22958.32","iter:  786<br />test_rmse_mean:  22952.06","iter:  787<br />test_rmse_mean:  22944.49","iter:  788<br />test_rmse_mean:  22941.75","iter:  789<br />test_rmse_mean:  22941.63","iter:  790<br />test_rmse_mean:  22942.89","iter:  791<br />test_rmse_mean:  22935.14","iter:  792<br />test_rmse_mean:  22937.19","iter:  793<br />test_rmse_mean:  22939.05","iter:  794<br />test_rmse_mean:  22939.35","iter:  795<br />test_rmse_mean:  22938.14","iter:  796<br />test_rmse_mean:  22936.07","iter:  797<br />test_rmse_mean:  22932.79","iter:  798<br />test_rmse_mean:  22931.47","iter:  799<br />test_rmse_mean:  22920.90","iter:  800<br />test_rmse_mean:  22917.65","iter:  801<br />test_rmse_mean:  22917.05","iter:  802<br />test_rmse_mean:  22915.02","iter:  803<br />test_rmse_mean:  22918.68","iter:  804<br />test_rmse_mean:  22915.07","iter:  805<br />test_rmse_mean:  22912.07","iter:  806<br />test_rmse_mean:  22909.89","iter:  807<br />test_rmse_mean:  22905.80","iter:  808<br />test_rmse_mean:  22906.01","iter:  809<br />test_rmse_mean:  22909.43","iter:  810<br />test_rmse_mean:  22907.23","iter:  811<br />test_rmse_mean:  22905.48","iter:  812<br />test_rmse_mean:  22900.97","iter:  813<br />test_rmse_mean:  22905.15","iter:  814<br />test_rmse_mean:  22904.76","iter:  815<br />test_rmse_mean:  22902.95","iter:  816<br />test_rmse_mean:  22897.28","iter:  817<br />test_rmse_mean:  22888.69","iter:  818<br />test_rmse_mean:  22883.71","iter:  819<br />test_rmse_mean:  22882.64","iter:  820<br />test_rmse_mean:  22883.74","iter:  821<br />test_rmse_mean:  22882.74","iter:  822<br />test_rmse_mean:  22875.52","iter:  823<br />test_rmse_mean:  22875.72","iter:  824<br />test_rmse_mean:  22877.36","iter:  825<br />test_rmse_mean:  22874.00","iter:  826<br />test_rmse_mean:  22872.89","iter:  827<br />test_rmse_mean:  22869.82","iter:  828<br />test_rmse_mean:  22862.82","iter:  829<br />test_rmse_mean:  22858.87","iter:  830<br />test_rmse_mean:  22855.32","iter:  831<br />test_rmse_mean:  22852.66","iter:  832<br />test_rmse_mean:  22848.65","iter:  833<br />test_rmse_mean:  22846.64","iter:  834<br />test_rmse_mean:  22841.14","iter:  835<br />test_rmse_mean:  22837.22","iter:  836<br />test_rmse_mean:  22833.42","iter:  837<br />test_rmse_mean:  22825.80","iter:  838<br />test_rmse_mean:  22820.67","iter:  839<br />test_rmse_mean:  22820.32","iter:  840<br />test_rmse_mean:  22819.30","iter:  841<br />test_rmse_mean:  22818.95","iter:  842<br />test_rmse_mean:  22822.92","iter:  843<br />test_rmse_mean:  22829.10","iter:  844<br />test_rmse_mean:  22832.50","iter:  845<br />test_rmse_mean:  22837.61","iter:  846<br />test_rmse_mean:  22838.43","iter:  847<br />test_rmse_mean:  22834.05","iter:  848<br />test_rmse_mean:  22835.99","iter:  849<br />test_rmse_mean:  22836.79","iter:  850<br />test_rmse_mean:  22833.39","iter:  851<br />test_rmse_mean:  22827.85","iter:  852<br />test_rmse_mean:  22824.51","iter:  853<br />test_rmse_mean:  22822.25","iter:  854<br />test_rmse_mean:  22821.48","iter:  855<br />test_rmse_mean:  22820.43","iter:  856<br />test_rmse_mean:  22820.41","iter:  857<br />test_rmse_mean:  22820.25","iter:  858<br />test_rmse_mean:  22815.70","iter:  859<br />test_rmse_mean:  22814.74","iter:  860<br />test_rmse_mean:  22819.69","iter:  861<br />test_rmse_mean:  22822.11","iter:  862<br />test_rmse_mean:  22819.03","iter:  863<br />test_rmse_mean:  22816.32","iter:  864<br />test_rmse_mean:  22813.61","iter:  865<br />test_rmse_mean:  22811.01","iter:  866<br />test_rmse_mean:  22809.36","iter:  867<br />test_rmse_mean:  22806.87","iter:  868<br />test_rmse_mean:  22805.59","iter:  869<br />test_rmse_mean:  22806.90","iter:  870<br />test_rmse_mean:  22808.82","iter:  871<br />test_rmse_mean:  22811.16","iter:  872<br />test_rmse_mean:  22810.22","iter:  873<br />test_rmse_mean:  22804.27","iter:  874<br />test_rmse_mean:  22803.33","iter:  875<br />test_rmse_mean:  22803.35","iter:  876<br />test_rmse_mean:  22802.88","iter:  877<br />test_rmse_mean:  22805.25","iter:  878<br />test_rmse_mean:  22791.28","iter:  879<br />test_rmse_mean:  22794.27","iter:  880<br />test_rmse_mean:  22791.29","iter:  881<br />test_rmse_mean:  22793.31","iter:  882<br />test_rmse_mean:  22789.62","iter:  883<br />test_rmse_mean:  22787.75","iter:  884<br />test_rmse_mean:  22781.37","iter:  885<br />test_rmse_mean:  22778.23","iter:  886<br />test_rmse_mean:  22773.15","iter:  887<br />test_rmse_mean:  22773.33","iter:  888<br />test_rmse_mean:  22771.87","iter:  889<br />test_rmse_mean:  22770.20","iter:  890<br />test_rmse_mean:  22771.69","iter:  891<br />test_rmse_mean:  22771.98","iter:  892<br />test_rmse_mean:  22769.82","iter:  893<br />test_rmse_mean:  22769.23","iter:  894<br />test_rmse_mean:  22771.23","iter:  895<br />test_rmse_mean:  22772.56","iter:  896<br />test_rmse_mean:  22769.66","iter:  897<br />test_rmse_mean:  22765.55","iter:  898<br />test_rmse_mean:  22767.51","iter:  899<br />test_rmse_mean:  22760.71","iter:  900<br />test_rmse_mean:  22754.80","iter:  901<br />test_rmse_mean:  22755.28","iter:  902<br />test_rmse_mean:  22757.32","iter:  903<br />test_rmse_mean:  22758.11","iter:  904<br />test_rmse_mean:  22756.80","iter:  905<br />test_rmse_mean:  22753.85","iter:  906<br />test_rmse_mean:  22750.01","iter:  907<br />test_rmse_mean:  22751.52","iter:  908<br />test_rmse_mean:  22745.62","iter:  909<br />test_rmse_mean:  22743.40","iter:  910<br />test_rmse_mean:  22746.52","iter:  911<br />test_rmse_mean:  22747.39","iter:  912<br />test_rmse_mean:  22745.40","iter:  913<br />test_rmse_mean:  22740.28","iter:  914<br />test_rmse_mean:  22742.47","iter:  915<br />test_rmse_mean:  22740.13","iter:  916<br />test_rmse_mean:  22738.38","iter:  917<br />test_rmse_mean:  22735.87","iter:  918<br />test_rmse_mean:  22733.69","iter:  919<br />test_rmse_mean:  22726.17","iter:  920<br />test_rmse_mean:  22720.14","iter:  921<br />test_rmse_mean:  22719.98","iter:  922<br />test_rmse_mean:  22716.26","iter:  923<br />test_rmse_mean:  22709.66","iter:  924<br />test_rmse_mean:  22710.51","iter:  925<br />test_rmse_mean:  22708.75","iter:  926<br />test_rmse_mean:  22715.10","iter:  927<br />test_rmse_mean:  22718.23","iter:  928<br />test_rmse_mean:  22719.84","iter:  929<br />test_rmse_mean:  22715.89","iter:  930<br />test_rmse_mean:  22715.39","iter:  931<br />test_rmse_mean:  22712.61","iter:  932<br />test_rmse_mean:  22716.24","iter:  933<br />test_rmse_mean:  22714.68","iter:  934<br />test_rmse_mean:  22714.78","iter:  935<br />test_rmse_mean:  22714.76","iter:  936<br />test_rmse_mean:  22713.04","iter:  937<br />test_rmse_mean:  22715.69","iter:  938<br />test_rmse_mean:  22712.84","iter:  939<br />test_rmse_mean:  22711.48","iter:  940<br />test_rmse_mean:  22708.33","iter:  941<br />test_rmse_mean:  22707.99","iter:  942<br />test_rmse_mean:  22713.49","iter:  943<br />test_rmse_mean:  22713.13","iter:  944<br />test_rmse_mean:  22711.81","iter:  945<br />test_rmse_mean:  22705.68","iter:  946<br />test_rmse_mean:  22698.31","iter:  947<br />test_rmse_mean:  22700.03","iter:  948<br />test_rmse_mean:  22695.88","iter:  949<br />test_rmse_mean:  22692.03","iter:  950<br />test_rmse_mean:  22689.14","iter:  951<br />test_rmse_mean:  22690.94","iter:  952<br />test_rmse_mean:  22687.70","iter:  953<br />test_rmse_mean:  22686.48","iter:  954<br />test_rmse_mean:  22684.81","iter:  955<br />test_rmse_mean:  22685.53","iter:  956<br />test_rmse_mean:  22681.82","iter:  957<br />test_rmse_mean:  22682.89","iter:  958<br />test_rmse_mean:  22682.46","iter:  959<br />test_rmse_mean:  22683.50","iter:  960<br />test_rmse_mean:  22679.86","iter:  961<br />test_rmse_mean:  22679.33","iter:  962<br />test_rmse_mean:  22676.75","iter:  963<br />test_rmse_mean:  22676.97","iter:  964<br />test_rmse_mean:  22675.36","iter:  965<br />test_rmse_mean:  22675.35","iter:  966<br />test_rmse_mean:  22678.71","iter:  967<br />test_rmse_mean:  22681.38","iter:  968<br />test_rmse_mean:  22680.36","iter:  969<br />test_rmse_mean:  22677.99","iter:  970<br />test_rmse_mean:  22675.66","iter:  971<br />test_rmse_mean:  22676.22","iter:  972<br />test_rmse_mean:  22677.39","iter:  973<br />test_rmse_mean:  22678.62","iter:  974<br />test_rmse_mean:  22680.80","iter:  975<br />test_rmse_mean:  22678.33","iter:  976<br />test_rmse_mean:  22681.12","iter:  977<br />test_rmse_mean:  22682.75","iter:  978<br />test_rmse_mean:  22679.98","iter:  979<br />test_rmse_mean:  22678.11","iter:  980<br />test_rmse_mean:  22679.83","iter:  981<br />test_rmse_mean:  22678.85","iter:  982<br />test_rmse_mean:  22677.42","iter:  983<br />test_rmse_mean:  22677.01","iter:  984<br />test_rmse_mean:  22675.66","iter:  985<br />test_rmse_mean:  22672.61","iter:  986<br />test_rmse_mean:  22664.24","iter:  987<br />test_rmse_mean:  22659.55","iter:  988<br />test_rmse_mean:  22661.11","iter:  989<br />test_rmse_mean:  22662.42","iter:  990<br />test_rmse_mean:  22660.75","iter:  991<br />test_rmse_mean:  22659.24","iter:  992<br />test_rmse_mean:  22660.56","iter:  993<br />test_rmse_mean:  22658.19","iter:  994<br />test_rmse_mean:  22654.16","iter:  995<br />test_rmse_mean:  22652.85","iter:  996<br />test_rmse_mean:  22652.47","iter:  997<br />test_rmse_mean:  22649.41","iter:  998<br />test_rmse_mean:  22652.67","iter:  999<br />test_rmse_mean:  22650.30","iter: 1000<br />test_rmse_mean:  22649.71","iter: 1001<br />test_rmse_mean:  22649.56","iter: 1002<br />test_rmse_mean:  22649.04","iter: 1003<br />test_rmse_mean:  22645.94","iter: 1004<br />test_rmse_mean:  22645.77","iter: 1005<br />test_rmse_mean:  22645.49","iter: 1006<br />test_rmse_mean:  22638.43","iter: 1007<br />test_rmse_mean:  22637.40","iter: 1008<br />test_rmse_mean:  22637.09","iter: 1009<br />test_rmse_mean:  22637.78","iter: 1010<br />test_rmse_mean:  22639.77","iter: 1011<br />test_rmse_mean:  22641.36","iter: 1012<br />test_rmse_mean:  22634.32","iter: 1013<br />test_rmse_mean:  22631.39","iter: 1014<br />test_rmse_mean:  22634.85","iter: 1015<br />test_rmse_mean:  22640.99","iter: 1016<br />test_rmse_mean:  22640.31","iter: 1017<br />test_rmse_mean:  22639.40","iter: 1018<br />test_rmse_mean:  22635.45","iter: 1019<br />test_rmse_mean:  22640.35","iter: 1020<br />test_rmse_mean:  22638.06","iter: 1021<br />test_rmse_mean:  22636.42","iter: 1022<br />test_rmse_mean:  22631.52","iter: 1023<br />test_rmse_mean:  22632.72","iter: 1024<br />test_rmse_mean:  22634.81","iter: 1025<br />test_rmse_mean:  22632.61","iter: 1026<br />test_rmse_mean:  22631.29","iter: 1027<br />test_rmse_mean:  22628.47","iter: 1028<br />test_rmse_mean:  22626.78","iter: 1029<br />test_rmse_mean:  22626.53","iter: 1030<br />test_rmse_mean:  22627.13","iter: 1031<br />test_rmse_mean:  22624.79","iter: 1032<br />test_rmse_mean:  22621.64","iter: 1033<br />test_rmse_mean:  22621.37","iter: 1034<br />test_rmse_mean:  22620.36","iter: 1035<br />test_rmse_mean:  22620.07","iter: 1036<br />test_rmse_mean:  22620.13","iter: 1037<br />test_rmse_mean:  22622.75","iter: 1038<br />test_rmse_mean:  22621.55","iter: 1039<br />test_rmse_mean:  22618.98","iter: 1040<br />test_rmse_mean:  22617.69","iter: 1041<br />test_rmse_mean:  22617.92","iter: 1042<br />test_rmse_mean:  22617.46","iter: 1043<br />test_rmse_mean:  22612.84","iter: 1044<br />test_rmse_mean:  22613.10","iter: 1045<br />test_rmse_mean:  22612.58","iter: 1046<br />test_rmse_mean:  22611.59","iter: 1047<br />test_rmse_mean:  22607.05","iter: 1048<br />test_rmse_mean:  22611.19","iter: 1049<br />test_rmse_mean:  22606.36","iter: 1050<br />test_rmse_mean:  22605.81","iter: 1051<br />test_rmse_mean:  22605.68","iter: 1052<br />test_rmse_mean:  22603.33","iter: 1053<br />test_rmse_mean:  22598.09","iter: 1054<br />test_rmse_mean:  22596.21","iter: 1055<br />test_rmse_mean:  22596.64","iter: 1056<br />test_rmse_mean:  22594.59","iter: 1057<br />test_rmse_mean:  22592.63","iter: 1058<br />test_rmse_mean:  22589.88","iter: 1059<br />test_rmse_mean:  22593.78","iter: 1060<br />test_rmse_mean:  22593.48","iter: 1061<br />test_rmse_mean:  22597.90","iter: 1062<br />test_rmse_mean:  22596.88","iter: 1063<br />test_rmse_mean:  22593.92","iter: 1064<br />test_rmse_mean:  22595.75","iter: 1065<br />test_rmse_mean:  22596.58","iter: 1066<br />test_rmse_mean:  22596.24","iter: 1067<br />test_rmse_mean:  22590.77","iter: 1068<br />test_rmse_mean:  22591.58","iter: 1069<br />test_rmse_mean:  22591.42","iter: 1070<br />test_rmse_mean:  22591.02","iter: 1071<br />test_rmse_mean:  22586.13","iter: 1072<br />test_rmse_mean:  22584.54","iter: 1073<br />test_rmse_mean:  22584.58","iter: 1074<br />test_rmse_mean:  22583.14","iter: 1075<br />test_rmse_mean:  22581.01","iter: 1076<br />test_rmse_mean:  22578.13","iter: 1077<br />test_rmse_mean:  22570.73","iter: 1078<br />test_rmse_mean:  22570.04","iter: 1079<br />test_rmse_mean:  22568.22","iter: 1080<br />test_rmse_mean:  22569.14","iter: 1081<br />test_rmse_mean:  22569.33","iter: 1082<br />test_rmse_mean:  22567.85","iter: 1083<br />test_rmse_mean:  22569.04","iter: 1084<br />test_rmse_mean:  22566.64","iter: 1085<br />test_rmse_mean:  22562.99","iter: 1086<br />test_rmse_mean:  22561.69","iter: 1087<br />test_rmse_mean:  22561.29","iter: 1088<br />test_rmse_mean:  22558.98","iter: 1089<br />test_rmse_mean:  22557.39","iter: 1090<br />test_rmse_mean:  22556.17","iter: 1091<br />test_rmse_mean:  22555.52","iter: 1092<br />test_rmse_mean:  22553.67","iter: 1093<br />test_rmse_mean:  22550.65","iter: 1094<br />test_rmse_mean:  22551.07","iter: 1095<br />test_rmse_mean:  22551.80","iter: 1096<br />test_rmse_mean:  22549.08","iter: 1097<br />test_rmse_mean:  22547.29","iter: 1098<br />test_rmse_mean:  22547.16","iter: 1099<br />test_rmse_mean:  22546.85","iter: 1100<br />test_rmse_mean:  22549.20","iter: 1101<br />test_rmse_mean:  22543.19","iter: 1102<br />test_rmse_mean:  22542.26","iter: 1103<br />test_rmse_mean:  22538.47","iter: 1104<br />test_rmse_mean:  22536.87","iter: 1105<br />test_rmse_mean:  22534.10","iter: 1106<br />test_rmse_mean:  22528.33","iter: 1107<br />test_rmse_mean:  22523.79","iter: 1108<br />test_rmse_mean:  22520.59","iter: 1109<br />test_rmse_mean:  22519.63","iter: 1110<br />test_rmse_mean:  22516.28","iter: 1111<br />test_rmse_mean:  22522.30","iter: 1112<br />test_rmse_mean:  22523.74","iter: 1113<br />test_rmse_mean:  22522.08","iter: 1114<br />test_rmse_mean:  22523.86","iter: 1115<br />test_rmse_mean:  22521.72","iter: 1116<br />test_rmse_mean:  22518.31","iter: 1117<br />test_rmse_mean:  22517.36","iter: 1118<br />test_rmse_mean:  22515.14","iter: 1119<br />test_rmse_mean:  22515.51","iter: 1120<br />test_rmse_mean:  22512.72","iter: 1121<br />test_rmse_mean:  22514.38","iter: 1122<br />test_rmse_mean:  22516.86","iter: 1123<br />test_rmse_mean:  22513.86","iter: 1124<br />test_rmse_mean:  22512.15","iter: 1125<br />test_rmse_mean:  22513.47","iter: 1126<br />test_rmse_mean:  22506.13","iter: 1127<br />test_rmse_mean:  22502.15","iter: 1128<br />test_rmse_mean:  22502.25","iter: 1129<br />test_rmse_mean:  22503.31","iter: 1130<br />test_rmse_mean:  22500.82","iter: 1131<br />test_rmse_mean:  22497.70","iter: 1132<br />test_rmse_mean:  22498.97","iter: 1133<br />test_rmse_mean:  22500.06","iter: 1134<br />test_rmse_mean:  22498.07","iter: 1135<br />test_rmse_mean:  22495.37","iter: 1136<br />test_rmse_mean:  22495.73","iter: 1137<br />test_rmse_mean:  22492.12","iter: 1138<br />test_rmse_mean:  22494.23","iter: 1139<br />test_rmse_mean:  22487.59","iter: 1140<br />test_rmse_mean:  22489.26","iter: 1141<br />test_rmse_mean:  22492.66","iter: 1142<br />test_rmse_mean:  22490.31","iter: 1143<br />test_rmse_mean:  22487.50","iter: 1144<br />test_rmse_mean:  22484.71","iter: 1145<br />test_rmse_mean:  22482.12","iter: 1146<br />test_rmse_mean:  22482.71","iter: 1147<br />test_rmse_mean:  22481.16","iter: 1148<br />test_rmse_mean:  22480.38","iter: 1149<br />test_rmse_mean:  22474.78","iter: 1150<br />test_rmse_mean:  22476.60","iter: 1151<br />test_rmse_mean:  22473.86","iter: 1152<br />test_rmse_mean:  22476.15","iter: 1153<br />test_rmse_mean:  22476.55","iter: 1154<br />test_rmse_mean:  22475.61","iter: 1155<br />test_rmse_mean:  22477.09","iter: 1156<br />test_rmse_mean:  22476.19","iter: 1157<br />test_rmse_mean:  22479.18","iter: 1158<br />test_rmse_mean:  22480.99","iter: 1159<br />test_rmse_mean:  22475.48","iter: 1160<br />test_rmse_mean:  22470.94","iter: 1161<br />test_rmse_mean:  22463.24","iter: 1162<br />test_rmse_mean:  22464.77","iter: 1163<br />test_rmse_mean:  22464.11","iter: 1164<br />test_rmse_mean:  22466.30","iter: 1165<br />test_rmse_mean:  22463.97","iter: 1166<br />test_rmse_mean:  22463.01","iter: 1167<br />test_rmse_mean:  22460.06","iter: 1168<br />test_rmse_mean:  22461.41","iter: 1169<br />test_rmse_mean:  22460.87","iter: 1170<br />test_rmse_mean:  22460.70","iter: 1171<br />test_rmse_mean:  22453.44","iter: 1172<br />test_rmse_mean:  22454.70","iter: 1173<br />test_rmse_mean:  22451.33","iter: 1174<br />test_rmse_mean:  22450.37","iter: 1175<br />test_rmse_mean:  22448.97","iter: 1176<br />test_rmse_mean:  22443.37","iter: 1177<br />test_rmse_mean:  22446.90","iter: 1178<br />test_rmse_mean:  22446.06","iter: 1179<br />test_rmse_mean:  22446.89","iter: 1180<br />test_rmse_mean:  22445.42","iter: 1181<br />test_rmse_mean:  22446.93","iter: 1182<br />test_rmse_mean:  22451.67","iter: 1183<br />test_rmse_mean:  22450.38","iter: 1184<br />test_rmse_mean:  22446.65","iter: 1185<br />test_rmse_mean:  22442.89","iter: 1186<br />test_rmse_mean:  22444.08","iter: 1187<br />test_rmse_mean:  22440.93","iter: 1188<br />test_rmse_mean:  22442.17","iter: 1189<br />test_rmse_mean:  22436.37","iter: 1190<br />test_rmse_mean:  22436.61","iter: 1191<br />test_rmse_mean:  22436.17","iter: 1192<br />test_rmse_mean:  22431.00","iter: 1193<br />test_rmse_mean:  22431.42","iter: 1194<br />test_rmse_mean:  22428.89","iter: 1195<br />test_rmse_mean:  22427.84","iter: 1196<br />test_rmse_mean:  22426.37","iter: 1197<br />test_rmse_mean:  22428.18","iter: 1198<br />test_rmse_mean:  22430.55","iter: 1199<br />test_rmse_mean:  22431.60","iter: 1200<br />test_rmse_mean:  22426.72","iter: 1201<br />test_rmse_mean:  22424.68","iter: 1202<br />test_rmse_mean:  22423.99","iter: 1203<br />test_rmse_mean:  22423.76","iter: 1204<br />test_rmse_mean:  22425.41","iter: 1205<br />test_rmse_mean:  22426.25","iter: 1206<br />test_rmse_mean:  22425.72","iter: 1207<br />test_rmse_mean:  22422.66","iter: 1208<br />test_rmse_mean:  22424.16","iter: 1209<br />test_rmse_mean:  22426.27","iter: 1210<br />test_rmse_mean:  22424.38","iter: 1211<br />test_rmse_mean:  22419.64","iter: 1212<br />test_rmse_mean:  22420.71","iter: 1213<br />test_rmse_mean:  22416.74","iter: 1214<br />test_rmse_mean:  22419.55","iter: 1215<br />test_rmse_mean:  22421.46","iter: 1216<br />test_rmse_mean:  22418.42","iter: 1217<br />test_rmse_mean:  22421.33","iter: 1218<br />test_rmse_mean:  22420.88","iter: 1219<br />test_rmse_mean:  22418.65","iter: 1220<br />test_rmse_mean:  22416.66","iter: 1221<br />test_rmse_mean:  22416.79","iter: 1222<br />test_rmse_mean:  22418.77","iter: 1223<br />test_rmse_mean:  22419.47","iter: 1224<br />test_rmse_mean:  22413.90","iter: 1225<br />test_rmse_mean:  22412.65","iter: 1226<br />test_rmse_mean:  22414.11","iter: 1227<br />test_rmse_mean:  22412.21","iter: 1228<br />test_rmse_mean:  22410.89","iter: 1229<br />test_rmse_mean:  22402.32","iter: 1230<br />test_rmse_mean:  22399.34","iter: 1231<br />test_rmse_mean:  22396.60","iter: 1232<br />test_rmse_mean:  22394.07","iter: 1233<br />test_rmse_mean:  22390.65","iter: 1234<br />test_rmse_mean:  22389.52","iter: 1235<br />test_rmse_mean:  22393.14","iter: 1236<br />test_rmse_mean:  22394.18","iter: 1237<br />test_rmse_mean:  22395.27","iter: 1238<br />test_rmse_mean:  22396.83","iter: 1239<br />test_rmse_mean:  22396.11","iter: 1240<br />test_rmse_mean:  22394.75","iter: 1241<br />test_rmse_mean:  22392.18","iter: 1242<br />test_rmse_mean:  22390.04","iter: 1243<br />test_rmse_mean:  22390.97","iter: 1244<br />test_rmse_mean:  22396.39","iter: 1245<br />test_rmse_mean:  22395.34","iter: 1246<br />test_rmse_mean:  22391.75","iter: 1247<br />test_rmse_mean:  22391.52","iter: 1248<br />test_rmse_mean:  22384.70","iter: 1249<br />test_rmse_mean:  22382.35","iter: 1250<br />test_rmse_mean:  22381.02","iter: 1251<br />test_rmse_mean:  22378.47","iter: 1252<br />test_rmse_mean:  22376.06","iter: 1253<br />test_rmse_mean:  22374.85","iter: 1254<br />test_rmse_mean:  22373.20","iter: 1255<br />test_rmse_mean:  22371.04","iter: 1256<br />test_rmse_mean:  22371.35","iter: 1257<br />test_rmse_mean:  22368.59","iter: 1258<br />test_rmse_mean:  22364.69","iter: 1259<br />test_rmse_mean:  22364.01","iter: 1260<br />test_rmse_mean:  22360.46","iter: 1261<br />test_rmse_mean:  22361.57","iter: 1262<br />test_rmse_mean:  22361.56","iter: 1263<br />test_rmse_mean:  22362.86","iter: 1264<br />test_rmse_mean:  22363.74","iter: 1265<br />test_rmse_mean:  22360.39","iter: 1266<br />test_rmse_mean:  22363.71","iter: 1267<br />test_rmse_mean:  22363.09","iter: 1268<br />test_rmse_mean:  22364.55","iter: 1269<br />test_rmse_mean:  22364.22","iter: 1270<br />test_rmse_mean:  22362.00","iter: 1271<br />test_rmse_mean:  22364.72","iter: 1272<br />test_rmse_mean:  22365.07","iter: 1273<br />test_rmse_mean:  22365.22","iter: 1274<br />test_rmse_mean:  22367.28","iter: 1275<br />test_rmse_mean:  22368.09","iter: 1276<br />test_rmse_mean:  22367.35","iter: 1277<br />test_rmse_mean:  22362.74","iter: 1278<br />test_rmse_mean:  22361.20","iter: 1279<br />test_rmse_mean:  22363.43","iter: 1280<br />test_rmse_mean:  22367.22","iter: 1281<br />test_rmse_mean:  22368.02","iter: 1282<br />test_rmse_mean:  22364.44","iter: 1283<br />test_rmse_mean:  22364.68","iter: 1284<br />test_rmse_mean:  22362.53","iter: 1285<br />test_rmse_mean:  22360.77","iter: 1286<br />test_rmse_mean:  22360.44","iter: 1287<br />test_rmse_mean:  22358.25","iter: 1288<br />test_rmse_mean:  22356.18","iter: 1289<br />test_rmse_mean:  22354.35","iter: 1290<br />test_rmse_mean:  22353.37","iter: 1291<br />test_rmse_mean:  22351.62","iter: 1292<br />test_rmse_mean:  22350.64","iter: 1293<br />test_rmse_mean:  22350.76","iter: 1294<br />test_rmse_mean:  22353.92","iter: 1295<br />test_rmse_mean:  22351.77","iter: 1296<br />test_rmse_mean:  22352.47","iter: 1297<br />test_rmse_mean:  22353.35","iter: 1298<br />test_rmse_mean:  22352.31","iter: 1299<br />test_rmse_mean:  22350.01","iter: 1300<br />test_rmse_mean:  22349.92","iter: 1301<br />test_rmse_mean:  22349.85","iter: 1302<br />test_rmse_mean:  22347.22","iter: 1303<br />test_rmse_mean:  22351.10","iter: 1304<br />test_rmse_mean:  22352.26","iter: 1305<br />test_rmse_mean:  22351.76","iter: 1306<br />test_rmse_mean:  22353.53","iter: 1307<br />test_rmse_mean:  22352.13","iter: 1308<br />test_rmse_mean:  22351.13","iter: 1309<br />test_rmse_mean:  22350.44","iter: 1310<br />test_rmse_mean:  22349.88","iter: 1311<br />test_rmse_mean:  22346.77","iter: 1312<br />test_rmse_mean:  22345.23","iter: 1313<br />test_rmse_mean:  22344.20","iter: 1314<br />test_rmse_mean:  22343.22","iter: 1315<br />test_rmse_mean:  22339.63","iter: 1316<br />test_rmse_mean:  22334.26","iter: 1317<br />test_rmse_mean:  22335.14","iter: 1318<br />test_rmse_mean:  22334.40","iter: 1319<br />test_rmse_mean:  22332.91","iter: 1320<br />test_rmse_mean:  22330.16","iter: 1321<br />test_rmse_mean:  22328.48","iter: 1322<br />test_rmse_mean:  22327.17","iter: 1323<br />test_rmse_mean:  22322.43","iter: 1324<br />test_rmse_mean:  22321.14","iter: 1325<br />test_rmse_mean:  22319.22","iter: 1326<br />test_rmse_mean:  22321.31","iter: 1327<br />test_rmse_mean:  22323.15","iter: 1328<br />test_rmse_mean:  22323.87","iter: 1329<br />test_rmse_mean:  22324.10","iter: 1330<br />test_rmse_mean:  22320.30","iter: 1331<br />test_rmse_mean:  22318.88","iter: 1332<br />test_rmse_mean:  22317.32","iter: 1333<br />test_rmse_mean:  22317.19","iter: 1334<br />test_rmse_mean:  22315.51","iter: 1335<br />test_rmse_mean:  22315.24","iter: 1336<br />test_rmse_mean:  22316.73","iter: 1337<br />test_rmse_mean:  22316.93","iter: 1338<br />test_rmse_mean:  22314.89","iter: 1339<br />test_rmse_mean:  22312.10","iter: 1340<br />test_rmse_mean:  22313.11","iter: 1341<br />test_rmse_mean:  22310.93","iter: 1342<br />test_rmse_mean:  22306.70","iter: 1343<br />test_rmse_mean:  22304.89","iter: 1344<br />test_rmse_mean:  22303.68","iter: 1345<br />test_rmse_mean:  22304.53","iter: 1346<br />test_rmse_mean:  22304.32","iter: 1347<br />test_rmse_mean:  22306.03","iter: 1348<br />test_rmse_mean:  22305.05","iter: 1349<br />test_rmse_mean:  22305.55","iter: 1350<br />test_rmse_mean:  22305.57","iter: 1351<br />test_rmse_mean:  22305.77","iter: 1352<br />test_rmse_mean:  22306.33","iter: 1353<br />test_rmse_mean:  22299.20","iter: 1354<br />test_rmse_mean:  22301.13","iter: 1355<br />test_rmse_mean:  22303.21","iter: 1356<br />test_rmse_mean:  22303.81","iter: 1357<br />test_rmse_mean:  22304.74","iter: 1358<br />test_rmse_mean:  22305.73","iter: 1359<br />test_rmse_mean:  22301.45","iter: 1360<br />test_rmse_mean:  22299.88","iter: 1361<br />test_rmse_mean:  22300.74","iter: 1362<br />test_rmse_mean:  22299.20","iter: 1363<br />test_rmse_mean:  22298.62","iter: 1364<br />test_rmse_mean:  22299.81","iter: 1365<br />test_rmse_mean:  22299.77","iter: 1366<br />test_rmse_mean:  22298.48","iter: 1367<br />test_rmse_mean:  22300.53","iter: 1368<br />test_rmse_mean:  22301.91","iter: 1369<br />test_rmse_mean:  22300.65","iter: 1370<br />test_rmse_mean:  22298.42","iter: 1371<br />test_rmse_mean:  22298.16","iter: 1372<br />test_rmse_mean:  22296.36","iter: 1373<br />test_rmse_mean:  22293.65","iter: 1374<br />test_rmse_mean:  22293.07","iter: 1375<br />test_rmse_mean:  22292.41","iter: 1376<br />test_rmse_mean:  22295.44","iter: 1377<br />test_rmse_mean:  22292.06","iter: 1378<br />test_rmse_mean:  22289.18","iter: 1379<br />test_rmse_mean:  22286.89","iter: 1380<br />test_rmse_mean:  22286.31","iter: 1381<br />test_rmse_mean:  22284.57","iter: 1382<br />test_rmse_mean:  22282.97","iter: 1383<br />test_rmse_mean:  22284.19","iter: 1384<br />test_rmse_mean:  22285.13","iter: 1385<br />test_rmse_mean:  22284.60","iter: 1386<br />test_rmse_mean:  22284.62","iter: 1387<br />test_rmse_mean:  22284.53","iter: 1388<br />test_rmse_mean:  22285.72","iter: 1389<br />test_rmse_mean:  22282.14","iter: 1390<br />test_rmse_mean:  22280.16","iter: 1391<br />test_rmse_mean:  22278.97","iter: 1392<br />test_rmse_mean:  22279.84","iter: 1393<br />test_rmse_mean:  22281.32","iter: 1394<br />test_rmse_mean:  22282.29","iter: 1395<br />test_rmse_mean:  22280.15","iter: 1396<br />test_rmse_mean:  22277.60","iter: 1397<br />test_rmse_mean:  22274.35","iter: 1398<br />test_rmse_mean:  22269.79","iter: 1399<br />test_rmse_mean:  22269.56","iter: 1400<br />test_rmse_mean:  22269.37","iter: 1401<br />test_rmse_mean:  22269.47","iter: 1402<br />test_rmse_mean:  22271.19","iter: 1403<br />test_rmse_mean:  22275.94","iter: 1404<br />test_rmse_mean:  22275.78","iter: 1405<br />test_rmse_mean:  22274.50","iter: 1406<br />test_rmse_mean:  22276.15","iter: 1407<br />test_rmse_mean:  22273.59","iter: 1408<br />test_rmse_mean:  22274.09","iter: 1409<br />test_rmse_mean:  22275.85","iter: 1410<br />test_rmse_mean:  22275.27","iter: 1411<br />test_rmse_mean:  22274.58","iter: 1412<br />test_rmse_mean:  22273.70","iter: 1413<br />test_rmse_mean:  22276.12","iter: 1414<br />test_rmse_mean:  22277.24","iter: 1415<br />test_rmse_mean:  22277.13","iter: 1416<br />test_rmse_mean:  22274.86","iter: 1417<br />test_rmse_mean:  22273.94","iter: 1418<br />test_rmse_mean:  22271.80","iter: 1419<br />test_rmse_mean:  22266.68","iter: 1420<br />test_rmse_mean:  22265.07","iter: 1421<br />test_rmse_mean:  22261.61","iter: 1422<br />test_rmse_mean:  22263.32","iter: 1423<br />test_rmse_mean:  22264.73","iter: 1424<br />test_rmse_mean:  22264.51","iter: 1425<br />test_rmse_mean:  22262.99","iter: 1426<br />test_rmse_mean:  22264.52","iter: 1427<br />test_rmse_mean:  22263.14","iter: 1428<br />test_rmse_mean:  22264.67","iter: 1429<br />test_rmse_mean:  22264.40","iter: 1430<br />test_rmse_mean:  22264.59","iter: 1431<br />test_rmse_mean:  22266.52","iter: 1432<br />test_rmse_mean:  22268.49","iter: 1433<br />test_rmse_mean:  22267.34","iter: 1434<br />test_rmse_mean:  22265.42","iter: 1435<br />test_rmse_mean:  22262.37","iter: 1436<br />test_rmse_mean:  22262.01","iter: 1437<br />test_rmse_mean:  22259.54","iter: 1438<br />test_rmse_mean:  22259.16","iter: 1439<br />test_rmse_mean:  22259.39","iter: 1440<br />test_rmse_mean:  22260.74","iter: 1441<br />test_rmse_mean:  22263.27","iter: 1442<br />test_rmse_mean:  22267.89","iter: 1443<br />test_rmse_mean:  22267.27","iter: 1444<br />test_rmse_mean:  22264.00","iter: 1445<br />test_rmse_mean:  22264.61","iter: 1446<br />test_rmse_mean:  22264.61","iter: 1447<br />test_rmse_mean:  22265.21","iter: 1448<br />test_rmse_mean:  22266.94","iter: 1449<br />test_rmse_mean:  22267.77","iter: 1450<br />test_rmse_mean:  22264.87","iter: 1451<br />test_rmse_mean:  22262.55","iter: 1452<br />test_rmse_mean:  22263.44","iter: 1453<br />test_rmse_mean:  22264.81","iter: 1454<br />test_rmse_mean:  22262.87","iter: 1455<br />test_rmse_mean:  22261.49","iter: 1456<br />test_rmse_mean:  22260.58","iter: 1457<br />test_rmse_mean:  22260.34","iter: 1458<br />test_rmse_mean:  22259.47","iter: 1459<br />test_rmse_mean:  22259.81","iter: 1460<br />test_rmse_mean:  22256.16","iter: 1461<br />test_rmse_mean:  22253.42","iter: 1462<br />test_rmse_mean:  22254.90","iter: 1463<br />test_rmse_mean:  22252.97","iter: 1464<br />test_rmse_mean:  22252.02","iter: 1465<br />test_rmse_mean:  22253.47","iter: 1466<br />test_rmse_mean:  22255.57","iter: 1467<br />test_rmse_mean:  22254.94","iter: 1468<br />test_rmse_mean:  22252.39","iter: 1469<br />test_rmse_mean:  22249.76","iter: 1470<br />test_rmse_mean:  22250.93","iter: 1471<br />test_rmse_mean:  22250.39","iter: 1472<br />test_rmse_mean:  22249.19","iter: 1473<br />test_rmse_mean:  22248.90","iter: 1474<br />test_rmse_mean:  22250.46","iter: 1475<br />test_rmse_mean:  22250.03","iter: 1476<br />test_rmse_mean:  22251.01","iter: 1477<br />test_rmse_mean:  22251.87","iter: 1478<br />test_rmse_mean:  22247.29","iter: 1479<br />test_rmse_mean:  22248.43","iter: 1480<br />test_rmse_mean:  22248.68","iter: 1481<br />test_rmse_mean:  22249.70","iter: 1482<br />test_rmse_mean:  22249.57","iter: 1483<br />test_rmse_mean:  22249.58","iter: 1484<br />test_rmse_mean:  22247.19","iter: 1485<br />test_rmse_mean:  22247.91","iter: 1486<br />test_rmse_mean:  22244.71","iter: 1487<br />test_rmse_mean:  22242.01","iter: 1488<br />test_rmse_mean:  22242.06","iter: 1489<br />test_rmse_mean:  22236.74","iter: 1490<br />test_rmse_mean:  22235.71","iter: 1491<br />test_rmse_mean:  22238.93","iter: 1492<br />test_rmse_mean:  22237.26","iter: 1493<br />test_rmse_mean:  22234.75","iter: 1494<br />test_rmse_mean:  22236.06","iter: 1495<br />test_rmse_mean:  22238.91","iter: 1496<br />test_rmse_mean:  22239.67","iter: 1497<br />test_rmse_mean:  22237.20","iter: 1498<br />test_rmse_mean:  22239.84","iter: 1499<br />test_rmse_mean:  22239.40","iter: 1500<br />test_rmse_mean:  22240.75","iter: 1501<br />test_rmse_mean:  22239.82","iter: 1502<br />test_rmse_mean:  22239.80","iter: 1503<br />test_rmse_mean:  22238.50","iter: 1504<br />test_rmse_mean:  22240.60","iter: 1505<br />test_rmse_mean:  22238.59","iter: 1506<br />test_rmse_mean:  22238.39","iter: 1507<br />test_rmse_mean:  22239.13","iter: 1508<br />test_rmse_mean:  22240.16","iter: 1509<br />test_rmse_mean:  22238.49","iter: 1510<br />test_rmse_mean:  22242.60","iter: 1511<br />test_rmse_mean:  22243.28","iter: 1512<br />test_rmse_mean:  22240.35","iter: 1513<br />test_rmse_mean:  22240.44","iter: 1514<br />test_rmse_mean:  22242.04","iter: 1515<br />test_rmse_mean:  22242.94","iter: 1516<br />test_rmse_mean:  22241.60","iter: 1517<br />test_rmse_mean:  22240.95","iter: 1518<br />test_rmse_mean:  22239.43","iter: 1519<br />test_rmse_mean:  22240.39","iter: 1520<br />test_rmse_mean:  22235.95","iter: 1521<br />test_rmse_mean:  22236.87","iter: 1522<br />test_rmse_mean:  22238.03","iter: 1523<br />test_rmse_mean:  22237.39","iter: 1524<br />test_rmse_mean:  22241.84","iter: 1525<br />test_rmse_mean:  22242.42","iter: 1526<br />test_rmse_mean:  22242.78","iter: 1527<br />test_rmse_mean:  22244.80","iter: 1528<br />test_rmse_mean:  22244.78","iter: 1529<br />test_rmse_mean:  22244.77","iter: 1530<br />test_rmse_mean:  22243.38","iter: 1531<br />test_rmse_mean:  22243.10","iter: 1532<br />test_rmse_mean:  22242.70","iter: 1533<br />test_rmse_mean:  22243.22","iter: 1534<br />test_rmse_mean:  22244.02","iter: 1535<br />test_rmse_mean:  22245.16","iter: 1536<br />test_rmse_mean:  22242.14","iter: 1537<br />test_rmse_mean:  22243.69","iter: 1538<br />test_rmse_mean:  22243.22","iter: 1539<br />test_rmse_mean:  22242.96","iter: 1540<br />test_rmse_mean:  22240.39","iter: 1541<br />test_rmse_mean:  22238.64","iter: 1542<br />test_rmse_mean:  22238.90","iter: 1543<br />test_rmse_mean:  22237.43"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-76.1,1620.1],"tickmode":"array","ticktext":["0","500","1000","1500"],"tickvals":[0,500,1000,1500],"categoryorder":"array","categoryarray":["0","500","1000","1500"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"iter","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-544.341323170001,200429.99884277],"tickmode":"array","ticktext":["0","50000","100000","150000","200000"],"tickvals":[0,50000,100000,150000,200000],"categoryorder":"array","categoryarray":["0","50000","100000","150000","200000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"train_rmse_mean","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"174834241e65":{"x":{},"y":{},"type":"scatter"},"174825c42c03":{"x":{},"y":{}}},"cur_data":"174834241e65","visdat":{"174834241e65":["function (y) ","x"],"174825c42c03":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>We can see that the training error increased. However, the error on unseen data reaches a minimum RMSE of with iterations. With simple adaptation of our parameters we managed to decrease it to some extent. At this point it should be clear that it would take increadible effort to manualy compute those errors for each possible combination of parameters that would potentially decrease the error further. Luckily, there are more elegant, automated solution for it. We can create our hyperparameter search grid along with columns to dump our results in.</p>
</div>
<div id="strategy-for-tuning" class="section level3">
<h3><span class="header-section-number">10.5.4</span> [STRATEGY FOR TUNING]</h3>
<div class="sourceCode" id="cb929"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb929-1" title="1"><span class="co"># Hyperparameter grid</span></a>
<a class="sourceLine" id="cb929-2" title="2">hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</a>
<a class="sourceLine" id="cb929-3" title="3">  <span class="dt">eta =</span> <span class="kw">c</span>(.<span class="dv">01</span>,<span class="fl">0.3</span>),</a>
<a class="sourceLine" id="cb929-4" title="4">  <span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>), </a>
<a class="sourceLine" id="cb929-5" title="5">  <span class="dt">min_child_weight =</span> <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb929-6" title="6">  <span class="dt">subsample =</span> <span class="fl">0.5</span>, </a>
<a class="sourceLine" id="cb929-7" title="7">  <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>,</a>
<a class="sourceLine" id="cb929-8" title="8">  <span class="dt">gamma =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>),</a>
<a class="sourceLine" id="cb929-9" title="9">  <span class="dt">lambda =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e-2</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>),</a>
<a class="sourceLine" id="cb929-10" title="10">  <span class="dt">alpha =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e-2</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>),</a>
<a class="sourceLine" id="cb929-11" title="11">  <span class="dt">rmse =</span> <span class="dv">0</span>,          <span class="co"># a place to dump RMSE results</span></a>
<a class="sourceLine" id="cb929-12" title="12">  <span class="dt">trees =</span> <span class="dv">0</span>          <span class="co"># a place to dump required number of trees</span></a>
<a class="sourceLine" id="cb929-13" title="13">)</a>
<a class="sourceLine" id="cb929-14" title="14"></a>
<a class="sourceLine" id="cb929-15" title="15"><span class="kw">nrow</span>(hyper_grid)</a></code></pre></div>
<pre><code>## [1] 1960</code></pre>
<p>Besides those parameters we discussed, xgboost provides additional hyperparameters <code>alpha</code>, <code>gamma</code> and <code>lambda</code> that can help to constrain model complexity and reduce overfitting. We introduced them in the grid as well. With the code above we create a pretty large search grid consisting of 1960 different hyperparameter combinations to model. It is important to note that running such a grid in a loop procedure could take a couple of hours. We will create such a loop procedure to loop through and apply a xgboost model for each hyperparameter combination (1960 in our case) and finally provide us the results in the hyper_grid data frame.</p>
<div class="sourceCode" id="cb931"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb931-1" title="1"><span class="co"># Grid search</span></a>
<a class="sourceLine" id="cb931-2" title="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(<span class="kw">nrow</span>(hyper_grid))) {</a>
<a class="sourceLine" id="cb931-3" title="3">    <span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb931-4" title="4">    m &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(<span class="dt">data =</span> ames_x_train, <span class="dt">label =</span> ames_y_train, </a>
<a class="sourceLine" id="cb931-5" title="5">        <span class="dt">nrounds =</span> <span class="dv">4000</span>, <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>, <span class="dt">early_stopping_rounds =</span> <span class="dv">50</span>, </a>
<a class="sourceLine" id="cb931-6" title="6">        <span class="dt">nfold =</span> <span class="dv">10</span>, <span class="dt">verbose =</span> <span class="dv">0</span>, <span class="dt">params =</span> <span class="kw">list</span>(<span class="dt">eta =</span> hyper_grid<span class="op">$</span>eta[i], </a>
<a class="sourceLine" id="cb931-7" title="7">            <span class="dt">max_depth =</span> hyper_grid<span class="op">$</span>max_depth[i], <span class="dt">min_child_weight =</span> hyper_grid<span class="op">$</span>min_child_weight[i], </a>
<a class="sourceLine" id="cb931-8" title="8">            <span class="dt">subsample =</span> hyper_grid<span class="op">$</span>subsample[i], <span class="dt">colsample_bytree =</span> hyper_grid<span class="op">$</span>colsample_bytree[i], </a>
<a class="sourceLine" id="cb931-9" title="9">            <span class="dt">gamma =</span> hyper_grid<span class="op">$</span>gamma[i], <span class="dt">lambda =</span> hyper_grid<span class="op">$</span>lambda[i], </a>
<a class="sourceLine" id="cb931-10" title="10">            <span class="dt">alpha =</span> hyper_grid<span class="op">$</span>alpha[i]))</a>
<a class="sourceLine" id="cb931-11" title="11">    hyper_grid<span class="op">$</span>rmse[i] &lt;-<span class="st"> </span><span class="kw">min</span>(m<span class="op">$</span>evaluation_log<span class="op">$</span>test_rmse_mean)</a>
<a class="sourceLine" id="cb931-12" title="12">    hyper_grid<span class="op">$</span>trees[i] &lt;-<span class="st"> </span>m<span class="op">$</span>best_iteration</a>
<a class="sourceLine" id="cb931-13" title="13">}</a>
<a class="sourceLine" id="cb931-14" title="14"></a>
<a class="sourceLine" id="cb931-15" title="15"><span class="co"># Results</span></a>
<a class="sourceLine" id="cb931-16" title="16">hg &lt;-<span class="st"> </span>hyper_grid <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(rmse <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(rmse) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb931-17" title="17"><span class="st">    </span><span class="kw">glimpse</span>()</a></code></pre></div>
<p>Here is a glimpse of results we obtained after several hours of processing:</p>
<div class="sourceCode" id="cb932"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb932-1" title="1">hyper_grid_optimal_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;hyper_grid_optimal_3.csv&quot;</span>)</a>
<a class="sourceLine" id="cb932-2" title="2">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">head</span>(hyper_grid_optimal_<span class="dv">3</span>), <span class="dt">caption =</span> <span class="st">&quot;Hyper grid output&quot;</span>)</a></code></pre></div>
<table>
<caption>
<span id="tab:unnamed-chunk-574">Table 10.3: </span>Hyper grid output
</caption>
<thead>
<tr>
<th style="text-align:right;">
X
</th>
<th style="text-align:right;">
eta
</th>
<th style="text-align:right;">
max_depth
</th>
<th style="text-align:right;">
min_child_weight
</th>
<th style="text-align:right;">
subsample
</th>
<th style="text-align:right;">
colsample_bytree
</th>
<th style="text-align:right;">
gamma
</th>
<th style="text-align:right;">
lambda
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
rmse
</th>
<th style="text-align:right;">
trees
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21553.35
</td>
<td style="text-align:right;">
3497
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21553.35
</td>
<td style="text-align:right;">
3497
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21553.35
</td>
<td style="text-align:right;">
3497
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
100
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21553.35
</td>
<td style="text-align:right;">
3497
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21553.53
</td>
<td style="text-align:right;">
3458
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21584.69
</td>
<td style="text-align:right;">
3292
</td>
</tr>
</tbody>
</table>
<p>In the first row we see the combination of parameters given that results in the lowest estimated error (RMSE) possible for the combination given. Subsequently, we will use those parameters to enhance prediction perfromance of our model.</p>
<div class="sourceCode" id="cb933"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb933-1" title="1"><span class="co"># The list of optimal hyperparameters</span></a>
<a class="sourceLine" id="cb933-2" title="2">params_optimal &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">eta =</span> <span class="fl">0.01</span>, <span class="dt">max_depth =</span> <span class="dv">1</span>, <span class="dt">min_child_weight =</span> <span class="dv">3</span>, </a>
<a class="sourceLine" id="cb933-3" title="3">    <span class="dt">subsample =</span> <span class="fl">0.5</span>, <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>, <span class="dt">lambda =</span> <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb933-4" title="4"></a>
<a class="sourceLine" id="cb933-5" title="5"><span class="co"># Train final model with optimal combination of the</span></a>
<a class="sourceLine" id="cb933-6" title="6"><span class="co"># given parameters</span></a>
<a class="sourceLine" id="cb933-7" title="7"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb933-8" title="8">xgb.fit.optimal &lt;-<span class="st"> </span><span class="kw">xgboost</span>(<span class="dt">params =</span> params_optimal, </a>
<a class="sourceLine" id="cb933-9" title="9">    <span class="dt">data =</span> ames_x_train, <span class="dt">label =</span> ames_y_train, <span class="dt">nrounds =</span> <span class="dv">3497</span>, </a>
<a class="sourceLine" id="cb933-10" title="10">    <span class="dt">nthreads =</span> <span class="dv">1</span>, <span class="dt">early_stopping_rounds =</span> <span class="dv">50</span>, <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>, </a>
<a class="sourceLine" id="cb933-11" title="11">    <span class="dt">verbose =</span> <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [12:39:16] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [12:39:16] WARNING: amalgamation/../src/learner.cc:480: 
## Parameters: { nthreads } might not be used.
## 
##   This may not be accurate due to some parameters are only used in language bindings but
##   passed down to XGBoost core.  Or some parameters are not used but slip through this
##   verification. Please open an issue if you find above cases.</code></pre>
<p>After computing the final model, we can make inferences about how features (i.e. variables in our data set besides sale price) are influencing our model. Measurement of feature importance occurs based on the sum of the reduction in the loss function (e.g. SSE) attributed to each variable at each split in a respective tree. In simplier terms, it is the relative contribution of the respective feature to the model computed by taking each feature’s contribution for each tree in the model. Therefore, those features with the highest average decrease in SSE (for regression) are identified as the one with the highest contribution. Thus, these features are among most important ones. To visualize feature importance plot we need to create imporance matrix first then plot it with ggplot-besed function “xgb.ggplot.importance”.</p>
<div class="sourceCode" id="cb935"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb935-1" title="1"><span class="co"># https://liuyanguu.github.io/post/2018/07/09/extreme-gradient-boosting-xgboost-better-than-random-forest-or-gradient-boosting/</span></a>
<a class="sourceLine" id="cb935-2" title="2"><span class="co"># Construct importance matrix</span></a>
<a class="sourceLine" id="cb935-3" title="3">importance_matrix &lt;-<span class="st"> </span><span class="kw">xgb.importance</span>(<span class="dt">model =</span> xgb.fit.optimal)</a>
<a class="sourceLine" id="cb935-4" title="4"><span class="co"># Variable importance plot with ggplot2</span></a>
<a class="sourceLine" id="cb935-5" title="5"><span class="kw">xgb.ggplot.importance</span>(importance_matrix, <span class="dt">top_n =</span> <span class="dv">15</span>, </a>
<a class="sourceLine" id="cb935-6" title="6">    <span class="dt">measure =</span> <span class="st">&quot;Gain&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-576-1.png" width="672" /></p>
<p>As we identified the most relevant features, now we can try to understand how the response variable (i.e. predicted sale price) changes based on these variables. For this we can use partial dependence plots (PDPs). They show the marginal effect one or two features have on the predicted outcome.
Let’s consider the “Gr_Liv_Area” variable. The PDP plot below displays the average change in predicted sales price as we vary “Gr_Liv_Area” while holding all other variables constant. More specifically, it shows the movement of the predicted sales price as the square footage of the ground floor in a house changes, while holding other variables constant. It is important to mention that PDPs are valid as long as the target variable (sale price) and the variable under observation (“Gr_Liv_Area”) are not correlated. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible.</p>
<div class="sourceCode" id="cb936"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb936-1" title="1"><span class="kw">library</span>(pdp)</a>
<a class="sourceLine" id="cb936-2" title="2">pdp &lt;-<span class="st"> </span>xgb.fit.optimal <span class="op">%&gt;%</span><span class="st"> </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;gr_liv_area&quot;</span>, </a>
<a class="sourceLine" id="cb936-3" title="3">    <span class="dt">n.trees =</span> <span class="dv">3497</span>, <span class="dt">grid.resolution =</span> <span class="dv">100</span>, <span class="dt">train =</span> ames_x_train) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb936-4" title="4"><span class="st">    </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_x_train) <span class="op">+</span><span class="st"> </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>dollar) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb936-5" title="5"><span class="st">    </span>ggplot2<span class="op">::</span><span class="kw">xlab</span>(<span class="dt">label =</span> <span class="st">&quot;Ground floor living area&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb936-6" title="6"><span class="st">    </span>ggplot2<span class="op">::</span><span class="kw">ylab</span>(<span class="dt">label =</span> <span class="st">&quot;Predicted sale price&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb936-7" title="7"><span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Partial dependency plot - Influence of the ground floor size on a house sale price&quot;</span>)</a>
<a class="sourceLine" id="cb936-8" title="8"><span class="kw">ggplotly</span>(pdp)</a></code></pre></div>
<div id="htmlwidget-bd9f939f5efbe34a8656" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-bd9f939f5efbe34a8656">{"x":{"data":[{"x":[334,387.616161616162,441.232323232323,494.848484848485,548.464646464646,602.080808080808,655.69696969697,709.313131313131,762.929292929293,816.545454545455,870.161616161616,923.777777777778,977.393939393939,1031.0101010101,1084.62626262626,1138.24242424242,1191.85858585859,1245.47474747475,1299.09090909091,1352.70707070707,1406.32323232323,1459.93939393939,1513.55555555556,1567.17171717172,1620.78787878788,1674.40404040404,1728.0202020202,1781.63636363636,1835.25252525253,1888.86868686869,1942.48484848485,1996.10101010101,2049.71717171717,2103.33333333333,2156.9494949495,2210.56565656566,2264.18181818182,2317.79797979798,2371.41414141414,2425.0303030303,2478.64646464646,2532.26262626263,2585.87878787879,2639.49494949495,2693.11111111111,2746.72727272727,2800.34343434343,2853.9595959596,2907.57575757576,2961.19191919192,3014.80808080808,3068.42424242424,3122.0404040404,3175.65656565657,3229.27272727273,3282.88888888889,3336.50505050505,3390.12121212121,3443.73737373737,3497.35353535354,3550.9696969697,3604.58585858586,3658.20202020202,3711.81818181818,3765.43434343434,3819.05050505051,3872.66666666667,3926.28282828283,3979.89898989899,4033.51515151515,4087.13131313131,4140.74747474748,4194.36363636364,4247.9797979798,4301.59595959596,4355.21212121212,4408.82828282828,4462.44444444444,4516.06060606061,4569.67676767677,4623.29292929293,4676.90909090909,4730.52525252525,4784.14141414141,4837.75757575758,4891.37373737374,4944.9898989899,4998.60606060606,5052.22222222222,5105.83838383838,5159.45454545455,5213.07070707071,5266.68686868687,5320.30303030303,5373.91919191919,5427.53535353535,5481.15151515152,5534.76767676768,5588.38383838384,5642],"y":[155916.981539972,155916.981539972,155916.981539972,155916.981539972,155916.981539972,155916.981539972,155916.981539972,155916.981539972,156235.791545528,156493.371641729,161716.635736955,163551.268979466,163643.947257063,167154.792437896,167318.453817584,167515.004720607,169107.399116765,170094.928479283,170094.928479283,170694.36975615,170694.36975615,170694.36975615,175365.005114467,175365.005114467,177922.542572988,182393.222642931,183212.79883859,187435.099382002,190227.151622625,192073.984527216,195444.983168686,203305.230687561,208077.209160375,208077.209160375,208077.209160375,208077.209160375,214182.722361331,222120.939878379,231041.890389065,232676.165382976,236128.109717487,236128.109717487,236128.109717487,236128.109717487,246492.021949586,247028.093339016,250694.696427484,264683.091710302,267664.197150511,267278.228309182,267278.228309182,267278.228309182,268810.38781661,287005.508029408,297990.871544691,298548.246171761,295972.307271371,291157.330826839,291728.102647041,305900.719670908,308310.915740684,308455.793275085,302241.587737457,301191.104214869,296680.792468339,296680.792468339,296680.792468339,296680.792468339,296680.792468339,296680.792468339,295464.413221505,293397.895663359,272795.521599489,272795.521599489,237048.731505723,237048.731505723,237048.731505723,237048.731505723,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525,166622.76847525],"text":["object[[1L]]:  334.0000<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  387.6162<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  441.2323<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  494.8485<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  548.4646<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  602.0808<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  655.6970<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  709.3131<br />object[[\"yhat\"]]: 155917.0","object[[1L]]:  762.9293<br />object[[\"yhat\"]]: 156235.8","object[[1L]]:  816.5455<br />object[[\"yhat\"]]: 156493.4","object[[1L]]:  870.1616<br />object[[\"yhat\"]]: 161716.6","object[[1L]]:  923.7778<br />object[[\"yhat\"]]: 163551.3","object[[1L]]:  977.3939<br />object[[\"yhat\"]]: 163643.9","object[[1L]]: 1031.0101<br />object[[\"yhat\"]]: 167154.8","object[[1L]]: 1084.6263<br />object[[\"yhat\"]]: 167318.5","object[[1L]]: 1138.2424<br />object[[\"yhat\"]]: 167515.0","object[[1L]]: 1191.8586<br />object[[\"yhat\"]]: 169107.4","object[[1L]]: 1245.4747<br />object[[\"yhat\"]]: 170094.9","object[[1L]]: 1299.0909<br />object[[\"yhat\"]]: 170094.9","object[[1L]]: 1352.7071<br />object[[\"yhat\"]]: 170694.4","object[[1L]]: 1406.3232<br />object[[\"yhat\"]]: 170694.4","object[[1L]]: 1459.9394<br />object[[\"yhat\"]]: 170694.4","object[[1L]]: 1513.5556<br />object[[\"yhat\"]]: 175365.0","object[[1L]]: 1567.1717<br />object[[\"yhat\"]]: 175365.0","object[[1L]]: 1620.7879<br />object[[\"yhat\"]]: 177922.5","object[[1L]]: 1674.4040<br />object[[\"yhat\"]]: 182393.2","object[[1L]]: 1728.0202<br />object[[\"yhat\"]]: 183212.8","object[[1L]]: 1781.6364<br />object[[\"yhat\"]]: 187435.1","object[[1L]]: 1835.2525<br />object[[\"yhat\"]]: 190227.2","object[[1L]]: 1888.8687<br />object[[\"yhat\"]]: 192074.0","object[[1L]]: 1942.4848<br />object[[\"yhat\"]]: 195445.0","object[[1L]]: 1996.1010<br />object[[\"yhat\"]]: 203305.2","object[[1L]]: 2049.7172<br />object[[\"yhat\"]]: 208077.2","object[[1L]]: 2103.3333<br />object[[\"yhat\"]]: 208077.2","object[[1L]]: 2156.9495<br />object[[\"yhat\"]]: 208077.2","object[[1L]]: 2210.5657<br />object[[\"yhat\"]]: 208077.2","object[[1L]]: 2264.1818<br />object[[\"yhat\"]]: 214182.7","object[[1L]]: 2317.7980<br />object[[\"yhat\"]]: 222120.9","object[[1L]]: 2371.4141<br />object[[\"yhat\"]]: 231041.9","object[[1L]]: 2425.0303<br />object[[\"yhat\"]]: 232676.2","object[[1L]]: 2478.6465<br />object[[\"yhat\"]]: 236128.1","object[[1L]]: 2532.2626<br />object[[\"yhat\"]]: 236128.1","object[[1L]]: 2585.8788<br />object[[\"yhat\"]]: 236128.1","object[[1L]]: 2639.4949<br />object[[\"yhat\"]]: 236128.1","object[[1L]]: 2693.1111<br />object[[\"yhat\"]]: 246492.0","object[[1L]]: 2746.7273<br />object[[\"yhat\"]]: 247028.1","object[[1L]]: 2800.3434<br />object[[\"yhat\"]]: 250694.7","object[[1L]]: 2853.9596<br />object[[\"yhat\"]]: 264683.1","object[[1L]]: 2907.5758<br />object[[\"yhat\"]]: 267664.2","object[[1L]]: 2961.1919<br />object[[\"yhat\"]]: 267278.2","object[[1L]]: 3014.8081<br />object[[\"yhat\"]]: 267278.2","object[[1L]]: 3068.4242<br />object[[\"yhat\"]]: 267278.2","object[[1L]]: 3122.0404<br />object[[\"yhat\"]]: 268810.4","object[[1L]]: 3175.6566<br />object[[\"yhat\"]]: 287005.5","object[[1L]]: 3229.2727<br />object[[\"yhat\"]]: 297990.9","object[[1L]]: 3282.8889<br />object[[\"yhat\"]]: 298548.2","object[[1L]]: 3336.5051<br />object[[\"yhat\"]]: 295972.3","object[[1L]]: 3390.1212<br />object[[\"yhat\"]]: 291157.3","object[[1L]]: 3443.7374<br />object[[\"yhat\"]]: 291728.1","object[[1L]]: 3497.3535<br />object[[\"yhat\"]]: 305900.7","object[[1L]]: 3550.9697<br />object[[\"yhat\"]]: 308310.9","object[[1L]]: 3604.5859<br />object[[\"yhat\"]]: 308455.8","object[[1L]]: 3658.2020<br />object[[\"yhat\"]]: 302241.6","object[[1L]]: 3711.8182<br />object[[\"yhat\"]]: 301191.1","object[[1L]]: 3765.4343<br />object[[\"yhat\"]]: 296680.8","object[[1L]]: 3819.0505<br />object[[\"yhat\"]]: 296680.8","object[[1L]]: 3872.6667<br />object[[\"yhat\"]]: 296680.8","object[[1L]]: 3926.2828<br />object[[\"yhat\"]]: 296680.8","object[[1L]]: 3979.8990<br />object[[\"yhat\"]]: 296680.8","object[[1L]]: 4033.5152<br />object[[\"yhat\"]]: 296680.8","object[[1L]]: 4087.1313<br />object[[\"yhat\"]]: 295464.4","object[[1L]]: 4140.7475<br />object[[\"yhat\"]]: 293397.9","object[[1L]]: 4194.3636<br />object[[\"yhat\"]]: 272795.5","object[[1L]]: 4247.9798<br />object[[\"yhat\"]]: 272795.5","object[[1L]]: 4301.5960<br />object[[\"yhat\"]]: 237048.7","object[[1L]]: 4355.2121<br />object[[\"yhat\"]]: 237048.7","object[[1L]]: 4408.8283<br />object[[\"yhat\"]]: 237048.7","object[[1L]]: 4462.4444<br />object[[\"yhat\"]]: 237048.7","object[[1L]]: 4516.0606<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4569.6768<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4623.2929<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4676.9091<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4730.5253<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4784.1414<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4837.7576<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4891.3737<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4944.9899<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 4998.6061<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5052.2222<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5105.8384<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5159.4545<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5213.0707<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5266.6869<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5320.3030<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5373.9192<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5427.5354<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5481.1515<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5534.7677<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5588.3838<br />object[[\"yhat\"]]: 166622.8","object[[1L]]: 5642.0000<br />object[[\"yhat\"]]: 166622.8"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"y":[148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475,null,148290.040953216,153323.821740475],"text":"x.rug[[1L]]: NA","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":72.3287671232877},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"Partial dependency plot - Influence of the ground floor size on a house sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[68.6,5907.4],"tickmode":"array","ticktext":["1000","2000","3000","4000","5000"],"tickvals":[1000,2000,3000,4000,5000],"categoryorder":"array","categoryarray":["1000","2000","3000","4000","5000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Ground floor living area","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[148290.040953216,316082.733861841],"tickmode":"array","ticktext":["$150,000","$200,000","$250,000","$300,000"],"tickvals":[150000,200000,250000,300000],"categoryorder":"array","categoryarray":["$150,000","$200,000","$250,000","$300,000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Predicted sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"17483d3f6908":{"x":{},"y":{},"type":"scatter"},"174867182987":{"x":{}}},"cur_data":"17483d3f6908","visdat":{"17483d3f6908":["function (y) ","x"],"174867182987":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>We use predict function to predict unseen observations from the test data set we created at the begining of the chapter. Since we already know the real sale prices from the test data set, we will be able to calculate the error of our predictive model.</p>
<div class="sourceCode" id="cb937"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb937-1" title="1"><span class="co"># Predict values for test data optimal</span></a>
<a class="sourceLine" id="cb937-2" title="2">pred.optimal &lt;-<span class="st"> </span><span class="kw">predict</span>(xgb.fit.optimal, ames_x_test)</a>
<a class="sourceLine" id="cb937-3" title="3"></a>
<a class="sourceLine" id="cb937-4" title="4"><span class="co"># Results with optimal parameters</span></a>
<a class="sourceLine" id="cb937-5" title="5"><span class="kw">RMSE</span>(pred.optimal, ames_test[, <span class="dv">307</span>])</a></code></pre></div>
<pre><code>## [1] 26007.49</code></pre>
<div class="sourceCode" id="cb939"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb939-1" title="1"><span class="co"># ...or another way</span></a>
<a class="sourceLine" id="cb939-2" title="2"><span class="kw">as.data.frame</span>(ames_test) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">residuals =</span> sale_price <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb939-3" title="3"><span class="st">    </span>pred.optimal) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">rmse =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residuals<span class="op">^</span><span class="dv">2</span>)))</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["rmse"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"26007.49"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Finally, we can nicely visualize predicted and actual sale price.</p>
<div class="sourceCode" id="cb940"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb940-1" title="1"><span class="co"># Plot predictions vs actual sale price</span></a>
<a class="sourceLine" id="cb940-2" title="2">ames_test &lt;-<span class="st"> </span><span class="kw">cbind</span>(ames_test, pred.optimal)</a>
<a class="sourceLine" id="cb940-3" title="3">ames_test &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(ames_test)</a>
<a class="sourceLine" id="cb940-4" title="4">p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(ames_test, <span class="kw">aes</span>(<span class="dt">x =</span> pred.optimal, <span class="dt">y =</span> sale_price, </a>
<a class="sourceLine" id="cb940-5" title="5">    <span class="dt">Predicted =</span> pred.optimal, <span class="dt">Tested =</span> sale_price)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb940-6" title="6"><span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb940-7" title="7"><span class="st">    </span><span class="kw">xlab</span>(<span class="dt">label =</span> <span class="st">&quot;Predicted sale price&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="dt">label =</span> <span class="st">&quot;Test sale price&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb940-8" title="8"><span class="st">    </span><span class="kw">ggtitle</span>(<span class="dt">label =</span> <span class="st">&quot;Predicted sale price vs sales price&quot;</span>)</a>
<a class="sourceLine" id="cb940-9" title="9"><span class="kw">ggplotly</span>(p, <span class="dt">tooltip =</span> <span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Tested&quot;</span>))</a></code></pre></div>
<div id="htmlwidget-a8ae33c04c7907213d48" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-a8ae33c04c7907213d48">{"x":{"data":[{"x":[148181.453125,274872.375,193788.171875,170932.203125,225060.484375,200307.59375,362863.375,113212.5234375,240727.140625,142582.890625,117686.296875,109241.28125,105881.328125,140927.46875,103579.0078125,298355.96875,207370.875,303413.15625,175594.203125,181637.515625,232710.640625,171839.53125,291708.15625,435214.78125,234435.953125,128775.84375,153533.40625,123146.9609375,189019.640625,222398.53125,308361.1875,167675.828125,169995.0625,170641.75,176177.53125,163540.546875,189714.71875,202319.890625,201046.953125,136614.5625,160445.671875,136503.71875,161493.015625,169762.3125,141970.375,242609.71875,132818.109375,138136.078125,259671.90625,126332.0234375,178202.90625,152809.96875,124058.9765625,113650.6015625,124873.8203125,148405.25,116951.1015625,131684.6875,135164.25,155911.09375,121904.390625,128093.2734375,164074.8125,123460.5625,144258.84375,129290.125,131669.046875,133788.515625,114438.453125,129082.9140625,130655.5390625,122613.65625,147434.765625,143131.46875,226126.796875,152532.4375,133303.25,172003.375,255867.453125,223786.359375,157881.765625,254586.171875,191519.234375,126017.6171875,165154.421875,122018.7578125,329006.53125,153978.015625,185554.359375,105093.8984375,99135.4453125,101646.265625,96229.2890625,116307.40625,73522.734375,148003.96875,208070.75,198586.4375,112561.484375,108839.8671875,109430.921875,158007.515625,174299.984375,216442.15625,218175.265625,102425.0546875,142158.046875,82011.2734375,197099.375,165003.65625,184861.9375,186832.140625,166683.625,405117.59375,336690.75,186828.109375,170443.28125,194834.140625,182135.4375,176521.984375,166120,141406.390625,184197,220819.734375,141008.703125,108774.3515625,123953.4453125,117960.9375,120235.5234375,139050.09375,134522.828125,138017.265625,139333.6875,163466.9375,363144.03125,476309.4375,415682,311923.40625,271746.6875,386409.46875,495468.8125,329247.1875,501357.6875,314958.5625,199950.765625,204934.21875,210300.625,376563,254298.34375,216727.546875,180564.890625,186427.53125,196075.6875,196826.109375,209952.390625,192925.203125,229156.140625,290929.09375,321104.625,251437.046875,260474.234375,265598.5625,193856.921875,347211.59375,215114.5,225644.28125,177581.3125,145152.34375,201845.234375,169927.9375,228742.5625,204281.96875,199212.40625,136273.3125,151852.78125,69820.375,115317.2578125,175241.25,197271.34375,161126.109375,232805.03125,153001.75,130398.8046875,151641.265625,210202.203125,201116.375,128467.4375,225256.96875,165497.71875,146632.984375,185326.65625,129800.65625,115210.7890625,116905.9921875,131399.5625,163556.859375,168195.375,216119.9375,144725.171875,129179.859375,130942.0078125,176600.234375,166120.5,127757.9921875,126557.4296875,121907.359375,97998.8125,114700.7265625,133363.140625,195171.625,154764.15625,80141.359375,99639.5390625,217332.703125,219467.65625,143551.625,185748.125,84542.328125,120256.796875,134473.046875,136997.578125,93048.34375,97400.234375,95441.703125,103162.0390625,112487.59375,148562.5625,43397.48046875,123488.4453125,127965.203125,112389.1171875,157205.46875,128796.5078125,155242.796875,132069.28125,108593.46875,78496.7265625,151793.96875,133932.375,111231.015625,95631.8046875,155225.640625,149028.15625,108523.8515625,204204.4375,219818.625,181975.234375,144056.375,298128.21875,303924.8125,341413.4375,279027.0625,283215.25,237095.75,154464.765625,182051.046875,214701.171875,208991.96875,182730.234375,120065.2734375,135600.265625,151646.640625,140060.53125,145395.890625,137239.09375,121020.265625,131221.640625,230547.5625,338146.59375,183630.046875,194299.078125,285284.34375,196339.53125,169614.75,116977.2890625,116793.4609375,109026.1875,109983.9921875,88919.1171875,127769.109375,136548.671875,130246.4453125,138688.9375,123151.78125,169277.0625,222945.984375,165483.859375,142617.09375,222281.75,129728.7890625,272511.90625,81762.734375,266013.1875,232284.90625,210745.8125,177751.171875,407123.9375,326514.40625,232244.703125,223701.15625,128155.625,159173.4375,105939.25,141402.578125,125450.0390625,148241.9375,196986.171875,170761.109375,209663.703125,185837.125,177342.125,177699.765625,183548.734375,205287.546875,334997.03125,339726.84375,367222.90625,185700.890625,175582.46875,144722.734375,317338.125,135183.5,118452.140625,181898.703125,101396.6328125,109263,346689.65625,472870.09375,361671.375,278050.15625,396345.03125,355025.21875,212951.28125,188489.484375,208393.59375,250561.21875,196243.078125,190396.03125,220412,366871,269328.28125,196838.015625,190542.953125,250579.171875,293338.0625,257456.96875,191059.984375,186181.1875,187454.078125,214707.984375,197725.984375,171162.921875,176262.015625,139804.8125,131941.375,175599.46875,137024.140625,378755.125,154602.65625,246944.375,265236.75,219300.109375,356244.59375,203882.875,192570.734375,140605.8125,147839.3125,287536.59375,152909.0625,160929.859375,115587.2578125,131019.4609375,156477.1875,142519.15625,126139.4921875,152483.828125,143141.234375,119933.6171875,205120.515625,160917.046875,215454.65625,155745.828125,121052.46875,112924.3671875,112639.71875,106381.2890625,139836.03125,116485.25,85566.828125,156699.359375,123389.9765625,112022.375,109285.546875,116434.546875,113107.25,125961.9140625,80349.046875,131089.09375,107453.46875,146664.71875,166442.328125,82063.671875,112812.59375,109466.5234375,110554.53125,116914.28125,134309.21875,135696.84375,130674.3046875,117697.703125,88777.453125,202868.578125,148818.765625,127675.3671875,105547.2734375,141371.15625,113845.6171875,109399.4921875,81063.8203125,139437.5,177485.390625,114033.609375,120187.6796875,110909.7265625,122255.8671875,105684.875,123885.3671875,108920.6171875,122743.0546875,112861.671875,131235.359375,149724.375,163617.9375,100747.1640625,143457.75,151104.890625,172506.578125,158841.25,147041.84375,196018.359375,116017.40625,265435.46875,235588.109375,188049.203125,252982.09375,193039.1875,202223.828125,172564.0625,231487.765625,216567.59375,143646.796875,117162.9453125,247106.1875,252651.59375,308910.375,239506.109375,178895.6875,153794.734375,198240.296875,166171.234375,127513.515625,191437,156354.375,71875.0859375,111051.765625,86677.1640625,175606.03125,151637.640625,132702.625,125913.2265625,164546.8125,157380.109375,122241.859375,224571.375,138608.484375,113948.40625,276431.59375,289885.09375,226802.46875,213746.40625,246843.046875,198674.859375,277323.34375,270066.8125,239036.296875,152552,225569.203125,224869.078125,327455.71875,260257.828125,331898.125,125561.203125,121544.890625,135242.28125,117649.9296875,88212.8203125,143164.78125,153607.28125,212999.453125,174943.390625,154093.125,174564.453125,336995.5625,235497.703125,352383.3125,188470.203125,147619.171875,166464.71875,128336.6875,140389.078125,116303.3671875,113979.75,133906.6875,375840.09375,363774.34375,275180.8125,285896.5625,313708.84375,316044.5,481031.15625,275353.40625,303360.46875,296526.78125,256195.890625,195908.890625,203830.6875,196505.234375,201806.53125,221664.140625,279587.84375,308798.0625,198576.21875,169306.171875,194728.484375,238806.765625,194688.09375,178821.125,218230.59375,258538.53125,260034.28125,196082.59375,178897.75,210578.71875,551211.75,320332.6875,313900.09375,400396.59375,300812.75,305873.59375,236828.609375,155816,185917.578125,195702.53125,157511.5,142812.59375,120460.359375,285450.3125,140429.296875,347118.125,190460.515625,215979.6875,191858.953125,208306.390625,192461.890625,257555.15625,259130.734375,188579.25,193739.515625,178822.59375,215069.3125,224628.859375,147623.46875,194166.59375,241410.296875,134860.46875,131474.703125,130600.84375,122215.15625,126300.6171875,146211.6875,140018.265625,68797.4453125,126645.7734375,130901.7421875,168415,147889.390625,128346.671875,132195.75,129662.703125,175560.078125,135836.140625,134114.75,118312.28125,146784.796875,127770.515625,314690.03125,112998.96875,128288.9609375,104260.84375,113505.546875,146787.109375,143236.3125,125017.5078125,100344.3203125,108625.984375,136238.78125,100107.8359375,117554.8359375,107945.984375,115202.3515625,132254.640625,80207.2734375,162143.828125,122917.3671875,117260.8515625,123973.3828125,124265.6875,268194.75,108483.1015625,112427.5078125,169640.953125,122586.9140625,110201.015625,127031.359375,111644.421875,119991.0078125,129137.5546875,141563.78125,145996.296875,147644.890625,153119.953125,151442.703125,91754.5390625,218326.40625,103884.359375,129637.0625,97316.3515625,145400.296875,103964.703125,181563.515625,196446.953125,301502.03125,223319.578125,194378.015625,268291.96875,201668.546875,216668.53125,191656.953125,273947.9375,190975.890625,191643.21875,184837,186263.109375,235205.65625,153619.609375,187919.03125,181029.125,184724.046875,173543.171875,139216.265625,141984.125,168084.21875,159944.546875,126901.1484375,97212.7109375,110712.078125,144496.28125,142996.421875,178289.03125,139292.234375,270361.375,199749.484375,134516.375,206236.71875,111933.7265625,133413.96875,145635.078125,128769.15625,74428.71875,280633.90625,160646.53125,138817.015625,220318.84375,202723.71875,219806.125,239457.140625,160541.90625,357333.15625,302341.71875,94313.0546875,95893.8515625,87362.2734375,178595.265625,116354.953125,127762.6953125,207355.734375,188482.34375,292671.375,238754.453125,234841.3125,177646.84375,420310.21875,514804.625,446906.1875,433992.1875,195022.53125,308919.6875,402299.65625,220781.328125,241725.3125,281981.15625,154508.71875,255224.828125,160410.0625,135618.28125,137496.1875,107177.984375,98800.390625,108792.9453125,147569.8125,406324.375,262217.84375,267245.40625,388742.6875,322612.625,273569.25,363725.5625,313125.75,378656.8125,200858.359375,341384.03125,200241.78125,181077.25,181905.21875,293679.8125,195968.953125,267522.65625,196271.5625,327943.75,332356.625,237804.34375,262363.125,185095.578125,153862.046875,165504.328125,154831.34375,160345.46875,118476.15625,126300.1796875,324062.0625,279629.25,157035.25,171855.546875,154372.21875,188524.0625,177669.6875,119311.8125,146857.71875,150664.53125,183444.46875,117813.4921875,143030.765625,125900.1328125,116635.203125,155703.515625,117286.6796875,295502.375,147395.5625,131265.875,133889.890625,250174,135755.890625,295731.625,140204.75,160347.0625,133819.28125,113963.265625,139186.234375,119075.9375,183065.578125,150281.140625,284583.34375,207757.390625,121888.0546875,85481.2109375,78586.203125,135847.421875,123488.5625,116365.6328125,167834.625,126006.25,150495.765625,98710.484375,128006,102507.765625,120617,162074.765625,148925.28125,133753.328125,76099.078125,145842.8125,128167.59375,95230.171875,132001.671875,95392.7734375,91841.1796875,224375.65625,124212.7265625,138414.1875,127560.703125,177572.5,186959.296875,178756.5,126407.1953125,154032.078125,148984.984375,122069.1640625,141282.96875,135579.6875,123583.9921875,256093.9375,246262.90625,274312.3125,236530.8125,277535.875,210090.5625,224892.625,154515.71875,184717.3125,210706.53125,220998.21875,247696.34375,124182.5234375,136386.609375,142771.09375,297511.25,259397.171875,229875.40625,227872.484375,249273.1875,153794.734375,166262.234375,231695.578125,192397.34375,113226.703125,156354.375,165937.328125,161562.828125,161562.828125,100785.7109375,103695.8125,127428.453125,128365.6328125,94631.765625,111648.015625,108987.796875,140093.671875,202265.65625,222906.265625,178152.078125,215207.96875,241679.578125,95403.390625,55346.78125,96787.1171875,79161.6015625,226917.8125,179482.578125,227800.84375,191304.015625,326415.46875,203564.140625,158762.78125,96519.59375,164357.75,145980.265625,235683.796875],"y":[172000,244000,195500,180400,212000,164000,394432,141000,210000,142000,115000,105500,88000,149900,120000,306000,214000,319900,175500,199500,216500,180000,290000,410000,271500,99500,138500,133000,169000,190000,362500,155000,149500,152000,177500,147110,206000,218500,212500,142250,143000,136300,180500,84900,142125,197600,116500,132000,180000,136000,165500,167500,108538,108000,135000,109000,107500,129000,97500,155000,115000,130000,129000,100000,150000,128500,128000,132000,123900,109500,114900,131500,154000,163000,270000,124000,127000,186000,218000,236000,147000,245350,187000,138500,150000,128200,318000,143000,185500,155891,100000,64000,80000,128000,58500,127000,160000,185000,102776,55993,50138,190000,169900,170000,214900,83500,119500,75500,159000,157000,185000,181316,174000,501837,372500,185000,181000,154000,200000,184000,157000,152000,197500,240900,165000,97000,118000,119500,143750,148500,123000,147000,137900,148800,337500,485000,555000,325000,256300,398800,610000,296000,445000,290000,196000,184500,230000,382500,248500,254000,184000,174000,188500,184100,207500,181000,214000,265000,260000,263550,257500,287090,225000,370878,238500,263000,159000,143500,193000,153000,224243,189000,171500,120000,162000,76000,122000,164500,195000,172500,180500,150000,154000,185000,206000,197900,113000,213250,172500,154000,177500,124500,122000,128900,140000,124000,159000,256000,155000,120000,153000,176000,135000,131000,126000,129900,99900,135000,149000,142900,156500,59000,78500,190000,200000,153000,157500,92900,139000,132500,127000,94550,93000,80000,91300,110000,124900,34900,149000,119000,115000,214500,155000,155000,179900,62500,63000,149900,137000,122000,113000,139500,131000,105000,213000,239900,131000,147983,269500,297000,332000,272500,239000,221800,145000,195000,227000,230000,187100,124000,140000,150500,136500,143500,133500,133900,133000,250000,313000,198500,211000,279500,191000,178000,100000,127000,118000,85000,99900,119900,103500,160000,139500,105000,177000,234000,205000,154900,224000,121000,230000,57625,251000,240000,215000,152000,410000,316500,201000,213500,139500,162000,86000,131250,112000,130000,173000,165000,192000,180000,181000,183000,185000,189000,355000,325000,387000,175500,181900,175000,306000,133000,123000,181000,103400,100500,394617,417500,379000,250000,412500,421250,219500,154000,207000,195000,191000,179000,226750,350000,264132,227680,182000,250000,339750,256000,207500,192500,182000,193500,189000,179200,153900,135000,142000,192000,140000,372500,179400,290000,305000,217500,150000,205000,215000,147000,135000,299800,173000,178400,135750,155000,180500,174900,140000,145500,142000,119000,201800,165000,227000,163000,133000,116000,130000,110000,100000,118500,89900,171000,138000,112500,105500,127500,136870,122600,64000,139500,95000,147000,140000,46500,107900,65000,132000,98000,129400,163000,128000,116900,55000,184000,138000,108000,79500,153000,105000,113000,81300,162900,207000,130000,127500,120000,127500,89500,125000,79900,124000,109900,145000,153000,185000,108000,152400,144000,241500,177000,155000,235000,125000,262280,225000,177439,248500,207500,193000,188000,221000,231500,158000,127000,230000,287000,274000,240000,183000,136500,210000,149300,108000,165400,148000,82500,99000,98000,179500,136500,168000,130000,161900,177000,110000,263400,126000,99500,392500,271000,213000,228500,228950,241500,287000,294000,264966,167500,218689,195000,305000,298751,370000,124000,115000,152500,98000,81000,138000,103000,225000,168000,160000,160000,349265,392000,441929,192000,148000,197000,152000,123000,120500,113500,142500,356000,314813,318000,322400,318000,338931,479069,260116,317000,285000,250000,194700,204000,200000,207000,209500,277500,318061,178900,168165,171925,198444,181134,156932,172500,226500,259000,188500,165500,211000,745000,322500,290000,419005,147000,311872,250000,147000,181000,201000,179000,128000,125500,300000,129000,285000,166000,193800,208900,207500,177000,239000,301000,194000,213750,187000,190000,226000,164000,188500,255000,156000,139900,151500,145000,124000,140500,147000,64000,137000,105000,126000,175000,144000,141000,120000,163500,142000,134500,115000,153000,135000,301600,109000,128500,64500,102000,152000,141000,89471,108500,114000,124500,104500,137500,102000,90000,153575,113000,169500,139400,127000,128500,122000,200500,126000,118500,165000,123000,108000,119900,115000,134500,129000,112000,165250,150000,137000,130000,109900,243000,118000,150000,86000,130000,96000,228500,179900,301000,220000,200141,246500,203000,212999,176432,277000,187500,204750,190550,200000,211000,131500,185000,179400,168675,167000,118500,133500,171500,140000,131750,79000,110000,148500,139000,238000,140000,315000,215000,123900,170000,115000,129850,150909,118400,68104,375000,183500,134000,245000,210000,244000,267300,174000,392000,281213,85500,93900,75190,196000,129500,129500,192100,185000,320000,250000,274725,165600,457347,545224,535000,438780,169000,318000,470000,235000,241500,250000,179900,294323,181000,155000,138800,97500,91500,89000,145000,412083,252000,293000,415000,300000,275500,345000,298236,360000,202500,332200,169990,169985,172785,258000,234000,264561,173000,350000,321000,252000,290000,186500,132000,142500,150000,125000,116000,137000,310000,262000,147400,183900,139000,200000,170000,135500,157500,146000,190000,129900,157500,140000,127000,146500,121000,235000,143000,145250,156000,167000,159000,180000,105000,159000,147000,115000,159500,120000,183000,157500,277500,207500,147500,135000,87500,146000,120000,124000,169000,156500,178000,105000,111500,108000,96900,155000,144000,157000,64500,159000,114500,88000,149000,89000,109000,220000,116500,148000,142500,180000,156500,157000,145000,168000,164000,130000,142500,136900,149900,209000,221500,233555,260000,294900,209700,220000,145000,193000,217000,217000,205000,132500,157500,128500,275000,230000,210900,274300,216837,133000,155900,233230,203160,98000,145000,140000,130000,137500,92000,107000,104900,125000,121000,128000,102000,131000,140000,250000,218000,239000,257000,102000,72000,106500,78000,228000,176000,250000,202000,312500,215000,164000,71000,131000,142500,188000],"text":["pred.optimal: 148181.45<br />sale_price: 172000","pred.optimal: 274872.38<br />sale_price: 244000","pred.optimal: 193788.17<br />sale_price: 195500","pred.optimal: 170932.20<br />sale_price: 180400","pred.optimal: 225060.48<br />sale_price: 212000","pred.optimal: 200307.59<br />sale_price: 164000","pred.optimal: 362863.38<br />sale_price: 394432","pred.optimal: 113212.52<br />sale_price: 141000","pred.optimal: 240727.14<br />sale_price: 210000","pred.optimal: 142582.89<br />sale_price: 142000","pred.optimal: 117686.30<br />sale_price: 115000","pred.optimal: 109241.28<br />sale_price: 105500","pred.optimal: 105881.33<br />sale_price:  88000","pred.optimal: 140927.47<br />sale_price: 149900","pred.optimal: 103579.01<br />sale_price: 120000","pred.optimal: 298355.97<br />sale_price: 306000","pred.optimal: 207370.88<br />sale_price: 214000","pred.optimal: 303413.16<br />sale_price: 319900","pred.optimal: 175594.20<br />sale_price: 175500","pred.optimal: 181637.52<br />sale_price: 199500","pred.optimal: 232710.64<br />sale_price: 216500","pred.optimal: 171839.53<br />sale_price: 180000","pred.optimal: 291708.16<br />sale_price: 290000","pred.optimal: 435214.78<br />sale_price: 410000","pred.optimal: 234435.95<br />sale_price: 271500","pred.optimal: 128775.84<br />sale_price:  99500","pred.optimal: 153533.41<br />sale_price: 138500","pred.optimal: 123146.96<br />sale_price: 133000","pred.optimal: 189019.64<br />sale_price: 169000","pred.optimal: 222398.53<br />sale_price: 190000","pred.optimal: 308361.19<br />sale_price: 362500","pred.optimal: 167675.83<br />sale_price: 155000","pred.optimal: 169995.06<br />sale_price: 149500","pred.optimal: 170641.75<br />sale_price: 152000","pred.optimal: 176177.53<br />sale_price: 177500","pred.optimal: 163540.55<br />sale_price: 147110","pred.optimal: 189714.72<br />sale_price: 206000","pred.optimal: 202319.89<br />sale_price: 218500","pred.optimal: 201046.95<br />sale_price: 212500","pred.optimal: 136614.56<br />sale_price: 142250","pred.optimal: 160445.67<br />sale_price: 143000","pred.optimal: 136503.72<br />sale_price: 136300","pred.optimal: 161493.02<br />sale_price: 180500","pred.optimal: 169762.31<br />sale_price:  84900","pred.optimal: 141970.38<br />sale_price: 142125","pred.optimal: 242609.72<br />sale_price: 197600","pred.optimal: 132818.11<br />sale_price: 116500","pred.optimal: 138136.08<br />sale_price: 132000","pred.optimal: 259671.91<br />sale_price: 180000","pred.optimal: 126332.02<br />sale_price: 136000","pred.optimal: 178202.91<br />sale_price: 165500","pred.optimal: 152809.97<br />sale_price: 167500","pred.optimal: 124058.98<br />sale_price: 108538","pred.optimal: 113650.60<br />sale_price: 108000","pred.optimal: 124873.82<br />sale_price: 135000","pred.optimal: 148405.25<br />sale_price: 109000","pred.optimal: 116951.10<br />sale_price: 107500","pred.optimal: 131684.69<br />sale_price: 129000","pred.optimal: 135164.25<br />sale_price:  97500","pred.optimal: 155911.09<br />sale_price: 155000","pred.optimal: 121904.39<br />sale_price: 115000","pred.optimal: 128093.27<br />sale_price: 130000","pred.optimal: 164074.81<br />sale_price: 129000","pred.optimal: 123460.56<br />sale_price: 100000","pred.optimal: 144258.84<br />sale_price: 150000","pred.optimal: 129290.12<br />sale_price: 128500","pred.optimal: 131669.05<br />sale_price: 128000","pred.optimal: 133788.52<br />sale_price: 132000","pred.optimal: 114438.45<br />sale_price: 123900","pred.optimal: 129082.91<br />sale_price: 109500","pred.optimal: 130655.54<br />sale_price: 114900","pred.optimal: 122613.66<br />sale_price: 131500","pred.optimal: 147434.77<br />sale_price: 154000","pred.optimal: 143131.47<br />sale_price: 163000","pred.optimal: 226126.80<br />sale_price: 270000","pred.optimal: 152532.44<br />sale_price: 124000","pred.optimal: 133303.25<br />sale_price: 127000","pred.optimal: 172003.38<br />sale_price: 186000","pred.optimal: 255867.45<br />sale_price: 218000","pred.optimal: 223786.36<br />sale_price: 236000","pred.optimal: 157881.77<br />sale_price: 147000","pred.optimal: 254586.17<br />sale_price: 245350","pred.optimal: 191519.23<br />sale_price: 187000","pred.optimal: 126017.62<br />sale_price: 138500","pred.optimal: 165154.42<br />sale_price: 150000","pred.optimal: 122018.76<br />sale_price: 128200","pred.optimal: 329006.53<br />sale_price: 318000","pred.optimal: 153978.02<br />sale_price: 143000","pred.optimal: 185554.36<br />sale_price: 185500","pred.optimal: 105093.90<br />sale_price: 155891","pred.optimal:  99135.45<br />sale_price: 100000","pred.optimal: 101646.27<br />sale_price:  64000","pred.optimal:  96229.29<br />sale_price:  80000","pred.optimal: 116307.41<br />sale_price: 128000","pred.optimal:  73522.73<br />sale_price:  58500","pred.optimal: 148003.97<br />sale_price: 127000","pred.optimal: 208070.75<br />sale_price: 160000","pred.optimal: 198586.44<br />sale_price: 185000","pred.optimal: 112561.48<br />sale_price: 102776","pred.optimal: 108839.87<br />sale_price:  55993","pred.optimal: 109430.92<br />sale_price:  50138","pred.optimal: 158007.52<br />sale_price: 190000","pred.optimal: 174299.98<br />sale_price: 169900","pred.optimal: 216442.16<br />sale_price: 170000","pred.optimal: 218175.27<br />sale_price: 214900","pred.optimal: 102425.05<br />sale_price:  83500","pred.optimal: 142158.05<br />sale_price: 119500","pred.optimal:  82011.27<br />sale_price:  75500","pred.optimal: 197099.38<br />sale_price: 159000","pred.optimal: 165003.66<br />sale_price: 157000","pred.optimal: 184861.94<br />sale_price: 185000","pred.optimal: 186832.14<br />sale_price: 181316","pred.optimal: 166683.62<br />sale_price: 174000","pred.optimal: 405117.59<br />sale_price: 501837","pred.optimal: 336690.75<br />sale_price: 372500","pred.optimal: 186828.11<br />sale_price: 185000","pred.optimal: 170443.28<br />sale_price: 181000","pred.optimal: 194834.14<br />sale_price: 154000","pred.optimal: 182135.44<br />sale_price: 200000","pred.optimal: 176521.98<br />sale_price: 184000","pred.optimal: 166120.00<br />sale_price: 157000","pred.optimal: 141406.39<br />sale_price: 152000","pred.optimal: 184197.00<br />sale_price: 197500","pred.optimal: 220819.73<br />sale_price: 240900","pred.optimal: 141008.70<br />sale_price: 165000","pred.optimal: 108774.35<br />sale_price:  97000","pred.optimal: 123953.45<br />sale_price: 118000","pred.optimal: 117960.94<br />sale_price: 119500","pred.optimal: 120235.52<br />sale_price: 143750","pred.optimal: 139050.09<br />sale_price: 148500","pred.optimal: 134522.83<br />sale_price: 123000","pred.optimal: 138017.27<br />sale_price: 147000","pred.optimal: 139333.69<br />sale_price: 137900","pred.optimal: 163466.94<br />sale_price: 148800","pred.optimal: 363144.03<br />sale_price: 337500","pred.optimal: 476309.44<br />sale_price: 485000","pred.optimal: 415682.00<br />sale_price: 555000","pred.optimal: 311923.41<br />sale_price: 325000","pred.optimal: 271746.69<br />sale_price: 256300","pred.optimal: 386409.47<br />sale_price: 398800","pred.optimal: 495468.81<br />sale_price: 610000","pred.optimal: 329247.19<br />sale_price: 296000","pred.optimal: 501357.69<br />sale_price: 445000","pred.optimal: 314958.56<br />sale_price: 290000","pred.optimal: 199950.77<br />sale_price: 196000","pred.optimal: 204934.22<br />sale_price: 184500","pred.optimal: 210300.62<br />sale_price: 230000","pred.optimal: 376563.00<br />sale_price: 382500","pred.optimal: 254298.34<br />sale_price: 248500","pred.optimal: 216727.55<br />sale_price: 254000","pred.optimal: 180564.89<br />sale_price: 184000","pred.optimal: 186427.53<br />sale_price: 174000","pred.optimal: 196075.69<br />sale_price: 188500","pred.optimal: 196826.11<br />sale_price: 184100","pred.optimal: 209952.39<br />sale_price: 207500","pred.optimal: 192925.20<br />sale_price: 181000","pred.optimal: 229156.14<br />sale_price: 214000","pred.optimal: 290929.09<br />sale_price: 265000","pred.optimal: 321104.62<br />sale_price: 260000","pred.optimal: 251437.05<br />sale_price: 263550","pred.optimal: 260474.23<br />sale_price: 257500","pred.optimal: 265598.56<br />sale_price: 287090","pred.optimal: 193856.92<br />sale_price: 225000","pred.optimal: 347211.59<br />sale_price: 370878","pred.optimal: 215114.50<br />sale_price: 238500","pred.optimal: 225644.28<br />sale_price: 263000","pred.optimal: 177581.31<br />sale_price: 159000","pred.optimal: 145152.34<br />sale_price: 143500","pred.optimal: 201845.23<br />sale_price: 193000","pred.optimal: 169927.94<br />sale_price: 153000","pred.optimal: 228742.56<br />sale_price: 224243","pred.optimal: 204281.97<br />sale_price: 189000","pred.optimal: 199212.41<br />sale_price: 171500","pred.optimal: 136273.31<br />sale_price: 120000","pred.optimal: 151852.78<br />sale_price: 162000","pred.optimal:  69820.38<br />sale_price:  76000","pred.optimal: 115317.26<br />sale_price: 122000","pred.optimal: 175241.25<br />sale_price: 164500","pred.optimal: 197271.34<br />sale_price: 195000","pred.optimal: 161126.11<br />sale_price: 172500","pred.optimal: 232805.03<br />sale_price: 180500","pred.optimal: 153001.75<br />sale_price: 150000","pred.optimal: 130398.80<br />sale_price: 154000","pred.optimal: 151641.27<br />sale_price: 185000","pred.optimal: 210202.20<br />sale_price: 206000","pred.optimal: 201116.38<br />sale_price: 197900","pred.optimal: 128467.44<br />sale_price: 113000","pred.optimal: 225256.97<br />sale_price: 213250","pred.optimal: 165497.72<br />sale_price: 172500","pred.optimal: 146632.98<br />sale_price: 154000","pred.optimal: 185326.66<br />sale_price: 177500","pred.optimal: 129800.66<br />sale_price: 124500","pred.optimal: 115210.79<br />sale_price: 122000","pred.optimal: 116905.99<br />sale_price: 128900","pred.optimal: 131399.56<br />sale_price: 140000","pred.optimal: 163556.86<br />sale_price: 124000","pred.optimal: 168195.38<br />sale_price: 159000","pred.optimal: 216119.94<br />sale_price: 256000","pred.optimal: 144725.17<br />sale_price: 155000","pred.optimal: 129179.86<br />sale_price: 120000","pred.optimal: 130942.01<br />sale_price: 153000","pred.optimal: 176600.23<br />sale_price: 176000","pred.optimal: 166120.50<br />sale_price: 135000","pred.optimal: 127757.99<br />sale_price: 131000","pred.optimal: 126557.43<br />sale_price: 126000","pred.optimal: 121907.36<br />sale_price: 129900","pred.optimal:  97998.81<br />sale_price:  99900","pred.optimal: 114700.73<br />sale_price: 135000","pred.optimal: 133363.14<br />sale_price: 149000","pred.optimal: 195171.62<br />sale_price: 142900","pred.optimal: 154764.16<br />sale_price: 156500","pred.optimal:  80141.36<br />sale_price:  59000","pred.optimal:  99639.54<br />sale_price:  78500","pred.optimal: 217332.70<br />sale_price: 190000","pred.optimal: 219467.66<br />sale_price: 200000","pred.optimal: 143551.62<br />sale_price: 153000","pred.optimal: 185748.12<br />sale_price: 157500","pred.optimal:  84542.33<br />sale_price:  92900","pred.optimal: 120256.80<br />sale_price: 139000","pred.optimal: 134473.05<br />sale_price: 132500","pred.optimal: 136997.58<br />sale_price: 127000","pred.optimal:  93048.34<br />sale_price:  94550","pred.optimal:  97400.23<br />sale_price:  93000","pred.optimal:  95441.70<br />sale_price:  80000","pred.optimal: 103162.04<br />sale_price:  91300","pred.optimal: 112487.59<br />sale_price: 110000","pred.optimal: 148562.56<br />sale_price: 124900","pred.optimal:  43397.48<br />sale_price:  34900","pred.optimal: 123488.45<br />sale_price: 149000","pred.optimal: 127965.20<br />sale_price: 119000","pred.optimal: 112389.12<br />sale_price: 115000","pred.optimal: 157205.47<br />sale_price: 214500","pred.optimal: 128796.51<br />sale_price: 155000","pred.optimal: 155242.80<br />sale_price: 155000","pred.optimal: 132069.28<br />sale_price: 179900","pred.optimal: 108593.47<br />sale_price:  62500","pred.optimal:  78496.73<br />sale_price:  63000","pred.optimal: 151793.97<br />sale_price: 149900","pred.optimal: 133932.38<br />sale_price: 137000","pred.optimal: 111231.02<br />sale_price: 122000","pred.optimal:  95631.80<br />sale_price: 113000","pred.optimal: 155225.64<br />sale_price: 139500","pred.optimal: 149028.16<br />sale_price: 131000","pred.optimal: 108523.85<br />sale_price: 105000","pred.optimal: 204204.44<br />sale_price: 213000","pred.optimal: 219818.62<br />sale_price: 239900","pred.optimal: 181975.23<br />sale_price: 131000","pred.optimal: 144056.38<br />sale_price: 147983","pred.optimal: 298128.22<br />sale_price: 269500","pred.optimal: 303924.81<br />sale_price: 297000","pred.optimal: 341413.44<br />sale_price: 332000","pred.optimal: 279027.06<br />sale_price: 272500","pred.optimal: 283215.25<br />sale_price: 239000","pred.optimal: 237095.75<br />sale_price: 221800","pred.optimal: 154464.77<br />sale_price: 145000","pred.optimal: 182051.05<br />sale_price: 195000","pred.optimal: 214701.17<br />sale_price: 227000","pred.optimal: 208991.97<br />sale_price: 230000","pred.optimal: 182730.23<br />sale_price: 187100","pred.optimal: 120065.27<br />sale_price: 124000","pred.optimal: 135600.27<br />sale_price: 140000","pred.optimal: 151646.64<br />sale_price: 150500","pred.optimal: 140060.53<br />sale_price: 136500","pred.optimal: 145395.89<br />sale_price: 143500","pred.optimal: 137239.09<br />sale_price: 133500","pred.optimal: 121020.27<br />sale_price: 133900","pred.optimal: 131221.64<br />sale_price: 133000","pred.optimal: 230547.56<br />sale_price: 250000","pred.optimal: 338146.59<br />sale_price: 313000","pred.optimal: 183630.05<br />sale_price: 198500","pred.optimal: 194299.08<br />sale_price: 211000","pred.optimal: 285284.34<br />sale_price: 279500","pred.optimal: 196339.53<br />sale_price: 191000","pred.optimal: 169614.75<br />sale_price: 178000","pred.optimal: 116977.29<br />sale_price: 100000","pred.optimal: 116793.46<br />sale_price: 127000","pred.optimal: 109026.19<br />sale_price: 118000","pred.optimal: 109983.99<br />sale_price:  85000","pred.optimal:  88919.12<br />sale_price:  99900","pred.optimal: 127769.11<br />sale_price: 119900","pred.optimal: 136548.67<br />sale_price: 103500","pred.optimal: 130246.45<br />sale_price: 160000","pred.optimal: 138688.94<br />sale_price: 139500","pred.optimal: 123151.78<br />sale_price: 105000","pred.optimal: 169277.06<br />sale_price: 177000","pred.optimal: 222945.98<br />sale_price: 234000","pred.optimal: 165483.86<br />sale_price: 205000","pred.optimal: 142617.09<br />sale_price: 154900","pred.optimal: 222281.75<br />sale_price: 224000","pred.optimal: 129728.79<br />sale_price: 121000","pred.optimal: 272511.91<br />sale_price: 230000","pred.optimal:  81762.73<br />sale_price:  57625","pred.optimal: 266013.19<br />sale_price: 251000","pred.optimal: 232284.91<br />sale_price: 240000","pred.optimal: 210745.81<br />sale_price: 215000","pred.optimal: 177751.17<br />sale_price: 152000","pred.optimal: 407123.94<br />sale_price: 410000","pred.optimal: 326514.41<br />sale_price: 316500","pred.optimal: 232244.70<br />sale_price: 201000","pred.optimal: 223701.16<br />sale_price: 213500","pred.optimal: 128155.62<br />sale_price: 139500","pred.optimal: 159173.44<br />sale_price: 162000","pred.optimal: 105939.25<br />sale_price:  86000","pred.optimal: 141402.58<br />sale_price: 131250","pred.optimal: 125450.04<br />sale_price: 112000","pred.optimal: 148241.94<br />sale_price: 130000","pred.optimal: 196986.17<br />sale_price: 173000","pred.optimal: 170761.11<br />sale_price: 165000","pred.optimal: 209663.70<br />sale_price: 192000","pred.optimal: 185837.12<br />sale_price: 180000","pred.optimal: 177342.12<br />sale_price: 181000","pred.optimal: 177699.77<br />sale_price: 183000","pred.optimal: 183548.73<br />sale_price: 185000","pred.optimal: 205287.55<br />sale_price: 189000","pred.optimal: 334997.03<br />sale_price: 355000","pred.optimal: 339726.84<br />sale_price: 325000","pred.optimal: 367222.91<br />sale_price: 387000","pred.optimal: 185700.89<br />sale_price: 175500","pred.optimal: 175582.47<br />sale_price: 181900","pred.optimal: 144722.73<br />sale_price: 175000","pred.optimal: 317338.12<br />sale_price: 306000","pred.optimal: 135183.50<br />sale_price: 133000","pred.optimal: 118452.14<br />sale_price: 123000","pred.optimal: 181898.70<br />sale_price: 181000","pred.optimal: 101396.63<br />sale_price: 103400","pred.optimal: 109263.00<br />sale_price: 100500","pred.optimal: 346689.66<br />sale_price: 394617","pred.optimal: 472870.09<br />sale_price: 417500","pred.optimal: 361671.38<br />sale_price: 379000","pred.optimal: 278050.16<br />sale_price: 250000","pred.optimal: 396345.03<br />sale_price: 412500","pred.optimal: 355025.22<br />sale_price: 421250","pred.optimal: 212951.28<br />sale_price: 219500","pred.optimal: 188489.48<br />sale_price: 154000","pred.optimal: 208393.59<br />sale_price: 207000","pred.optimal: 250561.22<br />sale_price: 195000","pred.optimal: 196243.08<br />sale_price: 191000","pred.optimal: 190396.03<br />sale_price: 179000","pred.optimal: 220412.00<br />sale_price: 226750","pred.optimal: 366871.00<br />sale_price: 350000","pred.optimal: 269328.28<br />sale_price: 264132","pred.optimal: 196838.02<br />sale_price: 227680","pred.optimal: 190542.95<br />sale_price: 182000","pred.optimal: 250579.17<br />sale_price: 250000","pred.optimal: 293338.06<br />sale_price: 339750","pred.optimal: 257456.97<br />sale_price: 256000","pred.optimal: 191059.98<br />sale_price: 207500","pred.optimal: 186181.19<br />sale_price: 192500","pred.optimal: 187454.08<br />sale_price: 182000","pred.optimal: 214707.98<br />sale_price: 193500","pred.optimal: 197725.98<br />sale_price: 189000","pred.optimal: 171162.92<br />sale_price: 179200","pred.optimal: 176262.02<br />sale_price: 153900","pred.optimal: 139804.81<br />sale_price: 135000","pred.optimal: 131941.38<br />sale_price: 142000","pred.optimal: 175599.47<br />sale_price: 192000","pred.optimal: 137024.14<br />sale_price: 140000","pred.optimal: 378755.12<br />sale_price: 372500","pred.optimal: 154602.66<br />sale_price: 179400","pred.optimal: 246944.38<br />sale_price: 290000","pred.optimal: 265236.75<br />sale_price: 305000","pred.optimal: 219300.11<br />sale_price: 217500","pred.optimal: 356244.59<br />sale_price: 150000","pred.optimal: 203882.88<br />sale_price: 205000","pred.optimal: 192570.73<br />sale_price: 215000","pred.optimal: 140605.81<br />sale_price: 147000","pred.optimal: 147839.31<br />sale_price: 135000","pred.optimal: 287536.59<br />sale_price: 299800","pred.optimal: 152909.06<br />sale_price: 173000","pred.optimal: 160929.86<br />sale_price: 178400","pred.optimal: 115587.26<br />sale_price: 135750","pred.optimal: 131019.46<br />sale_price: 155000","pred.optimal: 156477.19<br />sale_price: 180500","pred.optimal: 142519.16<br />sale_price: 174900","pred.optimal: 126139.49<br />sale_price: 140000","pred.optimal: 152483.83<br />sale_price: 145500","pred.optimal: 143141.23<br />sale_price: 142000","pred.optimal: 119933.62<br />sale_price: 119000","pred.optimal: 205120.52<br />sale_price: 201800","pred.optimal: 160917.05<br />sale_price: 165000","pred.optimal: 215454.66<br />sale_price: 227000","pred.optimal: 155745.83<br />sale_price: 163000","pred.optimal: 121052.47<br />sale_price: 133000","pred.optimal: 112924.37<br />sale_price: 116000","pred.optimal: 112639.72<br />sale_price: 130000","pred.optimal: 106381.29<br />sale_price: 110000","pred.optimal: 139836.03<br />sale_price: 100000","pred.optimal: 116485.25<br />sale_price: 118500","pred.optimal:  85566.83<br />sale_price:  89900","pred.optimal: 156699.36<br />sale_price: 171000","pred.optimal: 123389.98<br />sale_price: 138000","pred.optimal: 112022.38<br />sale_price: 112500","pred.optimal: 109285.55<br />sale_price: 105500","pred.optimal: 116434.55<br />sale_price: 127500","pred.optimal: 113107.25<br />sale_price: 136870","pred.optimal: 125961.91<br />sale_price: 122600","pred.optimal:  80349.05<br />sale_price:  64000","pred.optimal: 131089.09<br />sale_price: 139500","pred.optimal: 107453.47<br />sale_price:  95000","pred.optimal: 146664.72<br />sale_price: 147000","pred.optimal: 166442.33<br />sale_price: 140000","pred.optimal:  82063.67<br />sale_price:  46500","pred.optimal: 112812.59<br />sale_price: 107900","pred.optimal: 109466.52<br />sale_price:  65000","pred.optimal: 110554.53<br />sale_price: 132000","pred.optimal: 116914.28<br />sale_price:  98000","pred.optimal: 134309.22<br />sale_price: 129400","pred.optimal: 135696.84<br />sale_price: 163000","pred.optimal: 130674.30<br />sale_price: 128000","pred.optimal: 117697.70<br />sale_price: 116900","pred.optimal:  88777.45<br />sale_price:  55000","pred.optimal: 202868.58<br />sale_price: 184000","pred.optimal: 148818.77<br />sale_price: 138000","pred.optimal: 127675.37<br />sale_price: 108000","pred.optimal: 105547.27<br />sale_price:  79500","pred.optimal: 141371.16<br />sale_price: 153000","pred.optimal: 113845.62<br />sale_price: 105000","pred.optimal: 109399.49<br />sale_price: 113000","pred.optimal:  81063.82<br />sale_price:  81300","pred.optimal: 139437.50<br />sale_price: 162900","pred.optimal: 177485.39<br />sale_price: 207000","pred.optimal: 114033.61<br />sale_price: 130000","pred.optimal: 120187.68<br />sale_price: 127500","pred.optimal: 110909.73<br />sale_price: 120000","pred.optimal: 122255.87<br />sale_price: 127500","pred.optimal: 105684.88<br />sale_price:  89500","pred.optimal: 123885.37<br />sale_price: 125000","pred.optimal: 108920.62<br />sale_price:  79900","pred.optimal: 122743.05<br />sale_price: 124000","pred.optimal: 112861.67<br />sale_price: 109900","pred.optimal: 131235.36<br />sale_price: 145000","pred.optimal: 149724.38<br />sale_price: 153000","pred.optimal: 163617.94<br />sale_price: 185000","pred.optimal: 100747.16<br />sale_price: 108000","pred.optimal: 143457.75<br />sale_price: 152400","pred.optimal: 151104.89<br />sale_price: 144000","pred.optimal: 172506.58<br />sale_price: 241500","pred.optimal: 158841.25<br />sale_price: 177000","pred.optimal: 147041.84<br />sale_price: 155000","pred.optimal: 196018.36<br />sale_price: 235000","pred.optimal: 116017.41<br />sale_price: 125000","pred.optimal: 265435.47<br />sale_price: 262280","pred.optimal: 235588.11<br />sale_price: 225000","pred.optimal: 188049.20<br />sale_price: 177439","pred.optimal: 252982.09<br />sale_price: 248500","pred.optimal: 193039.19<br />sale_price: 207500","pred.optimal: 202223.83<br />sale_price: 193000","pred.optimal: 172564.06<br />sale_price: 188000","pred.optimal: 231487.77<br />sale_price: 221000","pred.optimal: 216567.59<br />sale_price: 231500","pred.optimal: 143646.80<br />sale_price: 158000","pred.optimal: 117162.95<br />sale_price: 127000","pred.optimal: 247106.19<br />sale_price: 230000","pred.optimal: 252651.59<br />sale_price: 287000","pred.optimal: 308910.38<br />sale_price: 274000","pred.optimal: 239506.11<br />sale_price: 240000","pred.optimal: 178895.69<br />sale_price: 183000","pred.optimal: 153794.73<br />sale_price: 136500","pred.optimal: 198240.30<br />sale_price: 210000","pred.optimal: 166171.23<br />sale_price: 149300","pred.optimal: 127513.52<br />sale_price: 108000","pred.optimal: 191437.00<br />sale_price: 165400","pred.optimal: 156354.38<br />sale_price: 148000","pred.optimal:  71875.09<br />sale_price:  82500","pred.optimal: 111051.77<br />sale_price:  99000","pred.optimal:  86677.16<br />sale_price:  98000","pred.optimal: 175606.03<br />sale_price: 179500","pred.optimal: 151637.64<br />sale_price: 136500","pred.optimal: 132702.62<br />sale_price: 168000","pred.optimal: 125913.23<br />sale_price: 130000","pred.optimal: 164546.81<br />sale_price: 161900","pred.optimal: 157380.11<br />sale_price: 177000","pred.optimal: 122241.86<br />sale_price: 110000","pred.optimal: 224571.38<br />sale_price: 263400","pred.optimal: 138608.48<br />sale_price: 126000","pred.optimal: 113948.41<br />sale_price:  99500","pred.optimal: 276431.59<br />sale_price: 392500","pred.optimal: 289885.09<br />sale_price: 271000","pred.optimal: 226802.47<br />sale_price: 213000","pred.optimal: 213746.41<br />sale_price: 228500","pred.optimal: 246843.05<br />sale_price: 228950","pred.optimal: 198674.86<br />sale_price: 241500","pred.optimal: 277323.34<br />sale_price: 287000","pred.optimal: 270066.81<br />sale_price: 294000","pred.optimal: 239036.30<br />sale_price: 264966","pred.optimal: 152552.00<br />sale_price: 167500","pred.optimal: 225569.20<br />sale_price: 218689","pred.optimal: 224869.08<br />sale_price: 195000","pred.optimal: 327455.72<br />sale_price: 305000","pred.optimal: 260257.83<br />sale_price: 298751","pred.optimal: 331898.12<br />sale_price: 370000","pred.optimal: 125561.20<br />sale_price: 124000","pred.optimal: 121544.89<br />sale_price: 115000","pred.optimal: 135242.28<br />sale_price: 152500","pred.optimal: 117649.93<br />sale_price:  98000","pred.optimal:  88212.82<br />sale_price:  81000","pred.optimal: 143164.78<br />sale_price: 138000","pred.optimal: 153607.28<br />sale_price: 103000","pred.optimal: 212999.45<br />sale_price: 225000","pred.optimal: 174943.39<br />sale_price: 168000","pred.optimal: 154093.12<br />sale_price: 160000","pred.optimal: 174564.45<br />sale_price: 160000","pred.optimal: 336995.56<br />sale_price: 349265","pred.optimal: 235497.70<br />sale_price: 392000","pred.optimal: 352383.31<br />sale_price: 441929","pred.optimal: 188470.20<br />sale_price: 192000","pred.optimal: 147619.17<br />sale_price: 148000","pred.optimal: 166464.72<br />sale_price: 197000","pred.optimal: 128336.69<br />sale_price: 152000","pred.optimal: 140389.08<br />sale_price: 123000","pred.optimal: 116303.37<br />sale_price: 120500","pred.optimal: 113979.75<br />sale_price: 113500","pred.optimal: 133906.69<br />sale_price: 142500","pred.optimal: 375840.09<br />sale_price: 356000","pred.optimal: 363774.34<br />sale_price: 314813","pred.optimal: 275180.81<br />sale_price: 318000","pred.optimal: 285896.56<br />sale_price: 322400","pred.optimal: 313708.84<br />sale_price: 318000","pred.optimal: 316044.50<br />sale_price: 338931","pred.optimal: 481031.16<br />sale_price: 479069","pred.optimal: 275353.41<br />sale_price: 260116","pred.optimal: 303360.47<br />sale_price: 317000","pred.optimal: 296526.78<br />sale_price: 285000","pred.optimal: 256195.89<br />sale_price: 250000","pred.optimal: 195908.89<br />sale_price: 194700","pred.optimal: 203830.69<br />sale_price: 204000","pred.optimal: 196505.23<br />sale_price: 200000","pred.optimal: 201806.53<br />sale_price: 207000","pred.optimal: 221664.14<br />sale_price: 209500","pred.optimal: 279587.84<br />sale_price: 277500","pred.optimal: 308798.06<br />sale_price: 318061","pred.optimal: 198576.22<br />sale_price: 178900","pred.optimal: 169306.17<br />sale_price: 168165","pred.optimal: 194728.48<br />sale_price: 171925","pred.optimal: 238806.77<br />sale_price: 198444","pred.optimal: 194688.09<br />sale_price: 181134","pred.optimal: 178821.12<br />sale_price: 156932","pred.optimal: 218230.59<br />sale_price: 172500","pred.optimal: 258538.53<br />sale_price: 226500","pred.optimal: 260034.28<br />sale_price: 259000","pred.optimal: 196082.59<br />sale_price: 188500","pred.optimal: 178897.75<br />sale_price: 165500","pred.optimal: 210578.72<br />sale_price: 211000","pred.optimal: 551211.75<br />sale_price: 745000","pred.optimal: 320332.69<br />sale_price: 322500","pred.optimal: 313900.09<br />sale_price: 290000","pred.optimal: 400396.59<br />sale_price: 419005","pred.optimal: 300812.75<br />sale_price: 147000","pred.optimal: 305873.59<br />sale_price: 311872","pred.optimal: 236828.61<br />sale_price: 250000","pred.optimal: 155816.00<br />sale_price: 147000","pred.optimal: 185917.58<br />sale_price: 181000","pred.optimal: 195702.53<br />sale_price: 201000","pred.optimal: 157511.50<br />sale_price: 179000","pred.optimal: 142812.59<br />sale_price: 128000","pred.optimal: 120460.36<br />sale_price: 125500","pred.optimal: 285450.31<br />sale_price: 300000","pred.optimal: 140429.30<br />sale_price: 129000","pred.optimal: 347118.12<br />sale_price: 285000","pred.optimal: 190460.52<br />sale_price: 166000","pred.optimal: 215979.69<br />sale_price: 193800","pred.optimal: 191858.95<br />sale_price: 208900","pred.optimal: 208306.39<br />sale_price: 207500","pred.optimal: 192461.89<br />sale_price: 177000","pred.optimal: 257555.16<br />sale_price: 239000","pred.optimal: 259130.73<br />sale_price: 301000","pred.optimal: 188579.25<br />sale_price: 194000","pred.optimal: 193739.52<br />sale_price: 213750","pred.optimal: 178822.59<br />sale_price: 187000","pred.optimal: 215069.31<br />sale_price: 190000","pred.optimal: 224628.86<br />sale_price: 226000","pred.optimal: 147623.47<br />sale_price: 164000","pred.optimal: 194166.59<br />sale_price: 188500","pred.optimal: 241410.30<br />sale_price: 255000","pred.optimal: 134860.47<br />sale_price: 156000","pred.optimal: 131474.70<br />sale_price: 139900","pred.optimal: 130600.84<br />sale_price: 151500","pred.optimal: 122215.16<br />sale_price: 145000","pred.optimal: 126300.62<br />sale_price: 124000","pred.optimal: 146211.69<br />sale_price: 140500","pred.optimal: 140018.27<br />sale_price: 147000","pred.optimal:  68797.45<br />sale_price:  64000","pred.optimal: 126645.77<br />sale_price: 137000","pred.optimal: 130901.74<br />sale_price: 105000","pred.optimal: 168415.00<br />sale_price: 126000","pred.optimal: 147889.39<br />sale_price: 175000","pred.optimal: 128346.67<br />sale_price: 144000","pred.optimal: 132195.75<br />sale_price: 141000","pred.optimal: 129662.70<br />sale_price: 120000","pred.optimal: 175560.08<br />sale_price: 163500","pred.optimal: 135836.14<br />sale_price: 142000","pred.optimal: 134114.75<br />sale_price: 134500","pred.optimal: 118312.28<br />sale_price: 115000","pred.optimal: 146784.80<br />sale_price: 153000","pred.optimal: 127770.52<br />sale_price: 135000","pred.optimal: 314690.03<br />sale_price: 301600","pred.optimal: 112998.97<br />sale_price: 109000","pred.optimal: 128288.96<br />sale_price: 128500","pred.optimal: 104260.84<br />sale_price:  64500","pred.optimal: 113505.55<br />sale_price: 102000","pred.optimal: 146787.11<br />sale_price: 152000","pred.optimal: 143236.31<br />sale_price: 141000","pred.optimal: 125017.51<br />sale_price:  89471","pred.optimal: 100344.32<br />sale_price: 108500","pred.optimal: 108625.98<br />sale_price: 114000","pred.optimal: 136238.78<br />sale_price: 124500","pred.optimal: 100107.84<br />sale_price: 104500","pred.optimal: 117554.84<br />sale_price: 137500","pred.optimal: 107945.98<br />sale_price: 102000","pred.optimal: 115202.35<br />sale_price:  90000","pred.optimal: 132254.64<br />sale_price: 153575","pred.optimal:  80207.27<br />sale_price: 113000","pred.optimal: 162143.83<br />sale_price: 169500","pred.optimal: 122917.37<br />sale_price: 139400","pred.optimal: 117260.85<br />sale_price: 127000","pred.optimal: 123973.38<br />sale_price: 128500","pred.optimal: 124265.69<br />sale_price: 122000","pred.optimal: 268194.75<br />sale_price: 200500","pred.optimal: 108483.10<br />sale_price: 126000","pred.optimal: 112427.51<br />sale_price: 118500","pred.optimal: 169640.95<br />sale_price: 165000","pred.optimal: 122586.91<br />sale_price: 123000","pred.optimal: 110201.02<br />sale_price: 108000","pred.optimal: 127031.36<br />sale_price: 119900","pred.optimal: 111644.42<br />sale_price: 115000","pred.optimal: 119991.01<br />sale_price: 134500","pred.optimal: 129137.55<br />sale_price: 129000","pred.optimal: 141563.78<br />sale_price: 112000","pred.optimal: 145996.30<br />sale_price: 165250","pred.optimal: 147644.89<br />sale_price: 150000","pred.optimal: 153119.95<br />sale_price: 137000","pred.optimal: 151442.70<br />sale_price: 130000","pred.optimal:  91754.54<br />sale_price: 109900","pred.optimal: 218326.41<br />sale_price: 243000","pred.optimal: 103884.36<br />sale_price: 118000","pred.optimal: 129637.06<br />sale_price: 150000","pred.optimal:  97316.35<br />sale_price:  86000","pred.optimal: 145400.30<br />sale_price: 130000","pred.optimal: 103964.70<br />sale_price:  96000","pred.optimal: 181563.52<br />sale_price: 228500","pred.optimal: 196446.95<br />sale_price: 179900","pred.optimal: 301502.03<br />sale_price: 301000","pred.optimal: 223319.58<br />sale_price: 220000","pred.optimal: 194378.02<br />sale_price: 200141","pred.optimal: 268291.97<br />sale_price: 246500","pred.optimal: 201668.55<br />sale_price: 203000","pred.optimal: 216668.53<br />sale_price: 212999","pred.optimal: 191656.95<br />sale_price: 176432","pred.optimal: 273947.94<br />sale_price: 277000","pred.optimal: 190975.89<br />sale_price: 187500","pred.optimal: 191643.22<br />sale_price: 204750","pred.optimal: 184837.00<br />sale_price: 190550","pred.optimal: 186263.11<br />sale_price: 200000","pred.optimal: 235205.66<br />sale_price: 211000","pred.optimal: 153619.61<br />sale_price: 131500","pred.optimal: 187919.03<br />sale_price: 185000","pred.optimal: 181029.12<br />sale_price: 179400","pred.optimal: 184724.05<br />sale_price: 168675","pred.optimal: 173543.17<br />sale_price: 167000","pred.optimal: 139216.27<br />sale_price: 118500","pred.optimal: 141984.12<br />sale_price: 133500","pred.optimal: 168084.22<br />sale_price: 171500","pred.optimal: 159944.55<br />sale_price: 140000","pred.optimal: 126901.15<br />sale_price: 131750","pred.optimal:  97212.71<br />sale_price:  79000","pred.optimal: 110712.08<br />sale_price: 110000","pred.optimal: 144496.28<br />sale_price: 148500","pred.optimal: 142996.42<br />sale_price: 139000","pred.optimal: 178289.03<br />sale_price: 238000","pred.optimal: 139292.23<br />sale_price: 140000","pred.optimal: 270361.38<br />sale_price: 315000","pred.optimal: 199749.48<br />sale_price: 215000","pred.optimal: 134516.38<br />sale_price: 123900","pred.optimal: 206236.72<br />sale_price: 170000","pred.optimal: 111933.73<br />sale_price: 115000","pred.optimal: 133413.97<br />sale_price: 129850","pred.optimal: 145635.08<br />sale_price: 150909","pred.optimal: 128769.16<br />sale_price: 118400","pred.optimal:  74428.72<br />sale_price:  68104","pred.optimal: 280633.91<br />sale_price: 375000","pred.optimal: 160646.53<br />sale_price: 183500","pred.optimal: 138817.02<br />sale_price: 134000","pred.optimal: 220318.84<br />sale_price: 245000","pred.optimal: 202723.72<br />sale_price: 210000","pred.optimal: 219806.12<br />sale_price: 244000","pred.optimal: 239457.14<br />sale_price: 267300","pred.optimal: 160541.91<br />sale_price: 174000","pred.optimal: 357333.16<br />sale_price: 392000","pred.optimal: 302341.72<br />sale_price: 281213","pred.optimal:  94313.05<br />sale_price:  85500","pred.optimal:  95893.85<br />sale_price:  93900","pred.optimal:  87362.27<br />sale_price:  75190","pred.optimal: 178595.27<br />sale_price: 196000","pred.optimal: 116354.95<br />sale_price: 129500","pred.optimal: 127762.70<br />sale_price: 129500","pred.optimal: 207355.73<br />sale_price: 192100","pred.optimal: 188482.34<br />sale_price: 185000","pred.optimal: 292671.38<br />sale_price: 320000","pred.optimal: 238754.45<br />sale_price: 250000","pred.optimal: 234841.31<br />sale_price: 274725","pred.optimal: 177646.84<br />sale_price: 165600","pred.optimal: 420310.22<br />sale_price: 457347","pred.optimal: 514804.62<br />sale_price: 545224","pred.optimal: 446906.19<br />sale_price: 535000","pred.optimal: 433992.19<br />sale_price: 438780","pred.optimal: 195022.53<br />sale_price: 169000","pred.optimal: 308919.69<br />sale_price: 318000","pred.optimal: 402299.66<br />sale_price: 470000","pred.optimal: 220781.33<br />sale_price: 235000","pred.optimal: 241725.31<br />sale_price: 241500","pred.optimal: 281981.16<br />sale_price: 250000","pred.optimal: 154508.72<br />sale_price: 179900","pred.optimal: 255224.83<br />sale_price: 294323","pred.optimal: 160410.06<br />sale_price: 181000","pred.optimal: 135618.28<br />sale_price: 155000","pred.optimal: 137496.19<br />sale_price: 138800","pred.optimal: 107177.98<br />sale_price:  97500","pred.optimal:  98800.39<br />sale_price:  91500","pred.optimal: 108792.95<br />sale_price:  89000","pred.optimal: 147569.81<br />sale_price: 145000","pred.optimal: 406324.38<br />sale_price: 412083","pred.optimal: 262217.84<br />sale_price: 252000","pred.optimal: 267245.41<br />sale_price: 293000","pred.optimal: 388742.69<br />sale_price: 415000","pred.optimal: 322612.62<br />sale_price: 300000","pred.optimal: 273569.25<br />sale_price: 275500","pred.optimal: 363725.56<br />sale_price: 345000","pred.optimal: 313125.75<br />sale_price: 298236","pred.optimal: 378656.81<br />sale_price: 360000","pred.optimal: 200858.36<br />sale_price: 202500","pred.optimal: 341384.03<br />sale_price: 332200","pred.optimal: 200241.78<br />sale_price: 169990","pred.optimal: 181077.25<br />sale_price: 169985","pred.optimal: 181905.22<br />sale_price: 172785","pred.optimal: 293679.81<br />sale_price: 258000","pred.optimal: 195968.95<br />sale_price: 234000","pred.optimal: 267522.66<br />sale_price: 264561","pred.optimal: 196271.56<br />sale_price: 173000","pred.optimal: 327943.75<br />sale_price: 350000","pred.optimal: 332356.62<br />sale_price: 321000","pred.optimal: 237804.34<br />sale_price: 252000","pred.optimal: 262363.12<br />sale_price: 290000","pred.optimal: 185095.58<br />sale_price: 186500","pred.optimal: 153862.05<br />sale_price: 132000","pred.optimal: 165504.33<br />sale_price: 142500","pred.optimal: 154831.34<br />sale_price: 150000","pred.optimal: 160345.47<br />sale_price: 125000","pred.optimal: 118476.16<br />sale_price: 116000","pred.optimal: 126300.18<br />sale_price: 137000","pred.optimal: 324062.06<br />sale_price: 310000","pred.optimal: 279629.25<br />sale_price: 262000","pred.optimal: 157035.25<br />sale_price: 147400","pred.optimal: 171855.55<br />sale_price: 183900","pred.optimal: 154372.22<br />sale_price: 139000","pred.optimal: 188524.06<br />sale_price: 200000","pred.optimal: 177669.69<br />sale_price: 170000","pred.optimal: 119311.81<br />sale_price: 135500","pred.optimal: 146857.72<br />sale_price: 157500","pred.optimal: 150664.53<br />sale_price: 146000","pred.optimal: 183444.47<br />sale_price: 190000","pred.optimal: 117813.49<br />sale_price: 129900","pred.optimal: 143030.77<br />sale_price: 157500","pred.optimal: 125900.13<br />sale_price: 140000","pred.optimal: 116635.20<br />sale_price: 127000","pred.optimal: 155703.52<br />sale_price: 146500","pred.optimal: 117286.68<br />sale_price: 121000","pred.optimal: 295502.38<br />sale_price: 235000","pred.optimal: 147395.56<br />sale_price: 143000","pred.optimal: 131265.88<br />sale_price: 145250","pred.optimal: 133889.89<br />sale_price: 156000","pred.optimal: 250174.00<br />sale_price: 167000","pred.optimal: 135755.89<br />sale_price: 159000","pred.optimal: 295731.62<br />sale_price: 180000","pred.optimal: 140204.75<br />sale_price: 105000","pred.optimal: 160347.06<br />sale_price: 159000","pred.optimal: 133819.28<br />sale_price: 147000","pred.optimal: 113963.27<br />sale_price: 115000","pred.optimal: 139186.23<br />sale_price: 159500","pred.optimal: 119075.94<br />sale_price: 120000","pred.optimal: 183065.58<br />sale_price: 183000","pred.optimal: 150281.14<br />sale_price: 157500","pred.optimal: 284583.34<br />sale_price: 277500","pred.optimal: 207757.39<br />sale_price: 207500","pred.optimal: 121888.05<br />sale_price: 147500","pred.optimal:  85481.21<br />sale_price: 135000","pred.optimal:  78586.20<br />sale_price:  87500","pred.optimal: 135847.42<br />sale_price: 146000","pred.optimal: 123488.56<br />sale_price: 120000","pred.optimal: 116365.63<br />sale_price: 124000","pred.optimal: 167834.62<br />sale_price: 169000","pred.optimal: 126006.25<br />sale_price: 156500","pred.optimal: 150495.77<br />sale_price: 178000","pred.optimal:  98710.48<br />sale_price: 105000","pred.optimal: 128006.00<br />sale_price: 111500","pred.optimal: 102507.77<br />sale_price: 108000","pred.optimal: 120617.00<br />sale_price:  96900","pred.optimal: 162074.77<br />sale_price: 155000","pred.optimal: 148925.28<br />sale_price: 144000","pred.optimal: 133753.33<br />sale_price: 157000","pred.optimal:  76099.08<br />sale_price:  64500","pred.optimal: 145842.81<br />sale_price: 159000","pred.optimal: 128167.59<br />sale_price: 114500","pred.optimal:  95230.17<br />sale_price:  88000","pred.optimal: 132001.67<br />sale_price: 149000","pred.optimal:  95392.77<br />sale_price:  89000","pred.optimal:  91841.18<br />sale_price: 109000","pred.optimal: 224375.66<br />sale_price: 220000","pred.optimal: 124212.73<br />sale_price: 116500","pred.optimal: 138414.19<br />sale_price: 148000","pred.optimal: 127560.70<br />sale_price: 142500","pred.optimal: 177572.50<br />sale_price: 180000","pred.optimal: 186959.30<br />sale_price: 156500","pred.optimal: 178756.50<br />sale_price: 157000","pred.optimal: 126407.20<br />sale_price: 145000","pred.optimal: 154032.08<br />sale_price: 168000","pred.optimal: 148984.98<br />sale_price: 164000","pred.optimal: 122069.16<br />sale_price: 130000","pred.optimal: 141282.97<br />sale_price: 142500","pred.optimal: 135579.69<br />sale_price: 136900","pred.optimal: 123583.99<br />sale_price: 149900","pred.optimal: 256093.94<br />sale_price: 209000","pred.optimal: 246262.91<br />sale_price: 221500","pred.optimal: 274312.31<br />sale_price: 233555","pred.optimal: 236530.81<br />sale_price: 260000","pred.optimal: 277535.88<br />sale_price: 294900","pred.optimal: 210090.56<br />sale_price: 209700","pred.optimal: 224892.62<br />sale_price: 220000","pred.optimal: 154515.72<br />sale_price: 145000","pred.optimal: 184717.31<br />sale_price: 193000","pred.optimal: 210706.53<br />sale_price: 217000","pred.optimal: 220998.22<br />sale_price: 217000","pred.optimal: 247696.34<br />sale_price: 205000","pred.optimal: 124182.52<br />sale_price: 132500","pred.optimal: 136386.61<br />sale_price: 157500","pred.optimal: 142771.09<br />sale_price: 128500","pred.optimal: 297511.25<br />sale_price: 275000","pred.optimal: 259397.17<br />sale_price: 230000","pred.optimal: 229875.41<br />sale_price: 210900","pred.optimal: 227872.48<br />sale_price: 274300","pred.optimal: 249273.19<br />sale_price: 216837","pred.optimal: 153794.73<br />sale_price: 133000","pred.optimal: 166262.23<br />sale_price: 155900","pred.optimal: 231695.58<br />sale_price: 233230","pred.optimal: 192397.34<br />sale_price: 203160","pred.optimal: 113226.70<br />sale_price:  98000","pred.optimal: 156354.38<br />sale_price: 145000","pred.optimal: 165937.33<br />sale_price: 140000","pred.optimal: 161562.83<br />sale_price: 130000","pred.optimal: 161562.83<br />sale_price: 137500","pred.optimal: 100785.71<br />sale_price:  92000","pred.optimal: 103695.81<br />sale_price: 107000","pred.optimal: 127428.45<br />sale_price: 104900","pred.optimal: 128365.63<br />sale_price: 125000","pred.optimal:  94631.77<br />sale_price: 121000","pred.optimal: 111648.02<br />sale_price: 128000","pred.optimal: 108987.80<br />sale_price: 102000","pred.optimal: 140093.67<br />sale_price: 131000","pred.optimal: 202265.66<br />sale_price: 140000","pred.optimal: 222906.27<br />sale_price: 250000","pred.optimal: 178152.08<br />sale_price: 218000","pred.optimal: 215207.97<br />sale_price: 239000","pred.optimal: 241679.58<br />sale_price: 257000","pred.optimal:  95403.39<br />sale_price: 102000","pred.optimal:  55346.78<br />sale_price:  72000","pred.optimal:  96787.12<br />sale_price: 106500","pred.optimal:  79161.60<br />sale_price:  78000","pred.optimal: 226917.81<br />sale_price: 228000","pred.optimal: 179482.58<br />sale_price: 176000","pred.optimal: 227800.84<br />sale_price: 250000","pred.optimal: 191304.02<br />sale_price: 202000","pred.optimal: 326415.47<br />sale_price: 312500","pred.optimal: 203564.14<br />sale_price: 215000","pred.optimal: 158762.78<br />sale_price: 164000","pred.optimal:  96519.59<br />sale_price:  71000","pred.optimal: 164357.75<br />sale_price: 131000","pred.optimal: 145980.27<br />sale_price: 142500","pred.optimal: 235683.80<br />sale_price: 188000"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[43397.48046875,49825.5091969937,56253.5379252373,62681.566653481,69109.5953817247,75537.6241099684,81965.652838212,88393.6815664557,94821.7102946994,101249.739022943,107677.767751187,114105.79647943,120533.825207674,126961.853935918,133389.882664161,139817.911392405,146245.940120649,152673.968848892,159101.997577136,165530.02630538,171958.055033623,178386.083761867,184814.112490111,191242.141218354,197670.169946598,204098.198674842,210526.227403085,216954.256131329,223382.284859573,229810.313587816,236238.34231606,242666.371044304,249094.399772547,255522.428500791,261950.457229035,268378.485957279,274806.514685522,281234.543413766,287662.57214201,294090.600870253,300518.629598497,306946.65832674,313374.687054984,319802.715783228,326230.744511472,332658.773239715,339086.801967959,345514.830696203,351942.859424446,358370.88815269,364798.916880934,371226.945609177,377654.974337421,384083.003065665,390511.031793908,396939.060522152,403367.089250396,409795.117978639,416223.146706883,422651.175435127,429079.20416337,435507.232891614,441935.261619858,448363.290348101,454791.319076345,461219.347804589,467647.376532832,474075.405261076,480503.43398932,486931.462717563,493359.491445807,499787.520174051,506215.548902294,512643.577630538,519071.606358782,525499.635087025,531927.663815269,538355.692543513,544783.721271756,551211.75],"y":[39920.9248522696,46492.1389745586,53063.3530968475,59634.5672191365,66205.7813414254,72776.9954637144,79348.2095860034,85919.4237082923,92490.6378305813,99061.8519528702,105633.066075159,112204.280197448,118775.494319737,125346.708442026,131917.922564315,138489.136686604,145060.350808893,151631.564931182,158202.779053471,164773.99317576,171345.207298049,177916.421420338,184487.635542627,191058.849664916,197630.063787205,204201.277909494,210772.492031783,217343.706154072,223914.92027636,230486.134398649,237057.348520938,243628.562643227,250199.776765516,256770.990887805,263342.205010094,269913.419132383,276484.633254672,283055.847376961,289627.06149925,296198.275621539,302769.489743828,309340.703866117,315911.917988406,322483.132110695,329054.346232984,335625.560355273,342196.774477562,348767.988599851,355339.20272214,361910.416844429,368481.630966718,375052.845089007,381624.059211296,388195.273333584,394766.487455873,401337.701578162,407908.915700451,414480.12982274,421051.343945029,427622.558067318,434193.772189607,440764.986311896,447336.200434185,453907.414556474,460478.628678763,467049.842801052,473621.056923341,480192.27104563,486763.485167919,493334.699290208,499905.913412497,506477.127534786,513048.341657075,519619.555779364,526190.769901653,532761.984023942,539333.198146231,545904.41226852,552475.626390808,559046.840513097],"text":"","type":"scatter","mode":"lines","name":"fitted values","line":{"width":3.77952755905512,"color":"rgba(51,102,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[43397.48046875,49825.5091969937,56253.5379252373,62681.566653481,69109.5953817247,75537.6241099684,81965.652838212,88393.6815664557,94821.7102946994,101249.739022943,107677.767751187,114105.79647943,120533.825207674,126961.853935918,133389.882664161,139817.911392405,146245.940120649,152673.968848892,159101.997577136,165530.02630538,171958.055033623,178386.083761867,184814.112490111,191242.141218354,197670.169946598,204098.198674842,210526.227403085,216954.256131329,223382.284859573,229810.313587816,236238.34231606,242666.371044304,249094.399772547,255522.428500791,261950.457229035,268378.485957279,274806.514685522,281234.543413766,287662.57214201,294090.600870253,300518.629598497,306946.65832674,313374.687054984,319802.715783228,326230.744511472,332658.773239715,339086.801967959,345514.830696203,351942.859424446,358370.88815269,364798.916880934,371226.945609177,377654.974337421,384083.003065665,390511.031793908,396939.060522152,403367.089250396,409795.117978639,416223.146706883,422651.175435127,429079.20416337,435507.232891614,441935.261619858,448363.290348101,454791.319076345,461219.347804589,467647.376532832,474075.405261076,480503.43398932,486931.462717563,493359.491445807,499787.520174051,506215.548902294,512643.577630538,519071.606358782,525499.635087025,531927.663815269,538355.692543513,544783.721271756,551211.75,551211.75,551211.75,544783.721271756,538355.692543513,531927.663815269,525499.635087025,519071.606358782,512643.577630538,506215.548902294,499787.520174051,493359.491445807,486931.462717563,480503.43398932,474075.405261076,467647.376532832,461219.347804589,454791.319076345,448363.290348101,441935.261619858,435507.232891614,429079.20416337,422651.175435127,416223.146706883,409795.117978639,403367.089250396,396939.060522152,390511.031793908,384083.003065665,377654.974337421,371226.945609177,364798.916880934,358370.88815269,351942.859424446,345514.830696203,339086.801967959,332658.773239715,326230.744511472,319802.715783228,313374.687054984,306946.65832674,300518.629598497,294090.600870253,287662.57214201,281234.543413766,274806.514685522,268378.485957279,261950.457229035,255522.428500791,249094.399772547,242666.371044304,236238.34231606,229810.313587816,223382.284859573,216954.256131329,210526.227403085,204098.198674842,197670.169946598,191242.141218354,184814.112490111,178386.083761867,171958.055033623,165530.02630538,159101.997577136,152673.968848892,146245.940120649,139817.911392405,133389.882664161,126961.853935918,120533.825207674,114105.79647943,107677.767751187,101249.739022943,94821.7102946994,88393.6815664557,81965.652838212,75537.6241099684,69109.5953817247,62681.566653481,56253.5379252373,49825.5091969937,43397.48046875,43397.48046875],"y":[36323.2869871921,43021.906375604,49719.0138121215,56414.4319595886,63107.9576204702,69799.3575743212,76488.3638082568,83174.6681230813,89857.9161494105,96537.7008963022,103213.556095815,109884.949819552,116551.279145474,123211.867052387,129865.963194756,136512.750684925,143151.361317341,149780.901534456,156400.490504149,163009.30966307,169606.660012868,176192.019986922,182765.094157861,189325.842965909,195874.486818433,202411.48360688,208937.484763142,215453.279063659,221959.734257007,228457.74463475,234948.189310206,241431.902679544,247909.656180527,254382.149229294,260850.006879328,267313.781954919,273773.959857926,280230.964733186,286685.166104052,293136.885421094,299586.402204915,306033.959624527,312479.769454892,318924.016418444,325366.861949159,331808.447433968,338248.896991819,344688.319849965,351126.812373056,357564.45979503,364001.337697824,370437.513275011,376873.046413042,383307.990617907,389742.39381078,396176.299012553,402609.74493402,409042.766485862,415475.395220322,421907.659714608,428339.585904481,434771.19737517,441202.515615662,447633.560241473,454064.34919026,460494.898893945,466925.22443052,473355.3396582,479785.257334233,486214.989220328,492644.546176406,499073.938244115,505503.174721377,511932.264229052,518361.214770654,524790.033785944,531218.728199106,537647.304462129,544075.768593926,550504.126215684,550504.126215684,567589.554810511,560875.484187691,554161.52007491,547447.668093355,540733.934261939,534020.325032651,527306.847329675,520593.508592772,513880.316825457,507167.280648588,500454.409360088,493741.713001605,487029.202433059,480316.889416162,473604.786708159,466892.908167266,460181.268871475,453469.885252708,446758.775248622,440047.958474733,433337.456420028,426627.292669737,419917.493159619,413208.086466883,406499.104143772,399790.581100967,393082.556049262,386375.072009549,379668.176903002,372961.924235611,366256.373893827,359551.593071223,352847.657349736,346144.651963305,339442.673276578,332741.830516809,326042.247802946,319344.06652192,312647.448107707,305952.577282741,299259.665821984,292568.956894448,285880.730020737,279195.306651418,272513.056309848,265834.40314086,259159.832546316,252489.897350505,245825.222606911,239166.507731671,232514.524162549,225870.106295714,219234.133244484,212607.499300423,205991.072212108,199385.640755977,192791.856363923,186210.176927392,179640.822853753,173083.754583229,166538.676688449,160005.067602793,153482.228327908,146969.340300445,140465.522688283,133969.881933874,127481.549831665,120999.709494,114523.610575344,108052.576054504,101586.003009438,95123.359511752,88664.1792935033,82208.0553637499,75754.6333531076,69303.6050623807,62854.7024786844,56407.6923815735,49962.3715735131,43518.5627173472,36323.2869871921],"text":"","type":"scatter","mode":"lines","line":{"width":3.77952755905512,"color":"transparent","dash":"solid"},"fill":"toself","fillcolor":"rgba(153,153,153,0.4)","hoveron":"points","hoverinfo":"x+y","showlegend":false,"xaxis":"x","yaxis":"y","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"Predicted sale price vs sales price","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[18006.7669921875,576602.463476563],"tickmode":"array","ticktext":["100000","200000","300000","400000","500000"],"tickvals":[100000,200000,300000,400000,500000],"categoryorder":"array","categoryarray":["100000","200000","300000","400000","500000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Predicted sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-605,780505],"tickmode":"array","ticktext":["0","200000","400000","600000"],"tickvals":[0,200000,400000,600000],"categoryorder":"array","categoryarray":["0","200000","400000","600000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Test sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"4260798b0a":{"x":{},"y":{},"Predicted":{},"Tested":{},"type":"scatter"},"42604716850":{"x":{},"y":{},"Predicted":{},"Tested":{}}},"cur_data":"4260798b0a","visdat":{"4260798b0a":["function (y) ","x"],"42604716850":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="exercise-in-progress" class="section level2">
<h2><span class="header-section-number">10.6</span> Exercise (IN PROGRESS)</h2>
<p>In this exercise you will fit a gradient boosting model using xgboost package to predict the number of e-scooters rented in an hour. Features in the data set are related to weather conditions and other important information. Your task will be to train the model on data from one month (i.e. May) and do prediction on data for another month (i.e. June).</p>
<div id="exercise-to-download-in-progress" class="section level3">
<h3><span class="header-section-number">10.6.1</span> Exercise to download (IN PROGRESS)</h3>
<p>In this exercise you will fit a gradient boosting model using xgboost package to predict the number of e-scooters rented in an hour. Features in the data set are related to weather conditions and other important information. Your task will be to train the model on data from one month (i.e. May) and do prediction on data for another month (i.e. June).</p>
<hr />

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="assignments.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
