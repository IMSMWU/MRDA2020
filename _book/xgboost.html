<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 XGBoost | Marketing Research Design &amp; Analysis 2020</title>
  <meta name="description" content="An Introduction to Statistics Using R" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="11 XGBoost | Marketing Research Design &amp; Analysis 2020" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An Introduction to Statistics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 XGBoost | Marketing Research Design &amp; Analysis 2020" />
  
  <meta name="twitter:description" content="An Introduction to Statistics Using R" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="questionnaire-design.html"/>
<link rel="next" href="exercises.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>
<script src="libs/detect-resize-0.5.3/jquery.resize.js"></script>
<link href="libs/jquery-ui-1.11.4/jquery-ui.min.css" rel="stylesheet" />
<script src="libs/jquery-ui-1.11.4/jquery-ui.min.js"></script>
<script src="libs/d3-3.5.2/d3.min.js"></script>
<script src="libs/vega-1.4.3/vega.min.js"></script>
<script src="libs/lodash-2.2.1/lodash.min.js"></script>
<script>var lodash = _.noConflict();</script>
<link href="libs/ggvis-0.4.5/css/ggvis.css" rel="stylesheet" />
<script src="libs/ggvis-0.4.5/js/ggvis.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">MRDA 2020</a></strong></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html"><i class="fa fa-check"></i>Course materials</a><ul>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#finding-your-way-to-r"><i class="fa fa-check"></i>Finding Your Way To R</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#datacamp"><i class="fa fa-check"></i>DataCamp</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#getting-help"><i class="fa fa-check"></i><b>1.2</b> Getting help</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#functions"><i class="fa fa-check"></i><b>1.3</b> Functions</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#packages"><i class="fa fa-check"></i><b>1.4</b> Packages</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#a-typical-r-session"><i class="fa fa-check"></i><b>1.5</b> A typical R session</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>2</b> Data handling</a><ul>
<li class="chapter" data-level="2.1" data-path="data-handling.html"><a href="data-handling.html#basic-data-handling"><i class="fa fa-check"></i><b>2.1</b> Basic data handling</a><ul>
<li class="chapter" data-level="2.1.1" data-path="data-handling.html"><a href="data-handling.html#creating-objects"><i class="fa fa-check"></i><b>2.1.1</b> Creating objects</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-handling.html"><a href="data-handling.html#data-types"><i class="fa fa-check"></i><b>2.1.2</b> Data types</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-handling.html"><a href="data-handling.html#data-structures"><i class="fa fa-check"></i><b>2.1.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-handling.html"><a href="data-handling.html#advanced-data-handling"><i class="fa fa-check"></i><b>2.2</b> Advanced data handling</a><ul>
<li class="chapter" data-level="2.2.1" data-path="data-handling.html"><a href="data-handling.html#the-dplyr-package"><i class="fa fa-check"></i><b>2.2.1</b> The dplyr package</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-handling.html"><a href="data-handling.html#dealing-with-strings"><i class="fa fa-check"></i><b>2.2.2</b> Dealing with strings</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-handling.html"><a href="data-handling.html#case-study"><i class="fa fa-check"></i><b>2.2.3</b> Case study</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-handling.html"><a href="data-handling.html#data-import-and-export"><i class="fa fa-check"></i><b>2.3</b> Data import and export</a><ul>
<li class="chapter" data-level="2.3.1" data-path="data-handling.html"><a href="data-handling.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>2.3.1</b> Getting data for this course</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-handling.html"><a href="data-handling.html#import-data-created-by-other-software-packages"><i class="fa fa-check"></i><b>2.3.2</b> Import data created by other software packages</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-handling.html"><a href="data-handling.html#export-data"><i class="fa fa-check"></i><b>2.3.3</b> Export data</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-handling.html"><a href="data-handling.html#import-data-from-the-web"><i class="fa fa-check"></i><b>2.3.4</b> Import data from the Web</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>3</b> Summarizing data</a><ul>
<li class="chapter" data-level="3.1" data-path="summarizing-data.html"><a href="summarizing-data.html#summary-statistics"><i class="fa fa-check"></i><b>3.1</b> Summary statistics</a><ul>
<li class="chapter" data-level="3.1.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>3.1.1</b> Categorical variables</a></li>
<li class="chapter" data-level="3.1.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>3.1.2</b> Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="summarizing-data.html"><a href="summarizing-data.html#data-visualization"><i class="fa fa-check"></i><b>3.2</b> Data visualization</a><ul>
<li class="chapter" data-level="3.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>3.2.1</b> Categorical variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>3.2.2</b> Continuous variables</a></li>
<li class="chapter" data-level="3.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#saving-plots"><i class="fa fa-check"></i><b>3.2.3</b> Saving plots</a></li>
<li class="chapter" data-level="3.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#additional-options"><i class="fa fa-check"></i><b>3.2.4</b> Additional options</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="summarizing-data.html"><a href="summarizing-data.html#writing-reports-using-r-markdown"><i class="fa fa-check"></i><b>3.3</b> Writing reports using R-Markdown</a><ul>
<li class="chapter" data-level="3.3.1" data-path="summarizing-data.html"><a href="summarizing-data.html#creating-a-new-r-markdown-document"><i class="fa fa-check"></i><b>3.3.1</b> Creating a new R-Markdown document</a></li>
<li class="chapter" data-level="3.3.2" data-path="summarizing-data.html"><a href="summarizing-data.html#text-and-equations"><i class="fa fa-check"></i><b>3.3.2</b> Text and Equations</a></li>
<li class="chapter" data-level="3.3.3" data-path="summarizing-data.html"><a href="summarizing-data.html#r-code"><i class="fa fa-check"></i><b>3.3.3</b> R-Code</a></li>
<li class="chapter" data-level="3.3.4" data-path="summarizing-data.html"><a href="summarizing-data.html#latex-math"><i class="fa fa-check"></i><b>3.3.4</b> LaTeX Math</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#if-we-knew-it-all"><i class="fa fa-check"></i><b>4.1</b> If we knew it all</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#sampling-from-a-known-population"><i class="fa fa-check"></i><b>4.1.1</b> Sampling from a known population</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>4.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>4.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#using-what-we-actually-know"><i class="fa fa-check"></i><b>4.3</b> Using what we actually know</a><ul>
<li class="chapter" data-level="4.3.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#confidence-intervals-for-the-sample-mean"><i class="fa fa-check"></i><b>4.3.1</b> Confidence Intervals for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>5</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-null-hypothesis"><i class="fa fa-check"></i><b>5.1.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="5.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#statistical-inference-on-a-sample"><i class="fa fa-check"></i><b>5.1.2</b> Statistical inference on a sample</a></li>
<li class="chapter" data-level="5.1.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#choosing-the-right-test"><i class="fa fa-check"></i><b>5.1.3</b> Choosing the right test</a></li>
<li class="chapter" data-level="5.1.4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#summary"><i class="fa fa-check"></i><b>5.1.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-t-test"><i class="fa fa-check"></i><b>5.2</b> One sample t-test</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-two-means"><i class="fa fa-check"></i><b>5.3</b> Comparing two means</a><ul>
<li class="chapter" data-level="5.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#independent-means-t-test"><i class="fa fa-check"></i><b>5.3.1</b> Independent-means t-test</a></li>
<li class="chapter" data-level="5.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#dependent-means-t-test"><i class="fa fa-check"></i><b>5.3.2</b> Dependent-means t-test</a></li>
<li class="chapter" data-level="5.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#further-considerations"><i class="fa fa-check"></i><b>5.3.3</b> Further considerations</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-several-means"><i class="fa fa-check"></i><b>5.4</b> Comparing several means</a><ul>
<li class="chapter" data-level="5.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#decomposing-variance"><i class="fa fa-check"></i><b>5.4.2</b> Decomposing variance</a></li>
<li class="chapter" data-level="5.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-way-anova"><i class="fa fa-check"></i><b>5.4.3</b> One-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>5.5</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="5.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#mann-whitney-u-test-a.k.a.-wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>5.5.1</b> Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test)</a></li>
<li class="chapter" data-level="5.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>5.5.2</b> Wilcoxon signed-rank test</a></li>
<li class="chapter" data-level="5.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>5.5.3</b> Kruskal-Wallis test</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#categorical-data"><i class="fa fa-check"></i><b>5.6</b> Categorical data</a><ul>
<li class="chapter" data-level="5.6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#confidence-intervals-for-proportions"><i class="fa fa-check"></i><b>5.6.1</b> Confidence intervals for proportions</a></li>
<li class="chapter" data-level="5.6.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#chi-square-test"><i class="fa fa-check"></i><b>5.6.2</b> Chi-square test</a></li>
<li class="chapter" data-level="5.6.3" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#sample-size"><i class="fa fa-check"></i><b>5.6.3</b> Sample size</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#summary-of-hypothesis-testing"><i class="fa fa-check"></i><b>5.7</b> Summary of hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#correlation"><i class="fa fa-check"></i><b>6.1</b> Correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#correlation-coefficient"><i class="fa fa-check"></i><b>6.1.1</b> Correlation coefficient</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#significance-testing"><i class="fa fa-check"></i><b>6.1.2</b> Significance testing</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#regression-1"><i class="fa fa-check"></i><b>6.2</b> Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.2.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2.2</b> Multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#potential-problems"><i class="fa fa-check"></i><b>6.3</b> Potential problems</a><ul>
<li class="chapter" data-level="6.3.1" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>6.3.1</b> Outliers</a></li>
<li class="chapter" data-level="6.3.2" data-path="regression.html"><a href="regression.html#influential-observations"><i class="fa fa-check"></i><b>6.3.2</b> Influential observations</a></li>
<li class="chapter" data-level="6.3.3" data-path="regression.html"><a href="regression.html#non-linearity"><i class="fa fa-check"></i><b>6.3.3</b> Non-linearity</a></li>
<li class="chapter" data-level="6.3.4" data-path="regression.html"><a href="regression.html#non-constant-error-variance"><i class="fa fa-check"></i><b>6.3.4</b> Non-constant error variance</a></li>
<li class="chapter" data-level="6.3.5" data-path="regression.html"><a href="regression.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>6.3.5</b> Non-normally distributed errors</a></li>
<li class="chapter" data-level="6.3.6" data-path="regression.html"><a href="regression.html#correlation-of-errors"><i class="fa fa-check"></i><b>6.3.6</b> Correlation of errors</a></li>
<li class="chapter" data-level="6.3.7" data-path="regression.html"><a href="regression.html#collinearity"><i class="fa fa-check"></i><b>6.3.7</b> Collinearity</a></li>
<li class="chapter" data-level="6.3.8" data-path="regression.html"><a href="regression.html#omitted-variables"><i class="fa fa-check"></i><b>6.3.8</b> Omitted Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#categorical-predictors"><i class="fa fa-check"></i><b>6.4</b> Categorical predictors</a><ul>
<li class="chapter" data-level="6.4.1" data-path="regression.html"><a href="regression.html#two-categories"><i class="fa fa-check"></i><b>6.4.1</b> Two categories</a></li>
<li class="chapter" data-level="6.4.2" data-path="regression.html"><a href="regression.html#more-than-two-categories"><i class="fa fa-check"></i><b>6.4.2</b> More than two categories</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>6.5</b> Extensions of the linear model</a><ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#interaction-effects"><i class="fa fa-check"></i><b>6.5.1</b> Interaction effects</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#non-linear-relationships"><i class="fa fa-check"></i><b>6.5.2</b> Non-linear relationships</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="regression.html"><a href="regression.html#summary-of-regression"><i class="fa fa-check"></i><b>6.6</b> Summary of regression</a></li>
<li class="chapter" data-level="6.7" data-path="regression.html"><a href="regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.7</b> Logistic regression</a><ul>
<li class="chapter" data-level="6.7.1" data-path="regression.html"><a href="regression.html#motivation-and-intuition"><i class="fa fa-check"></i><b>6.7.1</b> Motivation and intuition</a></li>
<li class="chapter" data-level="6.7.2" data-path="regression.html"><a href="regression.html#technical-details-of-the-model"><i class="fa fa-check"></i><b>6.7.2</b> Technical details of the model</a></li>
<li class="chapter" data-level="6.7.3" data-path="regression.html"><a href="regression.html#estimation-in-r"><i class="fa fa-check"></i><b>6.7.3</b> Estimation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html"><i class="fa fa-check"></i><b>7</b> Exploratory factor analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#steps-in-factor-analysis"><i class="fa fa-check"></i><b>7.2</b> Steps in factor analysis</a><ul>
<li class="chapter" data-level="7.2.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#are-the-assumptions-satisfied"><i class="fa fa-check"></i><b>7.2.1</b> Are the assumptions satisfied?</a></li>
<li class="chapter" data-level="7.2.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#deriving-factors"><i class="fa fa-check"></i><b>7.2.2</b> Deriving factors</a></li>
<li class="chapter" data-level="7.2.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#factor-interpretation"><i class="fa fa-check"></i><b>7.2.3</b> Factor interpretation</a></li>
<li class="chapter" data-level="7.2.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#creating-new-variables"><i class="fa fa-check"></i><b>7.2.4</b> Creating new variables</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#reliability-analysis"><i class="fa fa-check"></i><b>7.3</b> Reliability analysis</a></li>
<li class="chapter" data-level="7.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#summary-of-factor-analysis"><i class="fa fa-check"></i><b>7.4</b> Summary of factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>8</b> Appendix</a><ul>
<li class="chapter" data-level="8.1" data-path="appendix.html"><a href="appendix.html#random-variables-probability-distributions"><i class="fa fa-check"></i><b>8.1</b> Random Variables &amp; Probability Distributions</a><ul>
<li class="chapter" data-level="8.1.1" data-path="appendix.html"><a href="appendix.html#random-variables"><i class="fa fa-check"></i><b>8.1.1</b> Random variables</a></li>
<li class="chapter" data-level="8.1.2" data-path="appendix.html"><a href="appendix.html#probability-distributions"><i class="fa fa-check"></i><b>8.1.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="8.1.3" data-path="appendix.html"><a href="appendix.html#appendix"><i class="fa fa-check"></i><b>8.1.3</b> Appendix</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="regression.html"><a href="regression.html#regression"><i class="fa fa-check"></i><b>8.2</b> Regression</a><ul>
<li class="chapter" data-level="8.2.1" data-path="appendix.html"><a href="appendix.html#linear-regression"><i class="fa fa-check"></i><b>8.2.1</b> Linear regression</a></li>
<li class="chapter" data-level="8.2.2" data-path="regression.html"><a href="regression.html#logistic-regression"><i class="fa fa-check"></i><b>8.2.2</b> Logistic regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="assignments.html"><a href="assignments.html"><i class="fa fa-check"></i><b>9</b> Assignments</a><ul>
<li class="chapter" data-level="9.1" data-path="assignments.html"><a href="assignments.html#assignment-2-hypothesis-testing"><i class="fa fa-check"></i><b>9.1</b> Assignment 2 (Hypothesis Testing)</a></li>
<li class="chapter" data-level="9.2" data-path="assignments.html"><a href="assignments.html#assignment-3-hypothesis-testing-2"><i class="fa fa-check"></i><b>9.2</b> Assignment 3 (Hypothesis Testing 2)</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="questionnaire-design.html"><a href="questionnaire-design.html"><i class="fa fa-check"></i><b>10</b> Questionnaire design</a><ul>
<li class="chapter" data-level="10.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-design-process"><i class="fa fa-check"></i><b>10.1</b> Questionnaire design process</a><ul>
<li class="chapter" data-level="10.1.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#specification-of-the-information-needed"><i class="fa fa-check"></i><b>10.1.1</b> Specification of the information needed</a></li>
<li class="chapter" data-level="10.1.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#specify-the-interviewing-method"><i class="fa fa-check"></i><b>10.1.2</b> Specify the interviewing method</a></li>
<li class="chapter" data-level="10.1.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#determine-the-content-of-questions"><i class="fa fa-check"></i><b>10.1.3</b> Determine the content of questions</a></li>
<li class="chapter" data-level="10.1.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#inability-and-unwillingness-to-answer"><i class="fa fa-check"></i><b>10.1.4</b> Inability and unwillingness to answer</a></li>
<li class="chapter" data-level="10.1.5" data-path="questionnaire-design.html"><a href="questionnaire-design.html#decide-on-measurement-scales-and-scaling-techniques"><i class="fa fa-check"></i><b>10.1.5</b> Decide on measurement scales and scaling techniques</a></li>
<li class="chapter" data-level="10.1.6" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-structure"><i class="fa fa-check"></i><b>10.1.6</b> Questionnaire structure</a></li>
<li class="chapter" data-level="10.1.7" data-path="questionnaire-design.html"><a href="questionnaire-design.html#question-wording"><i class="fa fa-check"></i><b>10.1.7</b> Question wording</a></li>
<li class="chapter" data-level="10.1.8" data-path="questionnaire-design.html"><a href="questionnaire-design.html#choose-adequate-order"><i class="fa fa-check"></i><b>10.1.8</b> Choose adequate order</a></li>
<li class="chapter" data-level="10.1.9" data-path="questionnaire-design.html"><a href="questionnaire-design.html#test-your-questionnaire"><i class="fa fa-check"></i><b>10.1.9</b> Test your questionnaire</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-in-qualtrics"><i class="fa fa-check"></i><b>10.2</b> Questionnaire in Qualtrics</a></li>
<li class="chapter" data-level="10.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#question-types-and-data-analysis"><i class="fa fa-check"></i><b>10.3</b> Question types and data analysis</a><ul>
<li class="chapter" data-level="10.3.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#multiple-choice-with-a-single-answer"><i class="fa fa-check"></i><b>10.3.1</b> Multiple choice with a single answer</a></li>
<li class="chapter" data-level="10.3.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#multiple-choice-with-multiple-answers"><i class="fa fa-check"></i><b>10.3.2</b> Multiple choice with multiple answers</a></li>
<li class="chapter" data-level="10.3.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#rank-order-question"><i class="fa fa-check"></i><b>10.3.3</b> Rank order question</a></li>
<li class="chapter" data-level="10.3.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#constant-sum-question"><i class="fa fa-check"></i><b>10.3.4</b> Constant Sum question</a></li>
<li class="chapter" data-level="10.3.5" data-path="questionnaire-design.html"><a href="questionnaire-design.html#text-or-number-entry-question"><i class="fa fa-check"></i><b>10.3.5</b> Text or number entry question</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>11</b> XGBoost</a><ul>
<li class="chapter" data-level="11.1" data-path="xgboost.html"><a href="xgboost.html#what-is-machine-learning"><i class="fa fa-check"></i><b>11.1</b> What is machine learning?</a></li>
<li class="chapter" data-level="11.2" data-path="xgboost.html"><a href="xgboost.html#gradient-boosting"><i class="fa fa-check"></i><b>11.2</b> Gradient Boosting</a><ul>
<li class="chapter" data-level="11.2.1" data-path="xgboost.html"><a href="xgboost.html#elements-of-supervised-machine-learning"><i class="fa fa-check"></i><b>11.2.1</b> Elements of supervised machine learning</a></li>
<li class="chapter" data-level="11.2.2" data-path="xgboost.html"><a href="xgboost.html#principle-behind-boosting"><i class="fa fa-check"></i><b>11.2.2</b> Principle behind boosting</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="xgboost.html"><a href="xgboost.html#xgboost-package"><i class="fa fa-check"></i><b>11.3</b> xgboost package</a><ul>
<li class="chapter" data-level="11.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>11.3.1</b> Introduction</a></li>
<li class="chapter" data-level="11.3.2" data-path="xgboost.html"><a href="xgboost.html#data-preparation"><i class="fa fa-check"></i><b>11.3.2</b> Data preparation</a></li>
<li class="chapter" data-level="11.3.3" data-path="xgboost.html"><a href="xgboost.html#engineering"><i class="fa fa-check"></i><b>11.3.3</b> Engineering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>12</b> Exercises</a><ul>
<li class="chapter" data-level="12.1" data-path="exercises.html"><a href="exercises.html#exercise-in-machine-learning-in-progress"><i class="fa fa-check"></i><b>12.1</b> Exercise in Machine learning (IN PROGRESS)</a><ul>
<li class="chapter" data-level="12.1.1" data-path="exercises.html"><a href="exercises.html#exercise-to-download-in-progress"><i class="fa fa-check"></i><b>12.1.1</b> Exercise to download (IN PROGRESS)</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Marketing Research Design &amp; Analysis 2020</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="xgboost" class="section level1">
<h1><span class="header-section-number">11</span> XGBoost</h1>
<p>References:</p>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/abs/1603.02754">XGBoost: A Scalable Tree Boosting System</a></li>
<li><a href="https://bradleyboehmke.github.io/HOML/gbm.html#xgboost">Hands on machine-learning with R</a></li>
<li><a href="https://towardsdatascience.com/predicting-marketing-performance-with-machine-learning-c8472bc7807">Predicting marketing performance with ML</a></li>
<li><a href="https://doi.org/10.1016/j.dss.2014.03.001">A data-driven approach to predict the success of bank telemarketing</a></li>
<li><a href="http://uc-r.github.io/gbm_regression">Extensive tutorial</a></li>
<li><a href="http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/139-gradient-boosting-essentials-in-r-using-xgboost/">Gradient Boosting Essentials in R Using XGBOOST</a></li>
<li><a href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/">A Gentle Introduction to XGBoost for Applied Machine Learning</a></li>
<li><a href="https://datascienceplus.com/gradient-boosting-in-r/">Gradient Boosting in R</a></li>
</ol>
<div id="what-is-machine-learning" class="section level2">
<h2><span class="header-section-number">11.1</span> What is machine learning?</h2>
<p>In essence, the road to machine learning starts with regression. Some typical use cases of machine learning in marketing are:</p>
<ul>
<li>Segmenting customers based on common attributes or purchasing behavior for targeted marketing</li>
<li>Predicting coupon redemption rates for a given marketing campaign</li>
<li>Predicting customer churn so an organization can perform preventative intervention</li>
</ul>
<p>In its core, these tasks all seek to learn from data. In order to do so, we use a given set of features to train an algorithm and extract information we need. These algorithms, known as learners too, can be divided according to the amount and type of supervision needed during training. We distinguish supervised learners which construct predictive models, and unsupervised learners which build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish.</p>
<p>In this chapter we will focus on supervised machine learning, more specifically, on one method called extreme gradient boosting.</p>
</div>
<div id="gradient-boosting" class="section level2">
<h2><span class="header-section-number">11.2</span> Gradient Boosting</h2>
<p>When we talk about machine learning, there are two quite important factors that drive successful application:</p>
<ul>
<li>effective statistical models that are capable of capturing the complex data dependencies</li>
<li>and how scalable are learning systems that learn the model from large datasets.</li>
</ul>
<p>Among the machine learning methods commonly used in practice are gradient tree boosting methods. Gradient boosting machines (GBMs) are a very popular machine learning method, and in this chapter we will introduce you R package “xgboost” and show how it can be used for marketing purposes. It is a scalable machine learning system for tree boosting and GMBs have proven successful across many domains and are one of the leading methods you can find across Kaggle competitions.</p>
<p>When it comes to marketing, it can be used in uplift modeling, i.e. can help a company to identify those who are likely to buy products as a result of receiving a discount or a personalized advertisement. Consequently, it helps a company to maximize profits by keeping advertising costs and overall efforts to the minimum. In the perspective of data analysis and marketing, performance of a marketing campaign can be predicted using algoritham such as GBMs. For instance, in the banking industry optimizing targeting for telemarketing used to be one of the main issues, especially under a growing pressure induced by financial crisis in 2008. A commercial bank from Portugal used data-driven model to predict the result of a telemarketing phone call to sell long term deposits. It was a valuable tool to support client selection decisions made by bank managers. As a result, they identified that inbound calls and an increase in other, highly relevant attributes (such as agent experience or duration of pre-vious calls) enhance the probability for a successful deposit sell.</p>
<div id="elements-of-supervised-machine-learning" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Elements of supervised machine learning</h3>
<p>XGBoost is used in supervised machine learning. Let us decompose the meaning of supervised machine learning.</p>
<p>Supervised machine learning can be described as a process in which training data with multiple <strong>features </strong>(also called: predictor variables, independent variables, attributes, predictor) is used to predict a <strong>target variable</strong> (also called, dependent variable, response, outcome measurement).</p>
<p>The final outcome of supervised machine learning is a predictive model, so it is important to define what a model is. <strong>A model</strong>, in the context of supervised machine learning, contains a mathematical structure or algorithm by which the prediction of a target variable is made from the multiple features used as input. For instance, algorithm that helps you to predict the sale price of your house based on the house attributes. Another important term in the context of machine learning are <strong>parameters</strong>. They denote undetermined part that we need to learn from data.</p>
<p>Finally, as we said, models we build with supervised machine learning are predictive models. “Supervised” refers to a supervisory role of the target variable, which indicates the task that model needs to learn. More specifically, it means that the data which is used for training a model contains target variable. Given a set of training data, the learning algorithm attempts to find the combination of feature values that results in a predicted value as close to the actual target variable as possible.</p>
</div>
<div id="principle-behind-boosting" class="section level3">
<h3><span class="header-section-number">11.2.2</span> Principle behind boosting</h3>
<p>Boosting can be explained as a sequential process. That means that at each particular iteration, a new weak model is trained with respect to the error of the whole ensemble learned by that time. A weak model is one whose predictions (error rate) are only slightly better than random guessing. In simple words, in each iteration a better model is created by adding a new weak model to the existing one, where the purpose of the weak model is to slightly improve the remaining errors of the existing model. This process slowly learns from data and tries to improve its prediction in subsequent iterations.</p>
<p>Among other, boosting is used for solving both regression and classification problems.</p>
<p>Below you can find an illustrative example and explanation for each of them.
<img src="bigd.png" width="276" style="display: block; margin: auto;" /></p>
<p>In the illustration above you can see 4 boxes with pluses and minuses within them representing observations. The ultimate goal of a model we need to develop is of classification nature, i.e. to classify pluses (“+”) and minuses (-) within a box as accurate as possible.</p>
<p>It is important to mention that at the begining all observations are assigned equal weights. However, weights are subject to change after each iteration as misclassified observations in one iteration will be assigned higher weight in the next one. Opposingly, observations that are correctly classified in one iteration will be assigned lower weight in the subsequent iteration.</p>
<p>In the box 1, the first weak learner identified “+” signs just on the left side of the box. It simply misclassified three “+” signs in the middle “-” upper part and recognized only the two on the left side. Consequently, it split the box in two parts (blue and light red), meaning that everything that appears in the blue “-” marked area is classified as “+”, while the rest is classified as “-”.</p>
<p>Although our prediction model at this point does not do great job, it contains information useful for the next weak learner that is being added in the box 2. Next weak learner assigns more weight to three “+” signs that were previously misclassified. Similarly to the previous split, the weak learner split the box 2 again in blue “-” and red “-” marked area. Again, everything in the blue area (left from the splitting line) was classified as “+”, including three minus signs being misclassified. The rest was classified as -". Even though our predicting model looks a bit better, its classification is still incorrect.</p>
<p>In the box 3, our model is becoming even better in classifying. It splitted the box horizontally, so that everything below the line was classified as “-” and above the line as “+”. Despite the progress we still have some missclassified “-” in the blue-marked area as well as wrongly classified “+” below the splitting line (circled signs in the box 3).</p>
<p>Finally, in the box 4 we see the result from combining information obtained from numerous weak learners. It is a weighted combination of the weak classifiers resulting in a strong classifier.Each classifier individually proved pretty poor performance in predicting, as they all show certain misclassification error. However, after combining them, the ultimate goal to classify all points correctly is reached and strong classifier created.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-575"></span>
<img src="boosted_stumps.gif" alt="Boosted regression tree predictions (courtesy of [Brandon Greenwell](https://github.com/bgreenwell))"  />
<p class="caption">
Figure 11.1: Boosted regression tree predictions (courtesy of <a href="https://github.com/bgreenwell">Brandon Greenwell</a>)
</p>
</div>
<p>To understand the whole concept easier, try to follow the image above. On the one hand, the blue curve depicts the real underlying function, while the points depict observations. Moreover, observations include some noise, i.e. errors. On the other hand, you can observe red curve representing constantly improving boosted prediction. More specifically, it illustrates the adjusted predictions after each additional sequential tree is added to the algorithm. At the beginning, you can observe large errors (.i.e. big deviation of the red curve from the blue one) which the boosted algorithm reduces pretty fast. However, as the predictions (.i.e. red curve) get closer to the true underlying function (i.e.blue curve), the contribution to model improvement of each additional tree is smaller and smaller. In the end, the predicted values nearly match to the true underlying function.</p>
</div>
</div>
<div id="xgboost-package" class="section level2">
<h2><span class="header-section-number">11.3</span> xgboost package</h2>
<p>There is an extensive list of packages with GBMs and its variations. However, the most popular implementations which we will cover here is certainly xgboost, which is quite fast and efficient.</p>
<div id="introduction" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Introduction</h3>
<p>XGboost stands for “Extreme Gradient Boosting”, which is an efficient implementation of gradient boosting framework. The xgboost package has been quite popular and successful on Kaggle for data mining competitions.</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb915-1"><a href="xgboost.html#cb915-1"></a><span class="co"># Turn off scientific notation</span></span>
<span id="cb915-2"><a href="xgboost.html#cb915-2"></a><span class="kw">options</span>(<span class="dt">scipen =</span> <span class="dv">9999</span>)</span>
<span id="cb915-3"><a href="xgboost.html#cb915-3"></a></span>
<span id="cb915-4"><a href="xgboost.html#cb915-4"></a><span class="co"># Helper packages</span></span>
<span id="cb915-5"><a href="xgboost.html#cb915-5"></a><span class="kw">library</span>(dplyr)      <span class="co"># for general data wrangling needs</span></span>
<span id="cb915-6"><a href="xgboost.html#cb915-6"></a></span>
<span id="cb915-7"><a href="xgboost.html#cb915-7"></a><span class="co"># Modeling packages</span></span>
<span id="cb915-8"><a href="xgboost.html#cb915-8"></a><span class="kw">library</span>(xgboost)    <span class="co"># for fitting extreme gradient boosting</span></span>
<span id="cb915-9"><a href="xgboost.html#cb915-9"></a><span class="kw">library</span>(rsample)    <span class="co"># for split of data set in the training data and test data</span></span>
<span id="cb915-10"><a href="xgboost.html#cb915-10"></a><span class="kw">library</span>(AmesHousing)<span class="co"># data set</span></span>
<span id="cb915-11"><a href="xgboost.html#cb915-11"></a><span class="kw">library</span>(caret)      <span class="co"># for resampling and model training</span></span>
<span id="cb915-12"><a href="xgboost.html#cb915-12"></a><span class="kw">library</span>(plotly)</span>
<span id="cb915-13"><a href="xgboost.html#cb915-13"></a><span class="kw">library</span>(recipes)</span>
<span id="cb915-14"><a href="xgboost.html#cb915-14"></a><span class="kw">library</span>(pdp)</span>
<span id="cb915-15"><a href="xgboost.html#cb915-15"></a><span class="kw">library</span>(knitr)</span>
<span id="cb915-16"><a href="xgboost.html#cb915-16"></a><span class="kw">library</span>(gbm)</span>
<span id="cb915-17"><a href="xgboost.html#cb915-17"></a><span class="kw">library</span>(mlr)</span>
<span id="cb915-18"><a href="xgboost.html#cb915-18"></a><span class="kw">library</span>(ggplot2)</span></code></pre></div>
<p>To explain gradient boosting with xgboost, we will use typical Ames Iowa Housing data set and try to build a model that predict sale price for houses.</p>
<div class="sourceCode" id="cb916"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb916-1"><a href="xgboost.html#cb916-1"></a><span class="co"># Ames housing data</span></span>
<span id="cb916-2"><a href="xgboost.html#cb916-2"></a>ames &lt;-<span class="st"> </span>AmesHousing<span class="op">::</span><span class="kw">make_ames</span>()</span>
<span id="cb916-3"><a href="xgboost.html#cb916-3"></a></span>
<span id="cb916-4"><a href="xgboost.html#cb916-4"></a><span class="co"># Ensure correct naming</span></span>
<span id="cb916-5"><a href="xgboost.html#cb916-5"></a><span class="kw">library</span>(janitor)</span>
<span id="cb916-6"><a href="xgboost.html#cb916-6"></a>ames&lt;-<span class="st"> </span>ames<span class="op">%&gt;%</span><span class="kw">clean_names</span>()</span>
<span id="cb916-7"><a href="xgboost.html#cb916-7"></a></span>
<span id="cb916-8"><a href="xgboost.html#cb916-8"></a><span class="co"># Use mlr package to get an overview of your data</span></span>
<span id="cb916-9"><a href="xgboost.html#cb916-9"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">head</span>(<span class="kw">summarizeColumns</span>(ames)))</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:left;">
name
</th>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
na
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
disp
</th>
<th style="text-align:right;">
median
</th>
<th style="text-align:right;">
mad
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
max
</th>
<th style="text-align:right;">
nlevs
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ms_sub_class
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
0.6317406
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1079
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:left;">
ms_zoning
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
0.2242321
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2273
</td>
<td style="text-align:right;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
lot_frontage
</td>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
57.64778
</td>
<td style="text-align:right;">
33.4994408
</td>
<td style="text-align:right;">
63.0
</td>
<td style="text-align:right;">
25.2042
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
313
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
lot_area
</td>
<td style="text-align:left;">
integer
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
10147.92184
</td>
<td style="text-align:right;">
7880.0177594
</td>
<td style="text-align:right;">
9436.5
</td>
<td style="text-align:right;">
3024.5040
</td>
<td style="text-align:right;">
1300
</td>
<td style="text-align:right;">
215245
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
street
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
0.0040956
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
2918
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
alley
</td>
<td style="text-align:left;">
factor
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
0.0675768
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
78
</td>
<td style="text-align:right;">
2732
</td>
<td style="text-align:right;">
3
</td>
</tr>
</tbody>
</table>
</div>
<div id="data-preparation" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Data preparation</h3>
<p>First, we need to deal with data preparation. When using xgboost package, it is necessary to convert the categorical variables into numeric using one hot encoding.</p>
<p>What is one hot encoding?
Usually, when between several categories exist ordinal relationship (e.g. variable “place” can be “1st”, “2nd” and so on), all you need to do is so called the integer encoding. In the ordinal variable (such as “place”) the integer values have a natural ordered relationship between each other, so machine learning algorithms are able to understand this relationship. However, for categorical variables where no ordinal relationship exists (e.g. variable “pet” with “dog”, “cat” and “rabbit”), the integer encoding is not sufficient and you need one hot encoding. In the “pet” example, there are 3 categories, thus 3 binary variables are required. Therefore, “1” value is placed in the binary variable for the respective “color” and “0” values for the other colors.</p>
<div class="sourceCode" id="cb917"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb917-1"><a href="xgboost.html#cb917-1"></a><span class="kw">cat</span>(tabl)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th>dog</th>
<th>cat</th>
<th>rabbit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can apply one hot encoding to our data set by using R’s base function <code>model.matrix</code>. In the code below, ~.+0 leads to encoding of all categorical variables without producing an intercept.</p>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb918-1"><a href="xgboost.html#cb918-1"></a><span class="co"># One hot encoding (turning the test data into matrix with all numerical values)</span></span>
<span id="cb918-2"><a href="xgboost.html#cb918-2"></a>ames.he &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>.<span class="op">+</span><span class="dv">0</span>,<span class="dt">data =</span> ames)</span>
<span id="cb918-3"><a href="xgboost.html#cb918-3"></a></span>
<span id="cb918-4"><a href="xgboost.html#cb918-4"></a><span class="co"># Save variable names due to more practical addressing columns later on</span></span>
<span id="cb918-5"><a href="xgboost.html#cb918-5"></a>setcol &lt;-<span class="st"> </span><span class="kw">colnames</span>(ames.he)</span></code></pre></div>
<p>Next,we need to break our data set into training and test data, while ensuring we have consistent distributions between the training and test sets.</p>
<div class="sourceCode" id="cb919"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb919-1"><a href="xgboost.html#cb919-1"></a><span class="co"># Data split on test and train data</span></span>
<span id="cb919-2"><a href="xgboost.html#cb919-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb919-3"><a href="xgboost.html#cb919-3"></a></span>
<span id="cb919-4"><a href="xgboost.html#cb919-4"></a><span class="co"># Create partition</span></span>
<span id="cb919-5"><a href="xgboost.html#cb919-5"></a><span class="kw">library</span>(caret)</span>
<span id="cb919-6"><a href="xgboost.html#cb919-6"></a>index &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">createDataPartition</span>(ames<span class="op">$</span>sale_price, <span class="dt">p =</span> <span class="fl">0.7</span>, <span class="dt">list =</span> F)</span></code></pre></div>
<p><code>index</code> is a matrix with just one column that contains approximately 70% of rows (i.e. numbers of rows) from our original data set.</p>
<p>Now we use <code>index</code> to address columns we want to assign to our train data (<code>ames_train</code>), and columns that we want to assign to our test data (<code>ames_test</code>).</p>
<p>Note that by using “-” in front of <code>index</code> we assign to <code>ames_test</code> all observations <strong>except</strong> those that are in <code>index</code>.</p>
<div class="sourceCode" id="cb920"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb920-1"><a href="xgboost.html#cb920-1"></a><span class="co"># Split data set in train and test data</span></span>
<span id="cb920-2"><a href="xgboost.html#cb920-2"></a>ames_train &lt;-<span class="st"> </span>ames.he[index, ]</span>
<span id="cb920-3"><a href="xgboost.html#cb920-3"></a>ames_test &lt;-<span class="st"> </span>ames.he[<span class="op">-</span>index, ]</span></code></pre></div>
<p>If we take lake look at the dimensions of our test and train data, we can see that our split was successful.</p>
<div class="sourceCode" id="cb921"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb921-1"><a href="xgboost.html#cb921-1"></a><span class="kw">dim</span>(ames.he)   <span class="co"># 2930 observations in total</span></span>
<span id="cb921-2"><a href="xgboost.html#cb921-2"></a><span class="kw">dim</span>(ames_train)<span class="co"># 2053 observations for training data</span></span>
<span id="cb921-3"><a href="xgboost.html#cb921-3"></a><span class="kw">dim</span>(ames_test) <span class="co"># 877 observations for testing data</span></span></code></pre></div>
<p>We are still not done with preparation of data. Since our task is to build a predictive model for house pricing based on multiple features, our target variable (sale price) needs to be excluded from the test data. The “real” target variable data (the real sales price) will be used at the end when we test accuracy of our predictive model.</p>
<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb922-1"><a href="xgboost.html#cb922-1"></a><span class="co"># Test data </span></span>
<span id="cb922-2"><a href="xgboost.html#cb922-2"></a><span class="co">## Matrix containing all columns from the test data except dependent variable &quot;Sale_Price&quot;</span></span>
<span id="cb922-3"><a href="xgboost.html#cb922-3"></a><span class="kw">colnames</span>(ames_test[,<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>])<span class="co"># &quot;sale_price&quot; is a column number 308</span></span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;sale_price&quot;</code></pre>
<div class="sourceCode" id="cb924"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb924-1"><a href="xgboost.html#cb924-1"></a><span class="co"># Addressing &quot;Sale Price&quot; column in matrix and excluding it</span></span>
<span id="cb924-2"><a href="xgboost.html#cb924-2"></a>ames_x_test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ames_test[,<span class="op">-</span><span class="dv">308</span>]) </span>
<span id="cb924-3"><a href="xgboost.html#cb924-3"></a></span>
<span id="cb924-4"><a href="xgboost.html#cb924-4"></a><span class="co"># No &quot;Sale Price&quot; anymore here! It used to be among last columns.</span></span>
<span id="cb924-5"><a href="xgboost.html#cb924-5"></a><span class="kw">colnames</span>(ames_x_test)[<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>] </span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;longitude&quot;</code></pre>
<p>For training purposes, target variable (sale price) needs to be excluded from the rest of features, but not totally from the train data set.</p>
<div class="sourceCode" id="cb926"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb926-1"><a href="xgboost.html#cb926-1"></a><span class="co"># Train data set</span></span>
<span id="cb926-2"><a href="xgboost.html#cb926-2"></a><span class="co"># Matrix containing all columns from the train data except dependent variable &quot;Sale_Price&quot;</span></span>
<span id="cb926-3"><a href="xgboost.html#cb926-3"></a><span class="kw">colnames</span>(ames_train[,<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>])<span class="co"># &quot;sale_price&quot; is a column number 308</span></span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;sale_price&quot;</code></pre>
<div class="sourceCode" id="cb928"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb928-1"><a href="xgboost.html#cb928-1"></a><span class="co"># Addressing &quot;Sale Price&quot; column in matrix and excluding it</span></span>
<span id="cb928-2"><a href="xgboost.html#cb928-2"></a>ames_x_train &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ames_train[,<span class="op">-</span><span class="dv">308</span>])</span>
<span id="cb928-3"><a href="xgboost.html#cb928-3"></a></span>
<span id="cb928-4"><a href="xgboost.html#cb928-4"></a><span class="co"># No &quot;Sale Price&quot; anymore here!</span></span>
<span id="cb928-5"><a href="xgboost.html#cb928-5"></a><span class="kw">colnames</span>(ames_x_train[,<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>])</span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;longitude&quot;</code></pre>
<p>Therefore, the target variable is going to be stored separately because the learning algorithm in a predictive model attempts to discover and model the relationships among the target variable and the other features.</p>
<div class="sourceCode" id="cb930"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb930-1"><a href="xgboost.html#cb930-1"></a><span class="co"># Dependent/Target variable &quot;Sales_Price&quot; from the train data in a from of a vector</span></span>
<span id="cb930-2"><a href="xgboost.html#cb930-2"></a>ames_y_train &lt;-<span class="st"> </span>ames_train[,<span class="dv">308</span>]</span></code></pre></div>
</div>
<div id="engineering" class="section level3">
<h3><span class="header-section-number">11.3.3</span> Engineering</h3>
<p>In order to create a good predictive model, usually the most of time is spent optimizing parameters. Before we start training our model, let us take a closer look at what parameters we need to handle.</p>
<p>There are 3 categories XGBoost parameters can be divided into:</p>
<ol style="list-style-type: decimal">
<li>General parameters</li>
<li>Boosting parameters</li>
<li>Tree-specific parameters</li>
</ol>
<p>General parameters will not be discussed in further details, but it consists of 3 parameters:</p>
<ol style="list-style-type: decimal">
<li><code>booster</code> - determines the booster type (gbtree, gblinear or dart) to use. For regression, you can use any. By default it is gbtree (which we will use as well).<br />
</li>
<li><code>nthread</code> - refers to the number of cores activated when computing. By default it uses maximum cores available, which leads to the fastest computation.<br />
</li>
<li><code>silent</code> - refers to turning on (“1”) running messages in R console. By default “0” is set, so that console does not get flooded with messages.</li>
</ol>
<p>For general parameters we will be using default options.</p>
<p>Next, booster parameters control the performance of the selected booster(gbtree in our case). At this moment we will introduce just the main ones:</p>
<ol style="list-style-type: decimal">
<li><code>nrounds</code> - set the maximum number of iterations.<br />
</li>
<li><code>eta</code> - stands for the learning rate. It determines the rate at which our model learns patterns in data. After every iteration, it shrinks the feature weights to reach the best optimum. Smaller learning rates lead to longer computation time. It is important to note that smaller learning rates should be supported by increasing number of iterations. Otherwise, the risk of reaching the optimum is more likely. Usually, it lies between 0.01 - 0.3.</li>
<li><code>max_depth</code> - which determines the maximum depth of each tree. Generally, it is stands that larger the depth, more complex the model and consequently higher chances of overfitting.
4.<code>min_child_weight</code> - minimum number of observations required in each terminal node</li>
<li><code>subsample</code> - percent of training data to sample for each tree</li>
<li><code>colsample_bytrees</code> - percent of columns to sample from for each tree</li>
<li><code>early_stopping_rounds</code> - stopping the training model as soon as evaluation metric (for regression that is “RMSE”) does not improve for a given number of rounds</li>
</ol>
<p>Finally, learning task parameters define methods for the loss function and model evaluation:</p>
<ol style="list-style-type: decimal">
<li><code>objective</code>- for linear regression it should be set to “reg:linear”.</li>
<li><code>eval_metric</code> - this parameter depends on <code>objective</code>. Here we set metrics used to evaluate a model’s accuracy on validation data. When “reg:linear” set as objective, default metric is RMSE.</li>
</ol>
<p>A package with useful tools for parameter optimization is <code>mlr</code>. It includes extensive list of parameters for any type of algorithm. We can take a look at list with parameters for regression and check parameters we just discussed.</p>
<div class="sourceCode" id="cb931"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb931-1"><a href="xgboost.html#cb931-1"></a><span class="co"># Parameters for regression</span></span>
<span id="cb931-2"><a href="xgboost.html#cb931-2"></a><span class="kw">getParamSet</span>(<span class="st">&quot;regr.xgboost&quot;</span>)</span></code></pre></div>
<pre><code>##                                 Type  len           Def
## booster                     discrete    -        gbtree
## watchlist                    untyped    -        &lt;NULL&gt;
## eta                          numeric    -           0.3
## gamma                        numeric    -             0
## max_depth                    integer    -             6
## min_child_weight             numeric    -             1
## subsample                    numeric    -             1
## colsample_bytree             numeric    -             1
## colsample_bylevel            numeric    -             1
## colsample_bynode             numeric    -             1
## num_parallel_tree            integer    -             1
## lambda                       numeric    -             1
## lambda_bias                  numeric    -             0
## alpha                        numeric    -             0
## objective                    untyped    -    reg:linear
## eval_metric                  untyped    -          rmse
## base_score                   numeric    -           0.5
## max_delta_step               numeric    -             0
## missing                      numeric    -              
## monotone_constraints   integervector &lt;NA&gt;             0
## tweedie_variance_power       numeric    -           1.5
## nthread                      integer    -             -
## nrounds                      integer    -             -
## feval                        untyped    -        &lt;NULL&gt;
## verbose                      integer    -             1
## print_every_n                integer    -             1
## early_stopping_rounds        integer    -        &lt;NULL&gt;
## maximize                     logical    -        &lt;NULL&gt;
## sample_type                 discrete    -       uniform
## normalize_type              discrete    -          tree
## rate_drop                    numeric    -             0
## skip_drop                    numeric    -             0
## scale_pos_weight             numeric    -             1
## refresh_leaf                 logical    -          TRUE
## feature_selector            discrete    -        cyclic
## top_k                        integer    -             0
## predictor                   discrete    - cpu_predictor
## updater                      untyped    -             -
## sketch_eps                   numeric    -          0.03
## one_drop                     logical    -         FALSE
## tree_method                 discrete    -          auto
## grow_policy                 discrete    -     depthwise
## max_leaves                   integer    -             0
## max_bin                      integer    -           256
## callbacks                    untyped    -              
##                                                      Constr Req Tunable Trafo
## booster                                gbtree,gblinear,dart   -    TRUE     -
## watchlist                                                 -   -   FALSE     -
## eta                                                  0 to 1   -    TRUE     -
## gamma                                              0 to Inf   -    TRUE     -
## max_depth                                          0 to Inf   -    TRUE     -
## min_child_weight                                   0 to Inf   -    TRUE     -
## subsample                                            0 to 1   -    TRUE     -
## colsample_bytree                                     0 to 1   -    TRUE     -
## colsample_bylevel                                    0 to 1   -    TRUE     -
## colsample_bynode                                     0 to 1   -    TRUE     -
## num_parallel_tree                                  1 to Inf   -    TRUE     -
## lambda                                             0 to Inf   -    TRUE     -
## lambda_bias                                        0 to Inf   -    TRUE     -
## alpha                                              0 to Inf   -    TRUE     -
## objective                                                 -   -   FALSE     -
## eval_metric                                               -   -   FALSE     -
## base_score                                      -Inf to Inf   -   FALSE     -
## max_delta_step                                     0 to Inf   -    TRUE     -
## missing                                         -Inf to Inf   -   FALSE     -
## monotone_constraints                                -1 to 1   -    TRUE     -
## tweedie_variance_power                               1 to 2   Y    TRUE     -
## nthread                                            1 to Inf   -   FALSE     -
## nrounds                                            1 to Inf   -    TRUE     -
## feval                                                     -   -   FALSE     -
## verbose                                              0 to 2   -   FALSE     -
## print_every_n                                      1 to Inf   Y   FALSE     -
## early_stopping_rounds                              1 to Inf   -   FALSE     -
## maximize                                                  -   -   FALSE     -
## sample_type                                uniform,weighted   Y    TRUE     -
## normalize_type                                  tree,forest   Y    TRUE     -
## rate_drop                                            0 to 1   Y    TRUE     -
## skip_drop                                            0 to 1   Y    TRUE     -
## scale_pos_weight                                -Inf to Inf   -    TRUE     -
## refresh_leaf                                              -   -    TRUE     -
## feature_selector       cyclic,shuffle,random,greedy,thrifty   -    TRUE     -
## top_k                                              0 to Inf   -    TRUE     -
## predictor                       cpu_predictor,gpu_predictor   -    TRUE     -
## updater                                                   -   -    TRUE     -
## sketch_eps                                           0 to 1   -    TRUE     -
## one_drop                                                  -   Y    TRUE     -
## tree_method                 auto,exact,approx,hist,gpu_hist   Y    TRUE     -
## grow_policy                             depthwise,lossguide   Y    TRUE     -
## max_leaves                                         0 to Inf   Y    TRUE     -
## max_bin                                            2 to Inf   Y    TRUE     -
## callbacks                                                 -   -   FALSE     -</code></pre>
<p>To build a well-performing predictive model many iterations are necessary. Therefore, in order to determine how good or bad one model predicts the target variable, performance evaluation needs to be conducted. A technique that will be used to help us in evaluating performance of our future machine learning models is called <strong>k-fold cross-validation technique.</strong> K-fold cross-validation evaluates a model by training a couple of models on subsets of the available input data and evaluating them on the complementary subset of the data. In this process, training data is split into k groups (i.e. folds) of approximately equal size. Then the model is fit on k−1 folds and the remaining fold is used in computation of the model performance. This procedure is repeated k times, where each time, a different fold is treated as the validation set (i.e. used in computation of the model performance). Thus, the final cross-validation k-fold estimate is computed by averaging the k test errors. The final output provided is an approximation of the error we may expect on unseen data.</p>
<p>The first model to pass to the k-fold cross validation will be built using default parameters. As default for number of iterations (<code>nrounds</code>) is zero, we will set it on 200.</p>
<div class="sourceCode" id="cb933"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb933-1"><a href="xgboost.html#cb933-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb933-2"><a href="xgboost.html#cb933-2"></a>ames_xgb &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</span>
<span id="cb933-3"><a href="xgboost.html#cb933-3"></a>  <span class="dt">data =</span> ames_x_train,      <span class="co"># matrix with train data without sale price</span></span>
<span id="cb933-4"><a href="xgboost.html#cb933-4"></a>  <span class="dt">label =</span> ames_y_train,     <span class="co"># numerical vector with sale price with train data </span></span>
<span id="cb933-5"><a href="xgboost.html#cb933-5"></a>  <span class="dt">nrounds =</span> <span class="dv">200</span>,            <span class="co"># number of iterations </span></span>
<span id="cb933-6"><a href="xgboost.html#cb933-6"></a>  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>, <span class="co"># parameter referring to the function to be me minimised (RMSE)</span></span>
<span id="cb933-7"><a href="xgboost.html#cb933-7"></a>  <span class="dt">nfold =</span> <span class="dv">10</span>,                <span class="co"># data is randomly partitioned into n-fold equal size subsamples.</span></span>
<span id="cb933-8"><a href="xgboost.html#cb933-8"></a>  <span class="dt">params =</span> <span class="kw">list</span>(            <span class="co"># defining the list of parameters</span></span>
<span id="cb933-9"><a href="xgboost.html#cb933-9"></a>    <span class="dt">eta =</span> <span class="fl">0.3</span>,              <span class="co"># learning rate </span></span>
<span id="cb933-10"><a href="xgboost.html#cb933-10"></a>    <span class="dt">max_depth =</span> <span class="dv">6</span>,          <span class="co"># maximal depth of tree</span></span>
<span id="cb933-11"><a href="xgboost.html#cb933-11"></a>    <span class="dt">min_child_weight =</span> <span class="dv">1</span>,   <span class="co"># minimum number of observations required in each terminal node</span></span>
<span id="cb933-12"><a href="xgboost.html#cb933-12"></a>    <span class="dt">subsample =</span> <span class="dv">1</span>,          <span class="co"># percent of training data to sample for each tree</span></span>
<span id="cb933-13"><a href="xgboost.html#cb933-13"></a>    <span class="dt">colsample_bytree =</span> <span class="dv">1</span>    <span class="co"># percent of columns to sample from for each tree</span></span>
<span id="cb933-14"><a href="xgboost.html#cb933-14"></a>    ),</span>
<span id="cb933-15"><a href="xgboost.html#cb933-15"></a>  <span class="dt">verbose =</span> <span class="dv">0</span>               <span class="co"># print the statistics during the process (1 or 0)</span></span>
<span id="cb933-16"><a href="xgboost.html#cb933-16"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb934"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb934-1"><a href="xgboost.html#cb934-1"></a>(eval&lt;-<span class="st"> </span>ames_xgb<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span></span>
<span id="cb934-2"><a href="xgboost.html#cb934-2"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(</span>
<span id="cb934-3"><a href="xgboost.html#cb934-3"></a>    <span class="dt">ntrees.train =</span> <span class="kw">which</span>(train_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(train_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb934-4"><a href="xgboost.html#cb934-4"></a>    <span class="dt">rmse.train   =</span> <span class="kw">min</span>(train_rmse_mean),</span>
<span id="cb934-5"><a href="xgboost.html#cb934-5"></a>    <span class="dt">ntrees.test  =</span> <span class="kw">which</span>(test_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(test_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb934-6"><a href="xgboost.html#cb934-6"></a>    <span class="dt">rmse.test   =</span> <span class="kw">min</span>(test_rmse_mean)))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["ntrees.train"],"name":[1],"type":["int"],"align":["right"]},{"label":["rmse.train"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["ntrees.test"],"name":[3],"type":["int"],"align":["right"]},{"label":["rmse.test"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"200","2":"596.9639","3":"180","4":"25324.12"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>After conducting the 10-fold cross validation, we sorted the output so that it shows us at what iteration (round) our model reached the lowest errors when fitted to the seen part of training data (596.9638916) and unseen part of the training data ().</p>
<p><em>Side note: unseen part of the training data (rmse.test) has nothing to do with test data from the initial split we did at the very beginNing of the chapter and named as <code>ames_test</code> . Here we talk about unseen data in the process of k-fold cross-validation.</em></p>
<p>Unsurprisingly, our model performed very well when fitted to the seen data, suggesting the RMSE being``. Here we see an evidence of overfitting. In other words, our model fits the training part of the training data very well (i.e. suggests low RMSE for train data), but is not generalizable, i.e. when confronted with unseen data, its predictions are not as good as for the trained part(i.e. suggests high RMSE for unseen, test data).</p>
<div class="sourceCode" id="cb935"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb935-1"><a href="xgboost.html#cb935-1"></a><span class="co"># Plot error vs number trees</span></span>
<span id="cb935-2"><a href="xgboost.html#cb935-2"></a>pe&lt;-<span class="kw">ggplot</span>(ames_xgb<span class="op">$</span>evaluation_log) <span class="op">+</span></span>
<span id="cb935-3"><a href="xgboost.html#cb935-3"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, train_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb935-4"><a href="xgboost.html#cb935-4"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, test_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb935-5"><a href="xgboost.html#cb935-5"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="dt">label=</span> <span class="st">&quot;Iteration (round)&quot;</span>)<span class="op">+</span></span>
<span id="cb935-6"><a href="xgboost.html#cb935-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="dt">label =</span> <span class="st">&quot;Root Mean Square Error&quot;</span>)<span class="op">+</span></span>
<span id="cb935-7"><a href="xgboost.html#cb935-7"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="dt">label =</span> <span class="st">&quot;10-fold Cross-validation&quot;</span>)</span>
<span id="cb935-8"><a href="xgboost.html#cb935-8"></a><span class="kw">ggplotly</span>(pe)</span></code></pre></div>
<div id="htmlwidget-79f534a0b997fe28d7c5" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-79f534a0b997fe28d7c5">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[140594.590625,101286.6835938,73807.3007813,54337.7445311,40785.1953126,31380.5316406,24898.7707032,20446.4484375,17473.6289063,15400.1717773,13967.5167967,12978.5580078,12269.1209962,11734.3819337,11249.5094727,10852.1914063,10512.8968752,10225.8338867,9994.7424803,9751.919629,9519.4712889,9315.4802735,9117.538086,8910.4478515,8750.9584961,8593.2000976,8448.3762695,8278.2052734,8106.6848144,7939.721338,7771.9760743,7613.2281251,7462.256543,7337.0042479,7215.4797363,7085.5856446,6910.9421386,6796.204248,6677.9722168,6574.582422,6457.234082,6338.7857423,6233.3975586,6160.247168,6055.0296873,5962.9359376,5841.8116211,5740.5356933,5645.0677247,5540.2311523,5444.6581543,5364.8388671,5288.4354004,5201.0839356,5128.1607424,5069.8741212,4996.052246,4919.8977541,4822.1878419,4739.639502,4670.4357912,4613.490381,4552.1995606,4489.4087402,4403.4344971,4304.1592774,4231.0443604,4165.1486327,4090.1139649,4008.9815674,3944.6740722,3891.4822998,3829.4938967,3761.4835694,3726.1803956,3671.3133056,3630.294043,3580.0728272,3520.8025146,3471.8133545,3411.6533203,3349.0891359,3293.5902344,3253.8237304,3192.9784425,3148.837793,3108.5680663,3052.2456055,3007.3829591,2954.0199219,2911.3964357,2867.2989746,2819.4180419,2776.9638672,2727.8215821,2693.3880859,2663.0776368,2617.4836671,2573.2193604,2532.8037598,2499.6313477,2462.9624758,2432.0398927,2401.1086183,2370.060913,2341.9923952,2304.8312012,2267.9301147,2229.6168213,2202.0569336,2165.8003297,2138.5862183,2106.6027587,2074.3712403,2047.8174803,2014.0364867,1993.7470215,1967.0413207,1941.7843384,1918.6678466,1894.6834959,1864.8548705,1835.1144897,1795.4808105,1763.295105,1738.6042481,1710.9897094,1687.843225,1661.0684571,1631.4689576,1613.6443239,1588.7129395,1563.7790406,1531.9783203,1506.9308227,1487.3664551,1460.6477905,1441.3761963,1419.9145997,1397.0802979,1378.6616333,1361.2244751,1344.8429809,1329.2415161,1308.6594605,1282.1494387,1259.7858642,1240.207361,1221.0285522,1205.4603515,1190.8674256,1177.3331053,1162.7586182,1151.5120607,1140.4367431,1125.1754335,1107.6707398,1092.8359498,1075.6618226,1061.7395019,1044.5710634,1027.7472717,1014.5498169,1002.2487488,983.6441711,969.4413634,952.291034,936.2260744,922.0340455,907.7742737,898.2338135,886.1614868,872.0424926,862.0117492,847.1053711,835.2450501,826.1704101,817.2422851,807.9753052,794.2546937,783.8483949,773.7635986,760.4451233,749.0615845,738.7089355,728.3018678,718.2620545,709.0963682,697.2377866,688.6634523,678.9270814,668.306317,658.0195254,649.9507203,639.8951047,631.6645751,622.9106293,612.3728973,602.3153596,596.9638916],"text":["iter:   1<br />train_rmse_mean: 140594.5906","iter:   2<br />train_rmse_mean: 101286.6836","iter:   3<br />train_rmse_mean:  73807.3008","iter:   4<br />train_rmse_mean:  54337.7445","iter:   5<br />train_rmse_mean:  40785.1953","iter:   6<br />train_rmse_mean:  31380.5316","iter:   7<br />train_rmse_mean:  24898.7707","iter:   8<br />train_rmse_mean:  20446.4484","iter:   9<br />train_rmse_mean:  17473.6289","iter:  10<br />train_rmse_mean:  15400.1718","iter:  11<br />train_rmse_mean:  13967.5168","iter:  12<br />train_rmse_mean:  12978.5580","iter:  13<br />train_rmse_mean:  12269.1210","iter:  14<br />train_rmse_mean:  11734.3819","iter:  15<br />train_rmse_mean:  11249.5095","iter:  16<br />train_rmse_mean:  10852.1914","iter:  17<br />train_rmse_mean:  10512.8969","iter:  18<br />train_rmse_mean:  10225.8339","iter:  19<br />train_rmse_mean:   9994.7425","iter:  20<br />train_rmse_mean:   9751.9196","iter:  21<br />train_rmse_mean:   9519.4713","iter:  22<br />train_rmse_mean:   9315.4803","iter:  23<br />train_rmse_mean:   9117.5381","iter:  24<br />train_rmse_mean:   8910.4479","iter:  25<br />train_rmse_mean:   8750.9585","iter:  26<br />train_rmse_mean:   8593.2001","iter:  27<br />train_rmse_mean:   8448.3763","iter:  28<br />train_rmse_mean:   8278.2053","iter:  29<br />train_rmse_mean:   8106.6848","iter:  30<br />train_rmse_mean:   7939.7213","iter:  31<br />train_rmse_mean:   7771.9761","iter:  32<br />train_rmse_mean:   7613.2281","iter:  33<br />train_rmse_mean:   7462.2565","iter:  34<br />train_rmse_mean:   7337.0042","iter:  35<br />train_rmse_mean:   7215.4797","iter:  36<br />train_rmse_mean:   7085.5856","iter:  37<br />train_rmse_mean:   6910.9421","iter:  38<br />train_rmse_mean:   6796.2042","iter:  39<br />train_rmse_mean:   6677.9722","iter:  40<br />train_rmse_mean:   6574.5824","iter:  41<br />train_rmse_mean:   6457.2341","iter:  42<br />train_rmse_mean:   6338.7857","iter:  43<br />train_rmse_mean:   6233.3976","iter:  44<br />train_rmse_mean:   6160.2472","iter:  45<br />train_rmse_mean:   6055.0297","iter:  46<br />train_rmse_mean:   5962.9359","iter:  47<br />train_rmse_mean:   5841.8116","iter:  48<br />train_rmse_mean:   5740.5357","iter:  49<br />train_rmse_mean:   5645.0677","iter:  50<br />train_rmse_mean:   5540.2312","iter:  51<br />train_rmse_mean:   5444.6582","iter:  52<br />train_rmse_mean:   5364.8389","iter:  53<br />train_rmse_mean:   5288.4354","iter:  54<br />train_rmse_mean:   5201.0839","iter:  55<br />train_rmse_mean:   5128.1607","iter:  56<br />train_rmse_mean:   5069.8741","iter:  57<br />train_rmse_mean:   4996.0522","iter:  58<br />train_rmse_mean:   4919.8978","iter:  59<br />train_rmse_mean:   4822.1878","iter:  60<br />train_rmse_mean:   4739.6395","iter:  61<br />train_rmse_mean:   4670.4358","iter:  62<br />train_rmse_mean:   4613.4904","iter:  63<br />train_rmse_mean:   4552.1996","iter:  64<br />train_rmse_mean:   4489.4087","iter:  65<br />train_rmse_mean:   4403.4345","iter:  66<br />train_rmse_mean:   4304.1593","iter:  67<br />train_rmse_mean:   4231.0444","iter:  68<br />train_rmse_mean:   4165.1486","iter:  69<br />train_rmse_mean:   4090.1140","iter:  70<br />train_rmse_mean:   4008.9816","iter:  71<br />train_rmse_mean:   3944.6741","iter:  72<br />train_rmse_mean:   3891.4823","iter:  73<br />train_rmse_mean:   3829.4939","iter:  74<br />train_rmse_mean:   3761.4836","iter:  75<br />train_rmse_mean:   3726.1804","iter:  76<br />train_rmse_mean:   3671.3133","iter:  77<br />train_rmse_mean:   3630.2940","iter:  78<br />train_rmse_mean:   3580.0728","iter:  79<br />train_rmse_mean:   3520.8025","iter:  80<br />train_rmse_mean:   3471.8134","iter:  81<br />train_rmse_mean:   3411.6533","iter:  82<br />train_rmse_mean:   3349.0891","iter:  83<br />train_rmse_mean:   3293.5902","iter:  84<br />train_rmse_mean:   3253.8237","iter:  85<br />train_rmse_mean:   3192.9784","iter:  86<br />train_rmse_mean:   3148.8378","iter:  87<br />train_rmse_mean:   3108.5681","iter:  88<br />train_rmse_mean:   3052.2456","iter:  89<br />train_rmse_mean:   3007.3830","iter:  90<br />train_rmse_mean:   2954.0199","iter:  91<br />train_rmse_mean:   2911.3964","iter:  92<br />train_rmse_mean:   2867.2990","iter:  93<br />train_rmse_mean:   2819.4180","iter:  94<br />train_rmse_mean:   2776.9639","iter:  95<br />train_rmse_mean:   2727.8216","iter:  96<br />train_rmse_mean:   2693.3881","iter:  97<br />train_rmse_mean:   2663.0776","iter:  98<br />train_rmse_mean:   2617.4837","iter:  99<br />train_rmse_mean:   2573.2194","iter: 100<br />train_rmse_mean:   2532.8038","iter: 101<br />train_rmse_mean:   2499.6313","iter: 102<br />train_rmse_mean:   2462.9625","iter: 103<br />train_rmse_mean:   2432.0399","iter: 104<br />train_rmse_mean:   2401.1086","iter: 105<br />train_rmse_mean:   2370.0609","iter: 106<br />train_rmse_mean:   2341.9924","iter: 107<br />train_rmse_mean:   2304.8312","iter: 108<br />train_rmse_mean:   2267.9301","iter: 109<br />train_rmse_mean:   2229.6168","iter: 110<br />train_rmse_mean:   2202.0569","iter: 111<br />train_rmse_mean:   2165.8003","iter: 112<br />train_rmse_mean:   2138.5862","iter: 113<br />train_rmse_mean:   2106.6028","iter: 114<br />train_rmse_mean:   2074.3712","iter: 115<br />train_rmse_mean:   2047.8175","iter: 116<br />train_rmse_mean:   2014.0365","iter: 117<br />train_rmse_mean:   1993.7470","iter: 118<br />train_rmse_mean:   1967.0413","iter: 119<br />train_rmse_mean:   1941.7843","iter: 120<br />train_rmse_mean:   1918.6678","iter: 121<br />train_rmse_mean:   1894.6835","iter: 122<br />train_rmse_mean:   1864.8549","iter: 123<br />train_rmse_mean:   1835.1145","iter: 124<br />train_rmse_mean:   1795.4808","iter: 125<br />train_rmse_mean:   1763.2951","iter: 126<br />train_rmse_mean:   1738.6042","iter: 127<br />train_rmse_mean:   1710.9897","iter: 128<br />train_rmse_mean:   1687.8432","iter: 129<br />train_rmse_mean:   1661.0685","iter: 130<br />train_rmse_mean:   1631.4690","iter: 131<br />train_rmse_mean:   1613.6443","iter: 132<br />train_rmse_mean:   1588.7129","iter: 133<br />train_rmse_mean:   1563.7790","iter: 134<br />train_rmse_mean:   1531.9783","iter: 135<br />train_rmse_mean:   1506.9308","iter: 136<br />train_rmse_mean:   1487.3665","iter: 137<br />train_rmse_mean:   1460.6478","iter: 138<br />train_rmse_mean:   1441.3762","iter: 139<br />train_rmse_mean:   1419.9146","iter: 140<br />train_rmse_mean:   1397.0803","iter: 141<br />train_rmse_mean:   1378.6616","iter: 142<br />train_rmse_mean:   1361.2245","iter: 143<br />train_rmse_mean:   1344.8430","iter: 144<br />train_rmse_mean:   1329.2415","iter: 145<br />train_rmse_mean:   1308.6595","iter: 146<br />train_rmse_mean:   1282.1494","iter: 147<br />train_rmse_mean:   1259.7859","iter: 148<br />train_rmse_mean:   1240.2074","iter: 149<br />train_rmse_mean:   1221.0286","iter: 150<br />train_rmse_mean:   1205.4604","iter: 151<br />train_rmse_mean:   1190.8674","iter: 152<br />train_rmse_mean:   1177.3331","iter: 153<br />train_rmse_mean:   1162.7586","iter: 154<br />train_rmse_mean:   1151.5121","iter: 155<br />train_rmse_mean:   1140.4367","iter: 156<br />train_rmse_mean:   1125.1754","iter: 157<br />train_rmse_mean:   1107.6707","iter: 158<br />train_rmse_mean:   1092.8359","iter: 159<br />train_rmse_mean:   1075.6618","iter: 160<br />train_rmse_mean:   1061.7395","iter: 161<br />train_rmse_mean:   1044.5711","iter: 162<br />train_rmse_mean:   1027.7473","iter: 163<br />train_rmse_mean:   1014.5498","iter: 164<br />train_rmse_mean:   1002.2487","iter: 165<br />train_rmse_mean:    983.6442","iter: 166<br />train_rmse_mean:    969.4414","iter: 167<br />train_rmse_mean:    952.2910","iter: 168<br />train_rmse_mean:    936.2261","iter: 169<br />train_rmse_mean:    922.0340","iter: 170<br />train_rmse_mean:    907.7743","iter: 171<br />train_rmse_mean:    898.2338","iter: 172<br />train_rmse_mean:    886.1615","iter: 173<br />train_rmse_mean:    872.0425","iter: 174<br />train_rmse_mean:    862.0117","iter: 175<br />train_rmse_mean:    847.1054","iter: 176<br />train_rmse_mean:    835.2451","iter: 177<br />train_rmse_mean:    826.1704","iter: 178<br />train_rmse_mean:    817.2423","iter: 179<br />train_rmse_mean:    807.9753","iter: 180<br />train_rmse_mean:    794.2547","iter: 181<br />train_rmse_mean:    783.8484","iter: 182<br />train_rmse_mean:    773.7636","iter: 183<br />train_rmse_mean:    760.4451","iter: 184<br />train_rmse_mean:    749.0616","iter: 185<br />train_rmse_mean:    738.7089","iter: 186<br />train_rmse_mean:    728.3019","iter: 187<br />train_rmse_mean:    718.2621","iter: 188<br />train_rmse_mean:    709.0964","iter: 189<br />train_rmse_mean:    697.2378","iter: 190<br />train_rmse_mean:    688.6635","iter: 191<br />train_rmse_mean:    678.9271","iter: 192<br />train_rmse_mean:    668.3063","iter: 193<br />train_rmse_mean:    658.0195","iter: 194<br />train_rmse_mean:    649.9507","iter: 195<br />train_rmse_mean:    639.8951","iter: 196<br />train_rmse_mean:    631.6646","iter: 197<br />train_rmse_mean:    622.9106","iter: 198<br />train_rmse_mean:    612.3729","iter: 199<br />train_rmse_mean:    602.3154","iter: 200<br />train_rmse_mean:    596.9639"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[140830.0945313,102345.2226562,75785.7105468,57672.9675781,45915.3257813,38311.39375,33616.8185548,30879.5785156,29185.3003906,28172.7214846,27607.0398438,27279.8898437,27061.8242187,26881.2220704,26793.7443359,26636.1140624,26526.2494139,26461.0824218,26375.2812501,26277.4505858,26193.3636719,26127.6955078,26053.3458984,26004.0558594,25962.8048828,25943.5232421,25885.6173828,25876.8294922,25862.4753904,25857.2539061,25851.8714845,25804.6177732,25744.5044921,25729.4595703,25712.5003905,25687.5537109,25652.7566407,25644.3392579,25622.3544922,25614.5439453,25587.1652344,25588.3921876,25553.5648438,25563.8384766,25565.5972657,25558.8947266,25559.4994141,25530.3476561,25533.034961,25538.9935546,25521.7064452,25510.8695313,25501.2830079,25491.6109376,25494.9962891,25482.5376954,25471.4611329,25468.3007812,25475.3955079,25477.7287109,25466.0451171,25463.3246095,25463.7367187,25464.5187501,25450.4019532,25440.867578,25428.3396484,25427.5576172,25416.8123046,25423.8160158,25421.641797,25434.1546876,25437.7244142,25419.9599609,25420.5503907,25410.6675779,25407.8669923,25412.2048828,25400.1412109,25398.93125,25388.6587889,25387.0603517,25393.45625,25380.0558592,25385.3871094,25393.2226561,25389.4339843,25386.0212892,25382.3292969,25379.0605469,25377.0238278,25374.8541015,25370.3863281,25361.2626954,25357.2736328,25356.1662108,25359.5527343,25354.3248048,25350.5216797,25350.6001953,25355.1404297,25350.8302734,25349.5013673,25350.7478515,25349.5787109,25351.8277344,25348.1703127,25344.3224608,25349.6771485,25346.9451173,25340.8808595,25343.3115233,25344.7369141,25345.3152343,25347.8001953,25343.7220703,25344.9960937,25347.456836,25347.4773438,25347.0517577,25348.3919922,25350.945117,25347.727539,25349.5515626,25345.4984375,25342.9306642,25344.1294922,25343.3720703,25342.9529298,25336.338672,25337.2623048,25336.9714843,25337.049414,25335.7666016,25333.378125,25332.696875,25332.6232421,25330.8492188,25330.9355469,25326.6517578,25326.8832031,25326.2710937,25327.1251953,25326.4394531,25326.8101563,25324.433203,25327.0123048,25327.351953,25328.8867188,25330.1882814,25331.8199218,25333.0689451,25332.188086,25331.2548828,25330.8003906,25334.4595702,25334.1423829,25333.5736329,25334.2326172,25330.0507813,25328.7509766,25328.5701173,25328.2814453,25330.2099609,25329.8847658,25329.3027346,25331.875,25330.8146484,25329.6597657,25329.7882812,25330.2810547,25330.6984374,25329.5767577,25329.8841797,25329.5527342,25327.6300781,25325.6214844,25324.7638672,25326.0652345,25324.1220704,25325.8195313,25325.7355469,25325.7843751,25326.0160157,25326.1857421,25326.2892577,25327.3466797,25328.4289063,25329.7214844,25327.9976562,25327.6837889,25329.2517579,25330.9570314,25330.2984375,25328.8734374,25328.0291016,25328.3849611,25328.9683594,25326.9433596,25327.2771484],"text":["iter:   1<br />test_rmse_mean: 140830.09","iter:   2<br />test_rmse_mean: 102345.22","iter:   3<br />test_rmse_mean:  75785.71","iter:   4<br />test_rmse_mean:  57672.97","iter:   5<br />test_rmse_mean:  45915.33","iter:   6<br />test_rmse_mean:  38311.39","iter:   7<br />test_rmse_mean:  33616.82","iter:   8<br />test_rmse_mean:  30879.58","iter:   9<br />test_rmse_mean:  29185.30","iter:  10<br />test_rmse_mean:  28172.72","iter:  11<br />test_rmse_mean:  27607.04","iter:  12<br />test_rmse_mean:  27279.89","iter:  13<br />test_rmse_mean:  27061.82","iter:  14<br />test_rmse_mean:  26881.22","iter:  15<br />test_rmse_mean:  26793.74","iter:  16<br />test_rmse_mean:  26636.11","iter:  17<br />test_rmse_mean:  26526.25","iter:  18<br />test_rmse_mean:  26461.08","iter:  19<br />test_rmse_mean:  26375.28","iter:  20<br />test_rmse_mean:  26277.45","iter:  21<br />test_rmse_mean:  26193.36","iter:  22<br />test_rmse_mean:  26127.70","iter:  23<br />test_rmse_mean:  26053.35","iter:  24<br />test_rmse_mean:  26004.06","iter:  25<br />test_rmse_mean:  25962.80","iter:  26<br />test_rmse_mean:  25943.52","iter:  27<br />test_rmse_mean:  25885.62","iter:  28<br />test_rmse_mean:  25876.83","iter:  29<br />test_rmse_mean:  25862.48","iter:  30<br />test_rmse_mean:  25857.25","iter:  31<br />test_rmse_mean:  25851.87","iter:  32<br />test_rmse_mean:  25804.62","iter:  33<br />test_rmse_mean:  25744.50","iter:  34<br />test_rmse_mean:  25729.46","iter:  35<br />test_rmse_mean:  25712.50","iter:  36<br />test_rmse_mean:  25687.55","iter:  37<br />test_rmse_mean:  25652.76","iter:  38<br />test_rmse_mean:  25644.34","iter:  39<br />test_rmse_mean:  25622.35","iter:  40<br />test_rmse_mean:  25614.54","iter:  41<br />test_rmse_mean:  25587.17","iter:  42<br />test_rmse_mean:  25588.39","iter:  43<br />test_rmse_mean:  25553.56","iter:  44<br />test_rmse_mean:  25563.84","iter:  45<br />test_rmse_mean:  25565.60","iter:  46<br />test_rmse_mean:  25558.89","iter:  47<br />test_rmse_mean:  25559.50","iter:  48<br />test_rmse_mean:  25530.35","iter:  49<br />test_rmse_mean:  25533.03","iter:  50<br />test_rmse_mean:  25538.99","iter:  51<br />test_rmse_mean:  25521.71","iter:  52<br />test_rmse_mean:  25510.87","iter:  53<br />test_rmse_mean:  25501.28","iter:  54<br />test_rmse_mean:  25491.61","iter:  55<br />test_rmse_mean:  25495.00","iter:  56<br />test_rmse_mean:  25482.54","iter:  57<br />test_rmse_mean:  25471.46","iter:  58<br />test_rmse_mean:  25468.30","iter:  59<br />test_rmse_mean:  25475.40","iter:  60<br />test_rmse_mean:  25477.73","iter:  61<br />test_rmse_mean:  25466.05","iter:  62<br />test_rmse_mean:  25463.32","iter:  63<br />test_rmse_mean:  25463.74","iter:  64<br />test_rmse_mean:  25464.52","iter:  65<br />test_rmse_mean:  25450.40","iter:  66<br />test_rmse_mean:  25440.87","iter:  67<br />test_rmse_mean:  25428.34","iter:  68<br />test_rmse_mean:  25427.56","iter:  69<br />test_rmse_mean:  25416.81","iter:  70<br />test_rmse_mean:  25423.82","iter:  71<br />test_rmse_mean:  25421.64","iter:  72<br />test_rmse_mean:  25434.15","iter:  73<br />test_rmse_mean:  25437.72","iter:  74<br />test_rmse_mean:  25419.96","iter:  75<br />test_rmse_mean:  25420.55","iter:  76<br />test_rmse_mean:  25410.67","iter:  77<br />test_rmse_mean:  25407.87","iter:  78<br />test_rmse_mean:  25412.20","iter:  79<br />test_rmse_mean:  25400.14","iter:  80<br />test_rmse_mean:  25398.93","iter:  81<br />test_rmse_mean:  25388.66","iter:  82<br />test_rmse_mean:  25387.06","iter:  83<br />test_rmse_mean:  25393.46","iter:  84<br />test_rmse_mean:  25380.06","iter:  85<br />test_rmse_mean:  25385.39","iter:  86<br />test_rmse_mean:  25393.22","iter:  87<br />test_rmse_mean:  25389.43","iter:  88<br />test_rmse_mean:  25386.02","iter:  89<br />test_rmse_mean:  25382.33","iter:  90<br />test_rmse_mean:  25379.06","iter:  91<br />test_rmse_mean:  25377.02","iter:  92<br />test_rmse_mean:  25374.85","iter:  93<br />test_rmse_mean:  25370.39","iter:  94<br />test_rmse_mean:  25361.26","iter:  95<br />test_rmse_mean:  25357.27","iter:  96<br />test_rmse_mean:  25356.17","iter:  97<br />test_rmse_mean:  25359.55","iter:  98<br />test_rmse_mean:  25354.32","iter:  99<br />test_rmse_mean:  25350.52","iter: 100<br />test_rmse_mean:  25350.60","iter: 101<br />test_rmse_mean:  25355.14","iter: 102<br />test_rmse_mean:  25350.83","iter: 103<br />test_rmse_mean:  25349.50","iter: 104<br />test_rmse_mean:  25350.75","iter: 105<br />test_rmse_mean:  25349.58","iter: 106<br />test_rmse_mean:  25351.83","iter: 107<br />test_rmse_mean:  25348.17","iter: 108<br />test_rmse_mean:  25344.32","iter: 109<br />test_rmse_mean:  25349.68","iter: 110<br />test_rmse_mean:  25346.95","iter: 111<br />test_rmse_mean:  25340.88","iter: 112<br />test_rmse_mean:  25343.31","iter: 113<br />test_rmse_mean:  25344.74","iter: 114<br />test_rmse_mean:  25345.32","iter: 115<br />test_rmse_mean:  25347.80","iter: 116<br />test_rmse_mean:  25343.72","iter: 117<br />test_rmse_mean:  25345.00","iter: 118<br />test_rmse_mean:  25347.46","iter: 119<br />test_rmse_mean:  25347.48","iter: 120<br />test_rmse_mean:  25347.05","iter: 121<br />test_rmse_mean:  25348.39","iter: 122<br />test_rmse_mean:  25350.95","iter: 123<br />test_rmse_mean:  25347.73","iter: 124<br />test_rmse_mean:  25349.55","iter: 125<br />test_rmse_mean:  25345.50","iter: 126<br />test_rmse_mean:  25342.93","iter: 127<br />test_rmse_mean:  25344.13","iter: 128<br />test_rmse_mean:  25343.37","iter: 129<br />test_rmse_mean:  25342.95","iter: 130<br />test_rmse_mean:  25336.34","iter: 131<br />test_rmse_mean:  25337.26","iter: 132<br />test_rmse_mean:  25336.97","iter: 133<br />test_rmse_mean:  25337.05","iter: 134<br />test_rmse_mean:  25335.77","iter: 135<br />test_rmse_mean:  25333.38","iter: 136<br />test_rmse_mean:  25332.70","iter: 137<br />test_rmse_mean:  25332.62","iter: 138<br />test_rmse_mean:  25330.85","iter: 139<br />test_rmse_mean:  25330.94","iter: 140<br />test_rmse_mean:  25326.65","iter: 141<br />test_rmse_mean:  25326.88","iter: 142<br />test_rmse_mean:  25326.27","iter: 143<br />test_rmse_mean:  25327.13","iter: 144<br />test_rmse_mean:  25326.44","iter: 145<br />test_rmse_mean:  25326.81","iter: 146<br />test_rmse_mean:  25324.43","iter: 147<br />test_rmse_mean:  25327.01","iter: 148<br />test_rmse_mean:  25327.35","iter: 149<br />test_rmse_mean:  25328.89","iter: 150<br />test_rmse_mean:  25330.19","iter: 151<br />test_rmse_mean:  25331.82","iter: 152<br />test_rmse_mean:  25333.07","iter: 153<br />test_rmse_mean:  25332.19","iter: 154<br />test_rmse_mean:  25331.25","iter: 155<br />test_rmse_mean:  25330.80","iter: 156<br />test_rmse_mean:  25334.46","iter: 157<br />test_rmse_mean:  25334.14","iter: 158<br />test_rmse_mean:  25333.57","iter: 159<br />test_rmse_mean:  25334.23","iter: 160<br />test_rmse_mean:  25330.05","iter: 161<br />test_rmse_mean:  25328.75","iter: 162<br />test_rmse_mean:  25328.57","iter: 163<br />test_rmse_mean:  25328.28","iter: 164<br />test_rmse_mean:  25330.21","iter: 165<br />test_rmse_mean:  25329.88","iter: 166<br />test_rmse_mean:  25329.30","iter: 167<br />test_rmse_mean:  25331.88","iter: 168<br />test_rmse_mean:  25330.81","iter: 169<br />test_rmse_mean:  25329.66","iter: 170<br />test_rmse_mean:  25329.79","iter: 171<br />test_rmse_mean:  25330.28","iter: 172<br />test_rmse_mean:  25330.70","iter: 173<br />test_rmse_mean:  25329.58","iter: 174<br />test_rmse_mean:  25329.88","iter: 175<br />test_rmse_mean:  25329.55","iter: 176<br />test_rmse_mean:  25327.63","iter: 177<br />test_rmse_mean:  25325.62","iter: 178<br />test_rmse_mean:  25324.76","iter: 179<br />test_rmse_mean:  25326.07","iter: 180<br />test_rmse_mean:  25324.12","iter: 181<br />test_rmse_mean:  25325.82","iter: 182<br />test_rmse_mean:  25325.74","iter: 183<br />test_rmse_mean:  25325.78","iter: 184<br />test_rmse_mean:  25326.02","iter: 185<br />test_rmse_mean:  25326.19","iter: 186<br />test_rmse_mean:  25326.29","iter: 187<br />test_rmse_mean:  25327.35","iter: 188<br />test_rmse_mean:  25328.43","iter: 189<br />test_rmse_mean:  25329.72","iter: 190<br />test_rmse_mean:  25328.00","iter: 191<br />test_rmse_mean:  25327.68","iter: 192<br />test_rmse_mean:  25329.25","iter: 193<br />test_rmse_mean:  25330.96","iter: 194<br />test_rmse_mean:  25330.30","iter: 195<br />test_rmse_mean:  25328.87","iter: 196<br />test_rmse_mean:  25328.03","iter: 197<br />test_rmse_mean:  25328.38","iter: 198<br />test_rmse_mean:  25328.97","iter: 199<br />test_rmse_mean:  25326.94","iter: 200<br />test_rmse_mean:  25327.28"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"10-fold Cross-validation","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-8.95,209.95],"tickmode":"array","ticktext":["0","50","100","150","200"],"tickvals":[0,50,100,150,200],"categoryorder":"array","categoryarray":["0","50","100","150","200"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Iteration (round)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-6414.692640385,147841.751063285],"tickmode":"array","ticktext":["0","50000","100000"],"tickvals":[0,50000,100000],"categoryorder":"array","categoryarray":["0","50000","100000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Root Mean Square Error","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"87c3a958e2":{"x":{},"y":{},"type":"scatter"},"87c18ec64bf":{"x":{},"y":{}}},"cur_data":"87c3a958e2","visdat":{"87c3a958e2":["function (y) ","x"],"87c18ec64bf":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The gap between the blue and the red line in the graph above depicts the performance difference of our model when fitted to the seen data (<code>eval$rmse.train</code>) and when fitted to the unseen data (<code>eval$rmse.test</code>).
Our goal is to make our model perform with unseen data as good as possible, i.e. to minimize RMSE as much as possible.</p>
<p>To pursue that goal, parameters, that we discussed earlier, need to be as optimally tuned as possible.</p>
<p>Therefore, some parameters should be adapted. For the following cross-validation process, we will slightly adapt parameters:</p>
<ol style="list-style-type: decimal">
<li>Decrease the learning rate <code>eta</code> from 0.3 to 0.03</li>
<li>Set <code>early_stopping_rounds</code> at 50</li>
<li>Reduce maximum tree depth <code>max_depth</code> to 3</li>
<li>Increase minimum number of observations required in each terminal node <code>min_child_weight</code> to 3</li>
<li>Reduce <code>subsample</code> to 0.5</li>
<li><code>colsample_bytree</code> reduced to 0.5</li>
</ol>
<div class="sourceCode" id="cb936"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb936-1"><a href="xgboost.html#cb936-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb936-2"><a href="xgboost.html#cb936-2"></a>ames_xgb1 &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</span>
<span id="cb936-3"><a href="xgboost.html#cb936-3"></a>  <span class="dt">data =</span> ames_x_train,           <span class="co"># matrix with train data without sale price</span></span>
<span id="cb936-4"><a href="xgboost.html#cb936-4"></a>  <span class="dt">label =</span> ames_y_train,          <span class="co"># numerical vector with sale price with train data </span></span>
<span id="cb936-5"><a href="xgboost.html#cb936-5"></a>  <span class="dt">nrounds =</span> <span class="dv">2301</span>,                <span class="co"># number of iterations </span></span>
<span id="cb936-6"><a href="xgboost.html#cb936-6"></a>  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,      <span class="co"># indicating regression model</span></span>
<span id="cb936-7"><a href="xgboost.html#cb936-7"></a>  <span class="dt">early_stopping_rounds =</span> <span class="dv">50</span>,    <span class="co"># stopping the training model as soon as evaluation metric (RMSE) does not improve for a given number of rounds</span></span>
<span id="cb936-8"><a href="xgboost.html#cb936-8"></a>  <span class="dt">nfold =</span> <span class="dv">10</span>,                     <span class="co"># data is randomly partitioned into nfold equal size subsamples</span></span>
<span id="cb936-9"><a href="xgboost.html#cb936-9"></a>  <span class="dt">params =</span> <span class="kw">list</span>(</span>
<span id="cb936-10"><a href="xgboost.html#cb936-10"></a>    <span class="dt">eta =</span> <span class="fl">0.03</span>,                  <span class="co"># learning rate </span></span>
<span id="cb936-11"><a href="xgboost.html#cb936-11"></a>    <span class="dt">max_depth =</span> <span class="dv">3</span>,               <span class="co"># maximal depth of tree</span></span>
<span id="cb936-12"><a href="xgboost.html#cb936-12"></a>    <span class="dt">min_child_weight =</span> <span class="dv">3</span>,        <span class="co"># minimum number of observations required in each terminal node</span></span>
<span id="cb936-13"><a href="xgboost.html#cb936-13"></a>    <span class="dt">subsample =</span> <span class="fl">0.5</span>,             <span class="co"># percent of training data to sample for each tree</span></span>
<span id="cb936-14"><a href="xgboost.html#cb936-14"></a>    <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>       <span class="co"># percent of columns to sample from for each tree</span></span>
<span id="cb936-15"><a href="xgboost.html#cb936-15"></a>    ),</span>
<span id="cb936-16"><a href="xgboost.html#cb936-16"></a>  <span class="dt">verbose =</span> <span class="dv">0</span></span>
<span id="cb936-17"><a href="xgboost.html#cb936-17"></a>) </span>
<span id="cb936-18"><a href="xgboost.html#cb936-18"></a></span>
<span id="cb936-19"><a href="xgboost.html#cb936-19"></a><span class="co"># Checking results</span></span>
<span id="cb936-20"><a href="xgboost.html#cb936-20"></a>(eval1&lt;-ames_xgb1<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span></span>
<span id="cb936-21"><a href="xgboost.html#cb936-21"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(</span>
<span id="cb936-22"><a href="xgboost.html#cb936-22"></a>    <span class="dt">ntrees.train =</span> <span class="kw">which</span>(train_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(train_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb936-23"><a href="xgboost.html#cb936-23"></a>    <span class="dt">rmse.train   =</span> <span class="kw">min</span>(train_rmse_mean),</span>
<span id="cb936-24"><a href="xgboost.html#cb936-24"></a>    <span class="dt">ntrees.test  =</span> <span class="kw">which</span>(test_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(test_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb936-25"><a href="xgboost.html#cb936-25"></a>    <span class="dt">rmse.test   =</span> <span class="kw">min</span>(test_rmse_mean),))</span>
<span id="cb936-26"><a href="xgboost.html#cb936-26"></a></span>
<span id="cb936-27"><a href="xgboost.html#cb936-27"></a><span class="co"># Plot error vs number trees</span></span>
<span id="cb936-28"><a href="xgboost.html#cb936-28"></a>pr1 &lt;-<span class="kw">ggplot</span>(ames_xgb1<span class="op">$</span>evaluation_log) <span class="op">+</span></span>
<span id="cb936-29"><a href="xgboost.html#cb936-29"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, train_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb936-30"><a href="xgboost.html#cb936-30"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, test_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb936-31"><a href="xgboost.html#cb936-31"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="dt">label =</span> <span class="st">&quot;10-fold Cross-validation&quot;</span>)</span>
<span id="cb936-32"><a href="xgboost.html#cb936-32"></a><span class="kw">ggplotly</span>(pr1)</span></code></pre></div>
<p>We can see that the training error increased. However, the error on unseen data reaches a minimum RMSE of with iterations. With simple adaptation of our parameters we managed to decrease it to some extent. At this point it should be clear that it would take incredible effort to manually compute those errors for each possible combination of parameters that would potentially decrease the error further. Luckily, there are more elegant, automated solution for it. We can create a hyperparameter search grid along with columns to dump results in. Each row of the grid contains a combination of parameters we would like to model:</p>
<div class="sourceCode" id="cb937"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb937-1"><a href="xgboost.html#cb937-1"></a><span class="co"># Hyperparameter grid</span></span>
<span id="cb937-2"><a href="xgboost.html#cb937-2"></a>hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</span>
<span id="cb937-3"><a href="xgboost.html#cb937-3"></a>  <span class="dt">eta =</span> <span class="kw">c</span>(.<span class="dv">01</span>,<span class="fl">0.3</span>),</span>
<span id="cb937-4"><a href="xgboost.html#cb937-4"></a>  <span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>), </span>
<span id="cb937-5"><a href="xgboost.html#cb937-5"></a>  <span class="dt">min_child_weight =</span> <span class="dv">3</span>,</span>
<span id="cb937-6"><a href="xgboost.html#cb937-6"></a>  <span class="dt">subsample =</span> <span class="fl">0.5</span>, </span>
<span id="cb937-7"><a href="xgboost.html#cb937-7"></a>  <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>,</span>
<span id="cb937-8"><a href="xgboost.html#cb937-8"></a>  <span class="dt">gamma =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>),</span>
<span id="cb937-9"><a href="xgboost.html#cb937-9"></a>  <span class="dt">lambda =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e-2</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>),</span>
<span id="cb937-10"><a href="xgboost.html#cb937-10"></a>  <span class="dt">alpha =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e-2</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>),</span>
<span id="cb937-11"><a href="xgboost.html#cb937-11"></a>  <span class="dt">rmse =</span> <span class="dv">0</span>,          <span class="co"># a place to dump RMSE results</span></span>
<span id="cb937-12"><a href="xgboost.html#cb937-12"></a>  <span class="dt">trees =</span> <span class="dv">0</span>          <span class="co"># a place to dump required number of trees</span></span>
<span id="cb937-13"><a href="xgboost.html#cb937-13"></a>)</span>
<span id="cb937-14"><a href="xgboost.html#cb937-14"></a></span>
<span id="cb937-15"><a href="xgboost.html#cb937-15"></a><span class="co"># Head of the hyperparameter grid</span></span>
<span id="cb937-16"><a href="xgboost.html#cb937-16"></a><span class="kw">kable</span>(<span class="kw">head</span>(hyper_grid))</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:right;">
eta
</th>
<th style="text-align:right;">
max_depth
</th>
<th style="text-align:right;">
min_child_weight
</th>
<th style="text-align:right;">
subsample
</th>
<th style="text-align:right;">
colsample_bytree
</th>
<th style="text-align:right;">
gamma
</th>
<th style="text-align:right;">
lambda
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
rmse
</th>
<th style="text-align:right;">
trees
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>Besides those parameters we discussed, xgboost provides additional hyperparameters <code>alpha</code>, <code>gamma</code> and <code>lambda</code> that can help to constrain model complexity and reduce overfitting. We introduced them in the grid as well. With the code above we create a pretty large search grid consisting of 1960 different hyperparameter combinations to model. It is important to note that running such a grid in a loop procedure could take a couple of hours. We will create such a loop procedure to loop through and apply a xgboost model for each hyperparameter combination (1960 in our case) and finally provide us the results in the hyper_grid data frame.</p>
<div class="sourceCode" id="cb938"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb938-1"><a href="xgboost.html#cb938-1"></a><span class="co"># Grid search</span></span>
<span id="cb938-2"><a href="xgboost.html#cb938-2"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_len</span>(<span class="kw">nrow</span>(hyper_grid))) {</span>
<span id="cb938-3"><a href="xgboost.html#cb938-3"></a>  <span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb938-4"><a href="xgboost.html#cb938-4"></a>  m &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</span>
<span id="cb938-5"><a href="xgboost.html#cb938-5"></a>    <span class="dt">data =</span> ames_x_train,</span>
<span id="cb938-6"><a href="xgboost.html#cb938-6"></a>    <span class="dt">label =</span> ames_y_train,</span>
<span id="cb938-7"><a href="xgboost.html#cb938-7"></a>    <span class="dt">nrounds =</span> <span class="dv">4000</span>,</span>
<span id="cb938-8"><a href="xgboost.html#cb938-8"></a>    <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,</span>
<span id="cb938-9"><a href="xgboost.html#cb938-9"></a>    <span class="dt">early_stopping_rounds =</span> <span class="dv">50</span>, </span>
<span id="cb938-10"><a href="xgboost.html#cb938-10"></a>    <span class="dt">nfold =</span> <span class="dv">10</span>,</span>
<span id="cb938-11"><a href="xgboost.html#cb938-11"></a>    <span class="dt">verbose =</span> <span class="dv">0</span>,</span>
<span id="cb938-12"><a href="xgboost.html#cb938-12"></a>    <span class="dt">params =</span> <span class="kw">list</span>( </span>
<span id="cb938-13"><a href="xgboost.html#cb938-13"></a>      <span class="dt">eta =</span> hyper_grid<span class="op">$</span>eta[i], </span>
<span id="cb938-14"><a href="xgboost.html#cb938-14"></a>      <span class="dt">max_depth =</span> hyper_grid<span class="op">$</span>max_depth[i],</span>
<span id="cb938-15"><a href="xgboost.html#cb938-15"></a>      <span class="dt">min_child_weight =</span> hyper_grid<span class="op">$</span>min_child_weight[i],</span>
<span id="cb938-16"><a href="xgboost.html#cb938-16"></a>      <span class="dt">subsample =</span> hyper_grid<span class="op">$</span>subsample[i],</span>
<span id="cb938-17"><a href="xgboost.html#cb938-17"></a>      <span class="dt">colsample_bytree =</span> hyper_grid<span class="op">$</span>colsample_bytree[i],</span>
<span id="cb938-18"><a href="xgboost.html#cb938-18"></a>      <span class="dt">gamma =</span> hyper_grid<span class="op">$</span>gamma[i], </span>
<span id="cb938-19"><a href="xgboost.html#cb938-19"></a>      <span class="dt">lambda =</span> hyper_grid<span class="op">$</span>lambda[i], </span>
<span id="cb938-20"><a href="xgboost.html#cb938-20"></a>      <span class="dt">alpha =</span> hyper_grid<span class="op">$</span>alpha[i]</span>
<span id="cb938-21"><a href="xgboost.html#cb938-21"></a>    ) </span>
<span id="cb938-22"><a href="xgboost.html#cb938-22"></a>  )</span>
<span id="cb938-23"><a href="xgboost.html#cb938-23"></a>  hyper_grid<span class="op">$</span>rmse[i] &lt;-<span class="st"> </span><span class="kw">min</span>(m<span class="op">$</span>evaluation_log<span class="op">$</span>test_rmse_mean)</span>
<span id="cb938-24"><a href="xgboost.html#cb938-24"></a>  hyper_grid<span class="op">$</span>trees[i] &lt;-<span class="st"> </span>m<span class="op">$</span>best_iteration</span>
<span id="cb938-25"><a href="xgboost.html#cb938-25"></a>}</span>
<span id="cb938-26"><a href="xgboost.html#cb938-26"></a></span>
<span id="cb938-27"><a href="xgboost.html#cb938-27"></a><span class="co"># Results</span></span>
<span id="cb938-28"><a href="xgboost.html#cb938-28"></a>hyper_grid <span class="op">%&gt;%</span></span>
<span id="cb938-29"><a href="xgboost.html#cb938-29"></a><span class="st">  </span><span class="kw">filter</span>(rmse <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span></span>
<span id="cb938-30"><a href="xgboost.html#cb938-30"></a><span class="st">  </span><span class="kw">arrange</span>(rmse) <span class="op">%&gt;%</span></span>
<span id="cb938-31"><a href="xgboost.html#cb938-31"></a><span class="st">  </span><span class="kw">glimpse</span>()</span></code></pre></div>
<p>Here is a glimpse of results we obtained after several hours of processing:</p>
<div class="sourceCode" id="cb939"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb939-1"><a href="xgboost.html#cb939-1"></a><span class="co">## Observations: 98</span></span>
<span id="cb939-2"><a href="xgboost.html#cb939-2"></a><span class="co">## Variables: 10</span></span>
<span id="cb939-3"><a href="xgboost.html#cb939-3"></a><span class="co">## $ eta              &lt;dbl&gt; 0.01, 0.01, 0.01, 0.01, 0.01, 0.0…</span></span>
<span id="cb939-4"><a href="xgboost.html#cb939-4"></a><span class="co">## $ max_depth        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …</span></span>
<span id="cb939-5"><a href="xgboost.html#cb939-5"></a><span class="co">## $ min_child_weight &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …</span></span>
<span id="cb939-6"><a href="xgboost.html#cb939-6"></a><span class="co">## $ subsample        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…</span></span>
<span id="cb939-7"><a href="xgboost.html#cb939-7"></a><span class="co">## $ colsample_bytree &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…</span></span>
<span id="cb939-8"><a href="xgboost.html#cb939-8"></a><span class="co">## $ gamma            &lt;dbl&gt; 0, 1, 10, 100, 1000, 0, 1, 10, 10…</span></span>
<span id="cb939-9"><a href="xgboost.html#cb939-9"></a><span class="co">## $ lambda           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …</span></span>
<span id="cb939-10"><a href="xgboost.html#cb939-10"></a><span class="co">## $ alpha            &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.1…</span></span>
<span id="cb939-11"><a href="xgboost.html#cb939-11"></a><span class="co">## $ rmse             &lt;dbl&gt; 20488, 20488, 20488, 20488, 20488…</span></span>
<span id="cb939-12"><a href="xgboost.html#cb939-12"></a><span class="co">## $ trees            &lt;dbl&gt; 3944, 3944, 3944, 3944, 3944, 381…</span></span></code></pre></div>
<p>In the first “column” we see the combination of parameters given that results in the lowest estimated error (RMSE) possible for the combination given. Subsequently, we will use those parameters to enhance prediction performance of our model.</p>
<div class="sourceCode" id="cb940"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb940-1"><a href="xgboost.html#cb940-1"></a><span class="co"># The list of optimal hyperparameters</span></span>
<span id="cb940-2"><a href="xgboost.html#cb940-2"></a>params_optimal &lt;-<span class="st"> </span><span class="kw">list</span>(</span>
<span id="cb940-3"><a href="xgboost.html#cb940-3"></a>  <span class="dt">eta =</span> <span class="fl">0.01</span>,</span>
<span id="cb940-4"><a href="xgboost.html#cb940-4"></a>  <span class="dt">max_depth =</span> <span class="dv">3</span>,</span>
<span id="cb940-5"><a href="xgboost.html#cb940-5"></a>  <span class="dt">min_child_weight =</span> <span class="dv">3</span>,</span>
<span id="cb940-6"><a href="xgboost.html#cb940-6"></a>  <span class="dt">subsample =</span> <span class="fl">0.5</span>,</span>
<span id="cb940-7"><a href="xgboost.html#cb940-7"></a>  <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>,</span>
<span id="cb940-8"><a href="xgboost.html#cb940-8"></a>  <span class="dt">lambda =</span> <span class="dv">1</span></span>
<span id="cb940-9"><a href="xgboost.html#cb940-9"></a>)</span>
<span id="cb940-10"><a href="xgboost.html#cb940-10"></a></span>
<span id="cb940-11"><a href="xgboost.html#cb940-11"></a><span class="co"># Train final model  with optimal combination of the given parameters </span></span>
<span id="cb940-12"><a href="xgboost.html#cb940-12"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb940-13"><a href="xgboost.html#cb940-13"></a>xgb.fit.optimal &lt;-<span class="st"> </span><span class="kw">xgboost</span>(</span>
<span id="cb940-14"><a href="xgboost.html#cb940-14"></a>  <span class="dt">params =</span> params_optimal,</span>
<span id="cb940-15"><a href="xgboost.html#cb940-15"></a>  <span class="dt">data =</span> ames_x_train,</span>
<span id="cb940-16"><a href="xgboost.html#cb940-16"></a>  <span class="dt">label =</span> ames_y_train,</span>
<span id="cb940-17"><a href="xgboost.html#cb940-17"></a>  <span class="dt">nrounds =</span> <span class="dv">3944</span>,</span>
<span id="cb940-18"><a href="xgboost.html#cb940-18"></a>  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,</span>
<span id="cb940-19"><a href="xgboost.html#cb940-19"></a>  <span class="dt">verbose =</span> <span class="dv">0</span></span>
<span id="cb940-20"><a href="xgboost.html#cb940-20"></a>)</span></code></pre></div>
<pre><code>## [17:34:06] WARNING: amalgamation/../src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.</code></pre>
<p>After computing the final model, we can make inferences about how features (i.e. variables in our data set besides sale price) are influencing our model. Measurement of feature importance occurs based on the sum of the reduction in the loss function (e.g. SSE) attributed to each variable at each split in a respective tree. In other words, it is the relative contribution of the respective feature to the model computed by taking each feature’s contribution for each tree in the model. Therefore, those features with the highest average decrease in SSE (for regression) are identified as the one with the highest contribution. Thus, these features are among most important ones. To visualize feature importance plot we need to create importance matrix first then plot it with ggplot-based function “xgb.ggplot.importance”.</p>
<div class="sourceCode" id="cb942"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb942-1"><a href="xgboost.html#cb942-1"></a><span class="co"># Construct importance matrix</span></span>
<span id="cb942-2"><a href="xgboost.html#cb942-2"></a>importance_matrix &lt;-<span class="st"> </span><span class="kw">xgb.importance</span>(<span class="dt">model =</span> xgb.fit.optimal)</span>
<span id="cb942-3"><a href="xgboost.html#cb942-3"></a></span>
<span id="cb942-4"><a href="xgboost.html#cb942-4"></a><span class="co"># Variable importance plot with ggplot2</span></span>
<span id="cb942-5"><a href="xgboost.html#cb942-5"></a><span class="kw">xgb.ggplot.importance</span>(importance_matrix, <span class="dt">top_n =</span> <span class="dv">15</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-597-1.png" width="672" />
As we identified the most relevant features, now we can try to understand how the response variable (i.e. predicted sale price) changes based on these variables. For this we can use partial dependence plots (PDPs). They show the marginal effect one or two features have on the predicted outcome.
Let’s consider the <code>gr_liv_area</code> variable. The PDP plot below displays the average change in predicted sales price as we vary <code>gr_liv_area</code> while holding all other variables constant. More specifically, it shows the movement of the predicted sales price as the square footage of the ground floor in a house changes, while holding other variables constant. It is important to mention that PDPs are valid as long as the target variable (sale price) and the variable under observation (<code>gr_liv_area</code>) are not correlated. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible.</p>
<div class="sourceCode" id="cb943"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb943-1"><a href="xgboost.html#cb943-1"></a><span class="kw">library</span>(pdp)</span>
<span id="cb943-2"><a href="xgboost.html#cb943-2"></a>pdp &lt;-<span class="st"> </span>xgb.fit.optimal <span class="op">%&gt;%</span></span>
<span id="cb943-3"><a href="xgboost.html#cb943-3"></a><span class="st">  </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;gr_liv_area&quot;</span>, <span class="dt">n.trees =</span> <span class="dv">3944</span>, <span class="dt">grid.resolution =</span> <span class="dv">100</span>, <span class="dt">train =</span> ames_x_train) <span class="op">%&gt;%</span></span>
<span id="cb943-4"><a href="xgboost.html#cb943-4"></a><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_x_train) <span class="op">+</span></span>
<span id="cb943-5"><a href="xgboost.html#cb943-5"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>dollar) <span class="op">+</span></span>
<span id="cb943-6"><a href="xgboost.html#cb943-6"></a><span class="st">  </span>ggplot2<span class="op">::</span><span class="kw">xlab</span>(<span class="dt">label=</span> <span class="st">&quot;Ground floor living area&quot;</span>)<span class="op">+</span></span>
<span id="cb943-7"><a href="xgboost.html#cb943-7"></a><span class="st">  </span>ggplot2<span class="op">::</span><span class="kw">ylab</span>(<span class="dt">label=</span><span class="st">&quot;Predicted sale price&quot;</span>)<span class="op">+</span></span>
<span id="cb943-8"><a href="xgboost.html#cb943-8"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;PDP - Influence of the ground floor size on a house sale price&quot;</span>)</span>
<span id="cb943-9"><a href="xgboost.html#cb943-9"></a><span class="kw">ggplotly</span>(pdp)</span></code></pre></div>
<div id="htmlwidget-288eff2e8f22fcca78da" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-288eff2e8f22fcca78da">{"x":{"data":[{"x":[334,387.616161616162,441.232323232323,494.848484848485,548.464646464646,602.080808080808,655.69696969697,709.313131313131,762.929292929293,816.545454545455,870.161616161616,923.777777777778,977.393939393939,1031.0101010101,1084.62626262626,1138.24242424242,1191.85858585859,1245.47474747475,1299.09090909091,1352.70707070707,1406.32323232323,1459.93939393939,1513.55555555556,1567.17171717172,1620.78787878788,1674.40404040404,1728.0202020202,1781.63636363636,1835.25252525253,1888.86868686869,1942.48484848485,1996.10101010101,2049.71717171717,2103.33333333333,2156.9494949495,2210.56565656566,2264.18181818182,2317.79797979798,2371.41414141414,2425.0303030303,2478.64646464646,2532.26262626263,2585.87878787879,2639.49494949495,2693.11111111111,2746.72727272727,2800.34343434343,2853.9595959596,2907.57575757576,2961.19191919192,3014.80808080808,3068.42424242424,3122.0404040404,3175.65656565657,3229.27272727273,3282.88888888889,3336.50505050505,3390.12121212121,3443.73737373737,3497.35353535354,3550.9696969697,3604.58585858586,3658.20202020202,3711.81818181818,3765.43434343434,3819.05050505051,3872.66666666667,3926.28282828283,3979.89898989899,4033.51515151515,4087.13131313131,4140.74747474748,4194.36363636364,4247.9797979798,4301.59595959596,4355.21212121212,4408.82828282828,4462.44444444444,4516.06060606061,4569.67676767677,4623.29292929293,4676.90909090909,4730.52525252525,4784.14141414141,4837.75757575758,4891.37373737374,4944.9898989899,4998.60606060606,5052.22222222222,5105.83838383838,5159.45454545455,5213.07070707071,5266.68686868687,5320.30303030303,5373.91919191919,5427.53535353535,5481.15151515152,5534.76767676768,5588.38383838384,5642],"y":[155379.34718438,155379.34718438,155379.34718438,155379.34718438,156003.748713773,156042.96033244,156126.314511157,156237.434223545,157429.618837144,157944.958659964,159692.638897345,161647.515449951,161878.860280687,164205.222812272,164736.618146462,164961.420984535,166362.4988736,169413.312279286,169823.906830325,172217.453220135,173014.242585165,173923.381425429,178242.530985524,179022.219429265,180613.388132459,183407.14291966,184005.322656631,188222.950780869,189761.143380114,190830.420531691,193938.50996636,198585.668058177,201687.512744307,201207.891329,201118.290333506,200811.907638973,207607.389426297,210435.799550201,212571.004364802,213799.543549075,214601.924409401,212954.109375,214330.906660984,214746.4215934,221249.661573764,221985.120376431,223231.067211094,226327.745680863,226894.758261538,225477.878550444,225547.962048679,226223.93001096,229258.912544143,234632.367606095,235992.87915931,238131.704087768,237212.481791129,235969.92416966,235801.932001187,236959.735596536,240111.011321085,242360.227723149,242675.010769301,242675.010769301,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,233263.239154591,233263.239154591,222827.816921883,222827.816921883,222827.816921883,222827.816921883,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584],"text":["object[[1L]]:  334.0000<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  387.6162<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  441.2323<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  494.8485<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  548.4646<br />object[[\"yhat\"]]: 156003.7","object[[1L]]:  602.0808<br />object[[\"yhat\"]]: 156043.0","object[[1L]]:  655.6970<br />object[[\"yhat\"]]: 156126.3","object[[1L]]:  709.3131<br />object[[\"yhat\"]]: 156237.4","object[[1L]]:  762.9293<br />object[[\"yhat\"]]: 157429.6","object[[1L]]:  816.5455<br />object[[\"yhat\"]]: 157945.0","object[[1L]]:  870.1616<br />object[[\"yhat\"]]: 159692.6","object[[1L]]:  923.7778<br />object[[\"yhat\"]]: 161647.5","object[[1L]]:  977.3939<br />object[[\"yhat\"]]: 161878.9","object[[1L]]: 1031.0101<br />object[[\"yhat\"]]: 164205.2","object[[1L]]: 1084.6263<br />object[[\"yhat\"]]: 164736.6","object[[1L]]: 1138.2424<br />object[[\"yhat\"]]: 164961.4","object[[1L]]: 1191.8586<br />object[[\"yhat\"]]: 166362.5","object[[1L]]: 1245.4747<br />object[[\"yhat\"]]: 169413.3","object[[1L]]: 1299.0909<br />object[[\"yhat\"]]: 169823.9","object[[1L]]: 1352.7071<br />object[[\"yhat\"]]: 172217.5","object[[1L]]: 1406.3232<br />object[[\"yhat\"]]: 173014.2","object[[1L]]: 1459.9394<br />object[[\"yhat\"]]: 173923.4","object[[1L]]: 1513.5556<br />object[[\"yhat\"]]: 178242.5","object[[1L]]: 1567.1717<br />object[[\"yhat\"]]: 179022.2","object[[1L]]: 1620.7879<br />object[[\"yhat\"]]: 180613.4","object[[1L]]: 1674.4040<br />object[[\"yhat\"]]: 183407.1","object[[1L]]: 1728.0202<br />object[[\"yhat\"]]: 184005.3","object[[1L]]: 1781.6364<br />object[[\"yhat\"]]: 188223.0","object[[1L]]: 1835.2525<br />object[[\"yhat\"]]: 189761.1","object[[1L]]: 1888.8687<br />object[[\"yhat\"]]: 190830.4","object[[1L]]: 1942.4848<br />object[[\"yhat\"]]: 193938.5","object[[1L]]: 1996.1010<br />object[[\"yhat\"]]: 198585.7","object[[1L]]: 2049.7172<br />object[[\"yhat\"]]: 201687.5","object[[1L]]: 2103.3333<br />object[[\"yhat\"]]: 201207.9","object[[1L]]: 2156.9495<br />object[[\"yhat\"]]: 201118.3","object[[1L]]: 2210.5657<br />object[[\"yhat\"]]: 200811.9","object[[1L]]: 2264.1818<br />object[[\"yhat\"]]: 207607.4","object[[1L]]: 2317.7980<br />object[[\"yhat\"]]: 210435.8","object[[1L]]: 2371.4141<br />object[[\"yhat\"]]: 212571.0","object[[1L]]: 2425.0303<br />object[[\"yhat\"]]: 213799.5","object[[1L]]: 2478.6465<br />object[[\"yhat\"]]: 214601.9","object[[1L]]: 2532.2626<br />object[[\"yhat\"]]: 212954.1","object[[1L]]: 2585.8788<br />object[[\"yhat\"]]: 214330.9","object[[1L]]: 2639.4949<br />object[[\"yhat\"]]: 214746.4","object[[1L]]: 2693.1111<br />object[[\"yhat\"]]: 221249.7","object[[1L]]: 2746.7273<br />object[[\"yhat\"]]: 221985.1","object[[1L]]: 2800.3434<br />object[[\"yhat\"]]: 223231.1","object[[1L]]: 2853.9596<br />object[[\"yhat\"]]: 226327.7","object[[1L]]: 2907.5758<br />object[[\"yhat\"]]: 226894.8","object[[1L]]: 2961.1919<br />object[[\"yhat\"]]: 225477.9","object[[1L]]: 3014.8081<br />object[[\"yhat\"]]: 225548.0","object[[1L]]: 3068.4242<br />object[[\"yhat\"]]: 226223.9","object[[1L]]: 3122.0404<br />object[[\"yhat\"]]: 229258.9","object[[1L]]: 3175.6566<br />object[[\"yhat\"]]: 234632.4","object[[1L]]: 3229.2727<br />object[[\"yhat\"]]: 235992.9","object[[1L]]: 3282.8889<br />object[[\"yhat\"]]: 238131.7","object[[1L]]: 3336.5051<br />object[[\"yhat\"]]: 237212.5","object[[1L]]: 3390.1212<br />object[[\"yhat\"]]: 235969.9","object[[1L]]: 3443.7374<br />object[[\"yhat\"]]: 235801.9","object[[1L]]: 3497.3535<br />object[[\"yhat\"]]: 236959.7","object[[1L]]: 3550.9697<br />object[[\"yhat\"]]: 240111.0","object[[1L]]: 3604.5859<br />object[[\"yhat\"]]: 242360.2","object[[1L]]: 3658.2020<br />object[[\"yhat\"]]: 242675.0","object[[1L]]: 3711.8182<br />object[[\"yhat\"]]: 242675.0","object[[1L]]: 3765.4343<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3819.0505<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3872.6667<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3926.2828<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3979.8990<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4033.5152<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4087.1313<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4140.7475<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4194.3636<br />object[[\"yhat\"]]: 233263.2","object[[1L]]: 4247.9798<br />object[[\"yhat\"]]: 233263.2","object[[1L]]: 4301.5960<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4355.2121<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4408.8283<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4462.4444<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4516.0606<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4569.6768<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4623.2929<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4676.9091<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4730.5253<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4784.1414<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4837.7576<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4891.3737<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4944.9899<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4998.6061<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5052.2222<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5105.8384<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5159.4545<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5213.0707<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5266.6869<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5320.3030<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5373.9192<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5427.5354<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5481.1515<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5534.7677<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5588.3838<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5642.0000<br />object[[\"yhat\"]]: 202002.0"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"y":[151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436],"text":"x.rug[[1L]]: NA","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":72.3287671232877},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"PDP - Influence of the ground floor size on a house sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[68.6,5907.4],"tickmode":"array","ticktext":["1000","2000","3000","4000","5000"],"tickvals":[1000,2000,3000,4000,5000],"categoryorder":"array","categoryarray":["1000","2000","3000","4000","5000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Ground floor living area","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[151014.564005133,247039.793948547],"tickmode":"array","ticktext":["$175,000","$200,000","$225,000"],"tickvals":[175000,200000,225000],"categoryorder":"array","categoryarray":["$175,000","$200,000","$225,000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Predicted sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"87c342dba5":{"x":{},"y":{},"type":"scatter"},"87c5c713368":{"x":{}}},"cur_data":"87c342dba5","visdat":{"87c342dba5":["function (y) ","x"],"87c5c713368":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>We use predict function to predict unseen observations from the test data set we created at the beginning of the chapter. Since we already know the real sale prices from the test data set, we will be able to calculate the error of our predictive model.</p>
<div class="sourceCode" id="cb944"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb944-1"><a href="xgboost.html#cb944-1"></a><span class="co"># Predict values for test data optimal</span></span>
<span id="cb944-2"><a href="xgboost.html#cb944-2"></a>pred.optimal &lt;-<span class="st"> </span><span class="kw">predict</span>(xgb.fit.optimal, ames_x_test)</span>
<span id="cb944-3"><a href="xgboost.html#cb944-3"></a></span>
<span id="cb944-4"><a href="xgboost.html#cb944-4"></a><span class="co"># Results with optimal parameters</span></span>
<span id="cb944-5"><a href="xgboost.html#cb944-5"></a><span class="kw">RMSE</span>(pred.optimal, ames_test[,<span class="dv">308</span>])</span></code></pre></div>
<pre><code>## [1] 21988.76</code></pre>
<p>Finally, we can nicely visualize predicted and actual sale price.</p>
<div class="sourceCode" id="cb946"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb946-1"><a href="xgboost.html#cb946-1"></a><span class="co"># Plot predictions vs actual sale price</span></span>
<span id="cb946-2"><a href="xgboost.html#cb946-2"></a>pred.optimal &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(pred.optimal)</span>
<span id="cb946-3"><a href="xgboost.html#cb946-3"></a>ames_test_pred&lt;-<span class="st"> </span><span class="kw">data.frame</span>(ames_test[,<span class="dv">308</span>],pred.optimal)</span>
<span id="cb946-4"><a href="xgboost.html#cb946-4"></a><span class="kw">colnames</span>(ames_test_pred) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;sale_price&quot;</span>,<span class="st">&quot;pred.optimal&quot;</span>)</span>
<span id="cb946-5"><a href="xgboost.html#cb946-5"></a></span>
<span id="cb946-6"><a href="xgboost.html#cb946-6"></a>p&lt;-<span class="kw">ggplot</span>(ames_test_pred, <span class="kw">aes</span>(<span class="dt">x =</span> pred.optimal, <span class="dt">y =</span> sale_price, <span class="dt">Predicted=</span>pred.optimal, <span class="dt">Tested=</span>sale_price)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb946-7"><a href="xgboost.html#cb946-7"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb946-8"><a href="xgboost.html#cb946-8"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">TRUE</span>)<span class="op">+</span></span>
<span id="cb946-9"><a href="xgboost.html#cb946-9"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="dt">label =</span> <span class="st">&quot;Predicted sale price&quot;</span> )<span class="op">+</span></span>
<span id="cb946-10"><a href="xgboost.html#cb946-10"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="dt">label =</span> <span class="st">&quot;Test sale price&quot;</span>)<span class="op">+</span></span>
<span id="cb946-11"><a href="xgboost.html#cb946-11"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="dt">label =</span> <span class="st">&quot;Predicted sale price vs sales price&quot;</span>)</span>
<span id="cb946-12"><a href="xgboost.html#cb946-12"></a><span class="kw">ggplotly</span>(p,<span class="dt">tooltip =</span> <span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span>,<span class="st">&quot;Tested&quot;</span>))</span></code></pre></div>
<div id="htmlwidget-5f4465cb342cb94a3be5" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-5f4465cb342cb94a3be5">{"x":{"data":[{"x":[166271.390625,256515.046875,199401.265625,166460.34375,225155.5,223877.625,368326.125,113401.2734375,220700.8125,140805.265625,101971.65625,98567.53125,89476.40625,143621.203125,115159.3828125,296676.5625,205599.3125,295839.09375,165820.625,177696.546875,231610.796875,171528.625,297061.90625,385805.5625,246673.6875,129554.5,141805.203125,126570.65625,186036.25,233963.21875,300572.21875,161674.59375,161428.96875,160553.375,172177.6875,147506.28125,193459.3125,173138.25,195629.140625,149522.65625,154535.84375,142006.5625,171383.8125,170539.78125,139929.09375,208535.5,134901.40625,138378.015625,259089.625,132544.890625,176060.421875,153373.171875,138372.84375,116273.5859375,127647.765625,139465.28125,104709.5234375,131107.453125,125741.515625,156840.796875,117847.5546875,132056.859375,164390.578125,123470.0390625,161293.046875,134827.515625,129198.6484375,130815.359375,120447.109375,116926.1484375,133267.515625,130772.5,148793.359375,154093.328125,265012.28125,146097.59375,128131.09375,173764.5625,252534.078125,224337.546875,154118.65625,251924.578125,193866.203125,138395.625,155960.140625,130855.8984375,308689.5,144450.265625,182126.328125,97740.265625,109675.2109375,88958.640625,85920.1796875,124000.265625,62582.0234375,164724.71875,164124.09375,181570.296875,95982.765625,91794.1875,85455.2421875,164045.59375,159036.984375,187896.5625,224692.96875,92540.296875,127217.6015625,79107.6875,168133.828125,164828.796875,181321.140625,188156.171875,161093.484375,478695.53125,353490.5,189160.1875,166938.9375,190210.890625,180141.015625,182451.46875,158273.21875,142248.828125,190414.203125,233005.375,150057.609375,100398.546875,115107.2421875,121091.1640625,132085.9375,141957.75,138278.28125,139542.515625,142017.046875,165697.859375,340862.75,503946.3125,468924.28125,320431.21875,258773.25,371853.1875,503169.375,320362.875,489410.8125,306309.28125,195915.296875,198249.59375,207977.875,398985,243931.015625,214058.96875,169462.640625,178346.203125,189741.09375,192407.265625,207183.234375,185499.71875,218535.53125,301066.09375,313467.21875,259583.390625,253427.421875,252730.234375,215119.25,356741.03125,225461.875,231481.078125,182105.09375,136632.390625,201509.34375,158813.15625,215147.296875,197347.15625,194086.59375,131761.453125,164726.296875,69414.171875,119315.7890625,166612.34375,201669.0625,159032.140625,202200.453125,151330.1875,148461.140625,158918.796875,219082.546875,192936.171875,131355.109375,221285.359375,167833.34375,148635.46875,193297.296875,140437.875,117221.90625,127239.8515625,145058.78125,122045.5546875,165999.765625,226345.25,147813.4375,127404.1328125,139694.546875,180507.359375,131020.4609375,134795.765625,123721.6171875,129414.8671875,100391.125,122471.125,147543.84375,162698.28125,159235.90625,60427.6015625,87917.2265625,204091.546875,198683.046875,151526.640625,182105.109375,93445.7890625,120115.5234375,139347.640625,136876.578125,106529.015625,109731.359375,92143.75,96223.5703125,112303.296875,141219,51007.2109375,134826.3125,123963.3984375,108967.921875,178929.125,136449.78125,168397,144247.25,82399.0546875,75996.84375,142203.390625,139473.875,108788.0078125,91823.2578125,139814.046875,147576.234375,109972.8046875,196445.25,219606.859375,170327.296875,130373.71875,262348.28125,311990.75,340165.6875,282119.71875,266918.75,233105.8125,155296.296875,184340.03125,216493.921875,215416.765625,182522.921875,124220.234375,132183.296875,159623.46875,135198.765625,156089.53125,132050.453125,130047.8359375,139629.640625,236420.640625,312305.375,189984.234375,201520.609375,271926.875,189037.640625,177784.484375,108379.1796875,120775.1875,112632.0703125,102262.109375,81681.0625,124657.1484375,130118.8671875,116582.8125,147097.328125,118145.3359375,189893.96875,246364.109375,175892.859375,138325.578125,226791.0625,122915.21875,290950.15625,48078.31640625,244371.9375,233353.75,214170.640625,166382.53125,403252,330932.5625,220482.640625,234959.84375,134576.359375,160459.640625,97883.7265625,134661.59375,122597.828125,146224.25,200921.40625,169677.890625,204703,177710.953125,175968.984375,167880.765625,186159.234375,195615.21875,335472.25,323513.46875,369938.34375,184427.578125,182198.40625,148816.078125,319800.5625,140437.90625,123542.3515625,172410.59375,94020.921875,102221.5078125,357636.59375,465650.78125,363907.96875,278747.40625,394274.53125,366574.0625,204926.203125,182375.75,202610.75,235066.40625,192037.4375,182320.234375,219121.328125,373596.6875,268824.75,207962.921875,215675.34375,256638.078125,313613.125,249111.125,206215.15625,178793.265625,182867.125,203029.46875,198493.984375,169776.34375,161832.765625,136664.453125,130733.6328125,182827.25,137692.25,354421.65625,163707.765625,253430.640625,273379.15625,229426.8125,333305.3125,203135.5,203774.3125,152387.125,145780.234375,316436.46875,166200.921875,175294.734375,119723.2578125,146908.796875,159910.203125,155847.875,129731.25,150280.796875,145198,121750.8046875,219622.171875,156965.921875,231819.140625,160991.9375,133308.84375,115209.1796875,107105.4296875,104116.328125,126222.2578125,115286.0234375,77970.46875,163085.8125,132251.140625,115321.578125,110804.40625,123665.3515625,126486.6328125,131926.890625,77038.4296875,130729.265625,103064.7421875,148071.578125,134216.515625,53917.5390625,109783.65625,102840.6015625,118769.671875,90140.6171875,139457.46875,152949.34375,131515.109375,117237.59375,73413.4140625,195672.265625,145431.46875,141225.109375,107372.2265625,147150.421875,123158.0859375,115735.453125,78487.421875,151174.1875,194004.328125,123368.1328125,117507.1484375,116050.8515625,125682.953125,105146.2734375,120799.9921875,104629.3828125,116881.5234375,115892,139384.265625,147891.09375,176121.40625,106935.6171875,152457.8125,160460.34375,199565.453125,165896.375,148154.28125,207830.390625,118626.2890625,244537.421875,223760.90625,175129.125,241441.8125,199805.609375,208373.625,186915.984375,226950.734375,210998.578125,154750.46875,121598.5703125,235729.4375,263923.96875,279993.09375,233299.4375,189527.359375,141543.5625,202388.125,148372.828125,118823.03125,184482.921875,140780.609375,76623.125,119225.5546875,86211.46875,189470.84375,156046.109375,143180.640625,118756.875,176997.671875,172568.703125,126951.5234375,257460.75,133333.6875,110698.5234375,295261.28125,269972.6875,212724.765625,204302.671875,237609.828125,211695.265625,281012.4375,263084.5,250197.53125,166674.453125,219817.5625,231291.359375,325964.1875,269582.90625,330993.6875,128758.84375,109638.28125,140177.34375,112310.171875,85464.6484375,142182.59375,132086.171875,196387.6875,170528.4375,160415.046875,162803.828125,343199.78125,236800,379497.125,195962.9375,147521.78125,180318.953125,143471.78125,119410.6015625,116043.015625,104776.046875,140681.890625,371284.75,381954.84375,271368.875,286709.875,320856.53125,317895.9375,496971.28125,283634.125,298690.375,292908.6875,254474.4375,187748.140625,190408.890625,189984.953125,196941.734375,221516.984375,279625.9375,300599.0625,179451.796875,176547.671875,176663.984375,220071.8125,194520.3125,170503.3125,197813.8125,237195.390625,244479.171875,180147.53125,169322.0625,207027.6875,567872,313987.0625,296083.4375,399155.46875,313675.9375,341440.84375,234843.046875,145826.1875,186825.625,190970.40625,162419.734375,135268.796875,124727.578125,275641.875,142164,309569.65625,187713.859375,230297.609375,192966.28125,219302.25,176008.34375,255229.71875,277408.59375,187596.171875,201146.375,175574.6875,193799.75,239388.21875,162650.765625,193892.25,244522.3125,148426.390625,132863.703125,140861.328125,133945.609375,128057.5625,151114.390625,143514.5625,64417.50390625,131830.34375,128350.4921875,127487.4765625,155927.671875,135072.375,136356.671875,132716.484375,162981.75,144008.09375,140032.078125,123294.1875,148176.125,130698.703125,282161.84375,114934.5234375,129396.796875,106450.2265625,107172.8671875,145042.578125,127731.390625,111664.625,89753.671875,113267.390625,126494.28125,92214.65625,131224.984375,107158.046875,119548.5625,146179.78125,74055.3515625,154805.640625,130022.578125,125243.8046875,125761.78125,129285.640625,258356.15625,114463.5078125,113479.6015625,159887.75,124717.3984375,115747.640625,127322.96875,117471.234375,124556.890625,130945.359375,169585.34375,154066.40625,156753.296875,152819.296875,147973.375,81538.0625,213754.578125,107679.65625,146621.140625,94879.8125,137719.796875,95878.4375,181420.796875,195153.4375,280939.03125,223674.28125,186907.65625,259747.109375,206188.21875,215375.484375,178614.390625,282516,213788.125,208440.1875,186690.90625,193961.15625,234467.828125,142548.484375,189085.640625,182018.828125,177825.734375,175121.203125,131258.671875,135809.046875,169713.71875,143398.140625,125783.5234375,89397.53125,105047.1484375,140787.625,142991.546875,196122.609375,144291.375,294967.96875,209514.734375,122692.0546875,201533.96875,125568.1640625,134748.765625,165395.859375,129255.4296875,61046.1953125,287144.28125,168954.125,140323.5,220513.375,201368.5625,232433.21875,243802.921875,172565.671875,338061.84375,295648.8125,83466.5703125,87363.3203125,78508.9921875,185133.03125,131600.703125,126639.1015625,180214.734375,183871.640625,294832.8125,241543.6875,241958.046875,179477.046875,425047.34375,493195.03125,460147,406190.4375,180380.96875,316014.84375,429645.78125,241540.609375,213910.890625,295853.28125,168216.671875,267411.53125,176933.0625,151952.46875,133082.515625,107971.296875,94553.25,98655.421875,153615.0625,423206.8125,244889.671875,275170,404508.65625,313883.90625,274664.5625,365766.3125,303676.28125,358861.0625,190505.53125,344049.5625,179210.875,177157.71875,171499.90625,284833.90625,195476.65625,273309.8125,191171.015625,326439.84375,328552.53125,255487.875,297300.78125,192205.046875,137542.78125,145061.875,154506.71875,133579.90625,116389.3671875,129590.65625,314293.21875,258662.609375,153029.65625,168997.03125,145095.328125,203091.140625,190436.734375,132457.4375,141434.828125,160205.609375,192404.46875,121600.671875,156067.8125,127062.53125,125856.703125,140398.90625,124180.9609375,280587.03125,145892.671875,141591.78125,138770.09375,217428.859375,146333.734375,297376.125,106922.6015625,170570.546875,146087.578125,115117.4375,145515.828125,115143.765625,186823.3125,153022.21875,278870.75,211741.34375,131048.6875,77560.8984375,89190.0546875,136220.3125,137051.03125,117955.5078125,172122.046875,139933.859375,167321.640625,101342.1796875,131426.484375,94333.6953125,116663.3203125,169587.90625,149415.78125,144640,64047.87890625,152577.09375,125785.921875,97703.3359375,129268.6328125,97724.7421875,103458.328125,214828.0625,124432.515625,142277.40625,132651.640625,166702.75,158844.125,133345.5,134796.203125,164449.96875,161876.109375,132920.96875,143849.046875,137230.46875,131140.140625,226274.890625,235905.265625,256967.265625,239647.78125,267880.34375,215708.734375,223279.71875,151737.609375,194959.125,210324.390625,221314.578125,225817.40625,133013.390625,150876.921875,135448.15625,277078.28125,255182.09375,215176.59375,239893.953125,237239.84375,143125.5,147443.03125,240000.359375,196979.015625,117273.5078125,140370.90625,146548.109375,142440.8125,142487.90625,98286.46875,97476.6640625,133302.34375,120465.03125,97600.9296875,107850.921875,117131.5625,145921.828125,193662,255313.359375,197672.484375,229106.8125,242246.96875,94497.4765625,45369.69140625,91805.28125,67834.953125,222752.0625,171263.265625,214494.75,194313.9375,317802.71875,202277.796875,160477.1875,78144.109375,167069.578125,143612.40625,208354.78125],"y":[172000,244000,195500,180400,212000,164000,394432,141000,210000,142000,115000,105500,88000,149900,120000,306000,214000,319900,175500,199500,216500,180000,290000,410000,271500,99500,138500,133000,169000,190000,362500,155000,149500,152000,177500,147110,206000,218500,212500,142250,143000,136300,180500,84900,142125,197600,116500,132000,180000,136000,165500,167500,108538,108000,135000,109000,107500,129000,97500,155000,115000,130000,129000,100000,150000,128500,128000,132000,123900,109500,114900,131500,154000,163000,270000,124000,127000,186000,218000,236000,147000,245350,187000,138500,150000,128200,318000,143000,185500,155891,100000,64000,80000,128000,58500,127000,160000,185000,102776,55993,50138,190000,169900,170000,214900,83500,119500,75500,159000,157000,185000,181316,174000,501837,372500,185000,181000,154000,200000,184000,157000,152000,197500,240900,165000,97000,118000,119500,143750,148500,123000,147000,137900,148800,337500,485000,555000,325000,256300,398800,610000,296000,445000,290000,196000,184500,230000,382500,248500,254000,184000,174000,188500,184100,207500,181000,214000,265000,260000,263550,257500,287090,225000,370878,238500,263000,159000,143500,193000,153000,224243,189000,171500,120000,162000,76000,122000,164500,195000,172500,180500,150000,154000,185000,206000,197900,113000,213250,172500,154000,177500,124500,122000,128900,140000,124000,159000,256000,155000,120000,153000,176000,135000,131000,126000,129900,99900,135000,149000,142900,156500,59000,78500,190000,200000,153000,157500,92900,139000,132500,127000,94550,93000,80000,91300,110000,124900,34900,149000,119000,115000,214500,155000,155000,179900,62500,63000,149900,137000,122000,113000,139500,131000,105000,213000,239900,131000,147983,269500,297000,332000,272500,239000,221800,145000,195000,227000,230000,187100,124000,140000,150500,136500,143500,133500,133900,133000,250000,313000,198500,211000,279500,191000,178000,100000,127000,118000,85000,99900,119900,103500,160000,139500,105000,177000,234000,205000,154900,224000,121000,230000,57625,251000,240000,215000,152000,410000,316500,201000,213500,139500,162000,86000,131250,112000,130000,173000,165000,192000,180000,181000,183000,185000,189000,355000,325000,387000,175500,181900,175000,306000,133000,123000,181000,103400,100500,394617,417500,379000,250000,412500,421250,219500,154000,207000,195000,191000,179000,226750,350000,264132,227680,182000,250000,339750,256000,207500,192500,182000,193500,189000,179200,153900,135000,142000,192000,140000,372500,179400,290000,305000,217500,150000,205000,215000,147000,135000,299800,173000,178400,135750,155000,180500,174900,140000,145500,142000,119000,201800,165000,227000,163000,133000,116000,130000,110000,100000,118500,89900,171000,138000,112500,105500,127500,136870,122600,64000,139500,95000,147000,140000,46500,107900,65000,132000,98000,129400,163000,128000,116900,55000,184000,138000,108000,79500,153000,105000,113000,81300,162900,207000,130000,127500,120000,127500,89500,125000,79900,124000,109900,145000,153000,185000,108000,152400,144000,241500,177000,155000,235000,125000,262280,225000,177439,248500,207500,193000,188000,221000,231500,158000,127000,230000,287000,274000,240000,183000,136500,210000,149300,108000,165400,148000,82500,99000,98000,179500,136500,168000,130000,161900,177000,110000,263400,126000,99500,392500,271000,213000,228500,228950,241500,287000,294000,264966,167500,218689,195000,305000,298751,370000,124000,115000,152500,98000,81000,138000,103000,225000,168000,160000,160000,349265,392000,441929,192000,148000,197000,152000,123000,120500,113500,142500,356000,314813,318000,322400,318000,338931,479069,260116,317000,285000,250000,194700,204000,200000,207000,209500,277500,318061,178900,168165,171925,198444,181134,156932,172500,226500,259000,188500,165500,211000,745000,322500,290000,419005,147000,311872,250000,147000,181000,201000,179000,128000,125500,300000,129000,285000,166000,193800,208900,207500,177000,239000,301000,194000,213750,187000,190000,226000,164000,188500,255000,156000,139900,151500,145000,124000,140500,147000,64000,137000,105000,126000,175000,144000,141000,120000,163500,142000,134500,115000,153000,135000,301600,109000,128500,64500,102000,152000,141000,89471,108500,114000,124500,104500,137500,102000,90000,153575,113000,169500,139400,127000,128500,122000,200500,126000,118500,165000,123000,108000,119900,115000,134500,129000,112000,165250,150000,137000,130000,109900,243000,118000,150000,86000,130000,96000,228500,179900,301000,220000,200141,246500,203000,212999,176432,277000,187500,204750,190550,200000,211000,131500,185000,179400,168675,167000,118500,133500,171500,140000,131750,79000,110000,148500,139000,238000,140000,315000,215000,123900,170000,115000,129850,150909,118400,68104,375000,183500,134000,245000,210000,244000,267300,174000,392000,281213,85500,93900,75190,196000,129500,129500,192100,185000,320000,250000,274725,165600,457347,545224,535000,438780,169000,318000,470000,235000,241500,250000,179900,294323,181000,155000,138800,97500,91500,89000,145000,412083,252000,293000,415000,300000,275500,345000,298236,360000,202500,332200,169990,169985,172785,258000,234000,264561,173000,350000,321000,252000,290000,186500,132000,142500,150000,125000,116000,137000,310000,262000,147400,183900,139000,200000,170000,135500,157500,146000,190000,129900,157500,140000,127000,146500,121000,235000,143000,145250,156000,167000,159000,180000,105000,159000,147000,115000,159500,120000,183000,157500,277500,207500,147500,135000,87500,146000,120000,124000,169000,156500,178000,105000,111500,108000,96900,155000,144000,157000,64500,159000,114500,88000,149000,89000,109000,220000,116500,148000,142500,180000,156500,157000,145000,168000,164000,130000,142500,136900,149900,209000,221500,233555,260000,294900,209700,220000,145000,193000,217000,217000,205000,132500,157500,128500,275000,230000,210900,274300,216837,133000,155900,233230,203160,98000,145000,140000,130000,137500,92000,107000,104900,125000,121000,128000,102000,131000,140000,250000,218000,239000,257000,102000,72000,106500,78000,228000,176000,250000,202000,312500,215000,164000,71000,131000,142500,188000],"text":["pred.optimal: 166271.39<br />sale_price: 172000","pred.optimal: 256515.05<br />sale_price: 244000","pred.optimal: 199401.27<br />sale_price: 195500","pred.optimal: 166460.34<br />sale_price: 180400","pred.optimal: 225155.50<br />sale_price: 212000","pred.optimal: 223877.62<br />sale_price: 164000","pred.optimal: 368326.12<br />sale_price: 394432","pred.optimal: 113401.27<br />sale_price: 141000","pred.optimal: 220700.81<br />sale_price: 210000","pred.optimal: 140805.27<br />sale_price: 142000","pred.optimal: 101971.66<br />sale_price: 115000","pred.optimal:  98567.53<br />sale_price: 105500","pred.optimal:  89476.41<br />sale_price:  88000","pred.optimal: 143621.20<br />sale_price: 149900","pred.optimal: 115159.38<br />sale_price: 120000","pred.optimal: 296676.56<br />sale_price: 306000","pred.optimal: 205599.31<br />sale_price: 214000","pred.optimal: 295839.09<br />sale_price: 319900","pred.optimal: 165820.62<br />sale_price: 175500","pred.optimal: 177696.55<br />sale_price: 199500","pred.optimal: 231610.80<br />sale_price: 216500","pred.optimal: 171528.62<br />sale_price: 180000","pred.optimal: 297061.91<br />sale_price: 290000","pred.optimal: 385805.56<br />sale_price: 410000","pred.optimal: 246673.69<br />sale_price: 271500","pred.optimal: 129554.50<br />sale_price:  99500","pred.optimal: 141805.20<br />sale_price: 138500","pred.optimal: 126570.66<br />sale_price: 133000","pred.optimal: 186036.25<br />sale_price: 169000","pred.optimal: 233963.22<br />sale_price: 190000","pred.optimal: 300572.22<br />sale_price: 362500","pred.optimal: 161674.59<br />sale_price: 155000","pred.optimal: 161428.97<br />sale_price: 149500","pred.optimal: 160553.38<br />sale_price: 152000","pred.optimal: 172177.69<br />sale_price: 177500","pred.optimal: 147506.28<br />sale_price: 147110","pred.optimal: 193459.31<br />sale_price: 206000","pred.optimal: 173138.25<br />sale_price: 218500","pred.optimal: 195629.14<br />sale_price: 212500","pred.optimal: 149522.66<br />sale_price: 142250","pred.optimal: 154535.84<br />sale_price: 143000","pred.optimal: 142006.56<br />sale_price: 136300","pred.optimal: 171383.81<br />sale_price: 180500","pred.optimal: 170539.78<br />sale_price:  84900","pred.optimal: 139929.09<br />sale_price: 142125","pred.optimal: 208535.50<br />sale_price: 197600","pred.optimal: 134901.41<br />sale_price: 116500","pred.optimal: 138378.02<br />sale_price: 132000","pred.optimal: 259089.62<br />sale_price: 180000","pred.optimal: 132544.89<br />sale_price: 136000","pred.optimal: 176060.42<br />sale_price: 165500","pred.optimal: 153373.17<br />sale_price: 167500","pred.optimal: 138372.84<br />sale_price: 108538","pred.optimal: 116273.59<br />sale_price: 108000","pred.optimal: 127647.77<br />sale_price: 135000","pred.optimal: 139465.28<br />sale_price: 109000","pred.optimal: 104709.52<br />sale_price: 107500","pred.optimal: 131107.45<br />sale_price: 129000","pred.optimal: 125741.52<br />sale_price:  97500","pred.optimal: 156840.80<br />sale_price: 155000","pred.optimal: 117847.55<br />sale_price: 115000","pred.optimal: 132056.86<br />sale_price: 130000","pred.optimal: 164390.58<br />sale_price: 129000","pred.optimal: 123470.04<br />sale_price: 100000","pred.optimal: 161293.05<br />sale_price: 150000","pred.optimal: 134827.52<br />sale_price: 128500","pred.optimal: 129198.65<br />sale_price: 128000","pred.optimal: 130815.36<br />sale_price: 132000","pred.optimal: 120447.11<br />sale_price: 123900","pred.optimal: 116926.15<br />sale_price: 109500","pred.optimal: 133267.52<br />sale_price: 114900","pred.optimal: 130772.50<br />sale_price: 131500","pred.optimal: 148793.36<br />sale_price: 154000","pred.optimal: 154093.33<br />sale_price: 163000","pred.optimal: 265012.28<br />sale_price: 270000","pred.optimal: 146097.59<br />sale_price: 124000","pred.optimal: 128131.09<br />sale_price: 127000","pred.optimal: 173764.56<br />sale_price: 186000","pred.optimal: 252534.08<br />sale_price: 218000","pred.optimal: 224337.55<br />sale_price: 236000","pred.optimal: 154118.66<br />sale_price: 147000","pred.optimal: 251924.58<br />sale_price: 245350","pred.optimal: 193866.20<br />sale_price: 187000","pred.optimal: 138395.62<br />sale_price: 138500","pred.optimal: 155960.14<br />sale_price: 150000","pred.optimal: 130855.90<br />sale_price: 128200","pred.optimal: 308689.50<br />sale_price: 318000","pred.optimal: 144450.27<br />sale_price: 143000","pred.optimal: 182126.33<br />sale_price: 185500","pred.optimal:  97740.27<br />sale_price: 155891","pred.optimal: 109675.21<br />sale_price: 100000","pred.optimal:  88958.64<br />sale_price:  64000","pred.optimal:  85920.18<br />sale_price:  80000","pred.optimal: 124000.27<br />sale_price: 128000","pred.optimal:  62582.02<br />sale_price:  58500","pred.optimal: 164724.72<br />sale_price: 127000","pred.optimal: 164124.09<br />sale_price: 160000","pred.optimal: 181570.30<br />sale_price: 185000","pred.optimal:  95982.77<br />sale_price: 102776","pred.optimal:  91794.19<br />sale_price:  55993","pred.optimal:  85455.24<br />sale_price:  50138","pred.optimal: 164045.59<br />sale_price: 190000","pred.optimal: 159036.98<br />sale_price: 169900","pred.optimal: 187896.56<br />sale_price: 170000","pred.optimal: 224692.97<br />sale_price: 214900","pred.optimal:  92540.30<br />sale_price:  83500","pred.optimal: 127217.60<br />sale_price: 119500","pred.optimal:  79107.69<br />sale_price:  75500","pred.optimal: 168133.83<br />sale_price: 159000","pred.optimal: 164828.80<br />sale_price: 157000","pred.optimal: 181321.14<br />sale_price: 185000","pred.optimal: 188156.17<br />sale_price: 181316","pred.optimal: 161093.48<br />sale_price: 174000","pred.optimal: 478695.53<br />sale_price: 501837","pred.optimal: 353490.50<br />sale_price: 372500","pred.optimal: 189160.19<br />sale_price: 185000","pred.optimal: 166938.94<br />sale_price: 181000","pred.optimal: 190210.89<br />sale_price: 154000","pred.optimal: 180141.02<br />sale_price: 200000","pred.optimal: 182451.47<br />sale_price: 184000","pred.optimal: 158273.22<br />sale_price: 157000","pred.optimal: 142248.83<br />sale_price: 152000","pred.optimal: 190414.20<br />sale_price: 197500","pred.optimal: 233005.38<br />sale_price: 240900","pred.optimal: 150057.61<br />sale_price: 165000","pred.optimal: 100398.55<br />sale_price:  97000","pred.optimal: 115107.24<br />sale_price: 118000","pred.optimal: 121091.16<br />sale_price: 119500","pred.optimal: 132085.94<br />sale_price: 143750","pred.optimal: 141957.75<br />sale_price: 148500","pred.optimal: 138278.28<br />sale_price: 123000","pred.optimal: 139542.52<br />sale_price: 147000","pred.optimal: 142017.05<br />sale_price: 137900","pred.optimal: 165697.86<br />sale_price: 148800","pred.optimal: 340862.75<br />sale_price: 337500","pred.optimal: 503946.31<br />sale_price: 485000","pred.optimal: 468924.28<br />sale_price: 555000","pred.optimal: 320431.22<br />sale_price: 325000","pred.optimal: 258773.25<br />sale_price: 256300","pred.optimal: 371853.19<br />sale_price: 398800","pred.optimal: 503169.38<br />sale_price: 610000","pred.optimal: 320362.88<br />sale_price: 296000","pred.optimal: 489410.81<br />sale_price: 445000","pred.optimal: 306309.28<br />sale_price: 290000","pred.optimal: 195915.30<br />sale_price: 196000","pred.optimal: 198249.59<br />sale_price: 184500","pred.optimal: 207977.88<br />sale_price: 230000","pred.optimal: 398985.00<br />sale_price: 382500","pred.optimal: 243931.02<br />sale_price: 248500","pred.optimal: 214058.97<br />sale_price: 254000","pred.optimal: 169462.64<br />sale_price: 184000","pred.optimal: 178346.20<br />sale_price: 174000","pred.optimal: 189741.09<br />sale_price: 188500","pred.optimal: 192407.27<br />sale_price: 184100","pred.optimal: 207183.23<br />sale_price: 207500","pred.optimal: 185499.72<br />sale_price: 181000","pred.optimal: 218535.53<br />sale_price: 214000","pred.optimal: 301066.09<br />sale_price: 265000","pred.optimal: 313467.22<br />sale_price: 260000","pred.optimal: 259583.39<br />sale_price: 263550","pred.optimal: 253427.42<br />sale_price: 257500","pred.optimal: 252730.23<br />sale_price: 287090","pred.optimal: 215119.25<br />sale_price: 225000","pred.optimal: 356741.03<br />sale_price: 370878","pred.optimal: 225461.88<br />sale_price: 238500","pred.optimal: 231481.08<br />sale_price: 263000","pred.optimal: 182105.09<br />sale_price: 159000","pred.optimal: 136632.39<br />sale_price: 143500","pred.optimal: 201509.34<br />sale_price: 193000","pred.optimal: 158813.16<br />sale_price: 153000","pred.optimal: 215147.30<br />sale_price: 224243","pred.optimal: 197347.16<br />sale_price: 189000","pred.optimal: 194086.59<br />sale_price: 171500","pred.optimal: 131761.45<br />sale_price: 120000","pred.optimal: 164726.30<br />sale_price: 162000","pred.optimal:  69414.17<br />sale_price:  76000","pred.optimal: 119315.79<br />sale_price: 122000","pred.optimal: 166612.34<br />sale_price: 164500","pred.optimal: 201669.06<br />sale_price: 195000","pred.optimal: 159032.14<br />sale_price: 172500","pred.optimal: 202200.45<br />sale_price: 180500","pred.optimal: 151330.19<br />sale_price: 150000","pred.optimal: 148461.14<br />sale_price: 154000","pred.optimal: 158918.80<br />sale_price: 185000","pred.optimal: 219082.55<br />sale_price: 206000","pred.optimal: 192936.17<br />sale_price: 197900","pred.optimal: 131355.11<br />sale_price: 113000","pred.optimal: 221285.36<br />sale_price: 213250","pred.optimal: 167833.34<br />sale_price: 172500","pred.optimal: 148635.47<br />sale_price: 154000","pred.optimal: 193297.30<br />sale_price: 177500","pred.optimal: 140437.88<br />sale_price: 124500","pred.optimal: 117221.91<br />sale_price: 122000","pred.optimal: 127239.85<br />sale_price: 128900","pred.optimal: 145058.78<br />sale_price: 140000","pred.optimal: 122045.55<br />sale_price: 124000","pred.optimal: 165999.77<br />sale_price: 159000","pred.optimal: 226345.25<br />sale_price: 256000","pred.optimal: 147813.44<br />sale_price: 155000","pred.optimal: 127404.13<br />sale_price: 120000","pred.optimal: 139694.55<br />sale_price: 153000","pred.optimal: 180507.36<br />sale_price: 176000","pred.optimal: 131020.46<br />sale_price: 135000","pred.optimal: 134795.77<br />sale_price: 131000","pred.optimal: 123721.62<br />sale_price: 126000","pred.optimal: 129414.87<br />sale_price: 129900","pred.optimal: 100391.12<br />sale_price:  99900","pred.optimal: 122471.12<br />sale_price: 135000","pred.optimal: 147543.84<br />sale_price: 149000","pred.optimal: 162698.28<br />sale_price: 142900","pred.optimal: 159235.91<br />sale_price: 156500","pred.optimal:  60427.60<br />sale_price:  59000","pred.optimal:  87917.23<br />sale_price:  78500","pred.optimal: 204091.55<br />sale_price: 190000","pred.optimal: 198683.05<br />sale_price: 200000","pred.optimal: 151526.64<br />sale_price: 153000","pred.optimal: 182105.11<br />sale_price: 157500","pred.optimal:  93445.79<br />sale_price:  92900","pred.optimal: 120115.52<br />sale_price: 139000","pred.optimal: 139347.64<br />sale_price: 132500","pred.optimal: 136876.58<br />sale_price: 127000","pred.optimal: 106529.02<br />sale_price:  94550","pred.optimal: 109731.36<br />sale_price:  93000","pred.optimal:  92143.75<br />sale_price:  80000","pred.optimal:  96223.57<br />sale_price:  91300","pred.optimal: 112303.30<br />sale_price: 110000","pred.optimal: 141219.00<br />sale_price: 124900","pred.optimal:  51007.21<br />sale_price:  34900","pred.optimal: 134826.31<br />sale_price: 149000","pred.optimal: 123963.40<br />sale_price: 119000","pred.optimal: 108967.92<br />sale_price: 115000","pred.optimal: 178929.12<br />sale_price: 214500","pred.optimal: 136449.78<br />sale_price: 155000","pred.optimal: 168397.00<br />sale_price: 155000","pred.optimal: 144247.25<br />sale_price: 179900","pred.optimal:  82399.05<br />sale_price:  62500","pred.optimal:  75996.84<br />sale_price:  63000","pred.optimal: 142203.39<br />sale_price: 149900","pred.optimal: 139473.88<br />sale_price: 137000","pred.optimal: 108788.01<br />sale_price: 122000","pred.optimal:  91823.26<br />sale_price: 113000","pred.optimal: 139814.05<br />sale_price: 139500","pred.optimal: 147576.23<br />sale_price: 131000","pred.optimal: 109972.80<br />sale_price: 105000","pred.optimal: 196445.25<br />sale_price: 213000","pred.optimal: 219606.86<br />sale_price: 239900","pred.optimal: 170327.30<br />sale_price: 131000","pred.optimal: 130373.72<br />sale_price: 147983","pred.optimal: 262348.28<br />sale_price: 269500","pred.optimal: 311990.75<br />sale_price: 297000","pred.optimal: 340165.69<br />sale_price: 332000","pred.optimal: 282119.72<br />sale_price: 272500","pred.optimal: 266918.75<br />sale_price: 239000","pred.optimal: 233105.81<br />sale_price: 221800","pred.optimal: 155296.30<br />sale_price: 145000","pred.optimal: 184340.03<br />sale_price: 195000","pred.optimal: 216493.92<br />sale_price: 227000","pred.optimal: 215416.77<br />sale_price: 230000","pred.optimal: 182522.92<br />sale_price: 187100","pred.optimal: 124220.23<br />sale_price: 124000","pred.optimal: 132183.30<br />sale_price: 140000","pred.optimal: 159623.47<br />sale_price: 150500","pred.optimal: 135198.77<br />sale_price: 136500","pred.optimal: 156089.53<br />sale_price: 143500","pred.optimal: 132050.45<br />sale_price: 133500","pred.optimal: 130047.84<br />sale_price: 133900","pred.optimal: 139629.64<br />sale_price: 133000","pred.optimal: 236420.64<br />sale_price: 250000","pred.optimal: 312305.38<br />sale_price: 313000","pred.optimal: 189984.23<br />sale_price: 198500","pred.optimal: 201520.61<br />sale_price: 211000","pred.optimal: 271926.88<br />sale_price: 279500","pred.optimal: 189037.64<br />sale_price: 191000","pred.optimal: 177784.48<br />sale_price: 178000","pred.optimal: 108379.18<br />sale_price: 100000","pred.optimal: 120775.19<br />sale_price: 127000","pred.optimal: 112632.07<br />sale_price: 118000","pred.optimal: 102262.11<br />sale_price:  85000","pred.optimal:  81681.06<br />sale_price:  99900","pred.optimal: 124657.15<br />sale_price: 119900","pred.optimal: 130118.87<br />sale_price: 103500","pred.optimal: 116582.81<br />sale_price: 160000","pred.optimal: 147097.33<br />sale_price: 139500","pred.optimal: 118145.34<br />sale_price: 105000","pred.optimal: 189893.97<br />sale_price: 177000","pred.optimal: 246364.11<br />sale_price: 234000","pred.optimal: 175892.86<br />sale_price: 205000","pred.optimal: 138325.58<br />sale_price: 154900","pred.optimal: 226791.06<br />sale_price: 224000","pred.optimal: 122915.22<br />sale_price: 121000","pred.optimal: 290950.16<br />sale_price: 230000","pred.optimal:  48078.32<br />sale_price:  57625","pred.optimal: 244371.94<br />sale_price: 251000","pred.optimal: 233353.75<br />sale_price: 240000","pred.optimal: 214170.64<br />sale_price: 215000","pred.optimal: 166382.53<br />sale_price: 152000","pred.optimal: 403252.00<br />sale_price: 410000","pred.optimal: 330932.56<br />sale_price: 316500","pred.optimal: 220482.64<br />sale_price: 201000","pred.optimal: 234959.84<br />sale_price: 213500","pred.optimal: 134576.36<br />sale_price: 139500","pred.optimal: 160459.64<br />sale_price: 162000","pred.optimal:  97883.73<br />sale_price:  86000","pred.optimal: 134661.59<br />sale_price: 131250","pred.optimal: 122597.83<br />sale_price: 112000","pred.optimal: 146224.25<br />sale_price: 130000","pred.optimal: 200921.41<br />sale_price: 173000","pred.optimal: 169677.89<br />sale_price: 165000","pred.optimal: 204703.00<br />sale_price: 192000","pred.optimal: 177710.95<br />sale_price: 180000","pred.optimal: 175968.98<br />sale_price: 181000","pred.optimal: 167880.77<br />sale_price: 183000","pred.optimal: 186159.23<br />sale_price: 185000","pred.optimal: 195615.22<br />sale_price: 189000","pred.optimal: 335472.25<br />sale_price: 355000","pred.optimal: 323513.47<br />sale_price: 325000","pred.optimal: 369938.34<br />sale_price: 387000","pred.optimal: 184427.58<br />sale_price: 175500","pred.optimal: 182198.41<br />sale_price: 181900","pred.optimal: 148816.08<br />sale_price: 175000","pred.optimal: 319800.56<br />sale_price: 306000","pred.optimal: 140437.91<br />sale_price: 133000","pred.optimal: 123542.35<br />sale_price: 123000","pred.optimal: 172410.59<br />sale_price: 181000","pred.optimal:  94020.92<br />sale_price: 103400","pred.optimal: 102221.51<br />sale_price: 100500","pred.optimal: 357636.59<br />sale_price: 394617","pred.optimal: 465650.78<br />sale_price: 417500","pred.optimal: 363907.97<br />sale_price: 379000","pred.optimal: 278747.41<br />sale_price: 250000","pred.optimal: 394274.53<br />sale_price: 412500","pred.optimal: 366574.06<br />sale_price: 421250","pred.optimal: 204926.20<br />sale_price: 219500","pred.optimal: 182375.75<br />sale_price: 154000","pred.optimal: 202610.75<br />sale_price: 207000","pred.optimal: 235066.41<br />sale_price: 195000","pred.optimal: 192037.44<br />sale_price: 191000","pred.optimal: 182320.23<br />sale_price: 179000","pred.optimal: 219121.33<br />sale_price: 226750","pred.optimal: 373596.69<br />sale_price: 350000","pred.optimal: 268824.75<br />sale_price: 264132","pred.optimal: 207962.92<br />sale_price: 227680","pred.optimal: 215675.34<br />sale_price: 182000","pred.optimal: 256638.08<br />sale_price: 250000","pred.optimal: 313613.12<br />sale_price: 339750","pred.optimal: 249111.12<br />sale_price: 256000","pred.optimal: 206215.16<br />sale_price: 207500","pred.optimal: 178793.27<br />sale_price: 192500","pred.optimal: 182867.12<br />sale_price: 182000","pred.optimal: 203029.47<br />sale_price: 193500","pred.optimal: 198493.98<br />sale_price: 189000","pred.optimal: 169776.34<br />sale_price: 179200","pred.optimal: 161832.77<br />sale_price: 153900","pred.optimal: 136664.45<br />sale_price: 135000","pred.optimal: 130733.63<br />sale_price: 142000","pred.optimal: 182827.25<br />sale_price: 192000","pred.optimal: 137692.25<br />sale_price: 140000","pred.optimal: 354421.66<br />sale_price: 372500","pred.optimal: 163707.77<br />sale_price: 179400","pred.optimal: 253430.64<br />sale_price: 290000","pred.optimal: 273379.16<br />sale_price: 305000","pred.optimal: 229426.81<br />sale_price: 217500","pred.optimal: 333305.31<br />sale_price: 150000","pred.optimal: 203135.50<br />sale_price: 205000","pred.optimal: 203774.31<br />sale_price: 215000","pred.optimal: 152387.12<br />sale_price: 147000","pred.optimal: 145780.23<br />sale_price: 135000","pred.optimal: 316436.47<br />sale_price: 299800","pred.optimal: 166200.92<br />sale_price: 173000","pred.optimal: 175294.73<br />sale_price: 178400","pred.optimal: 119723.26<br />sale_price: 135750","pred.optimal: 146908.80<br />sale_price: 155000","pred.optimal: 159910.20<br />sale_price: 180500","pred.optimal: 155847.88<br />sale_price: 174900","pred.optimal: 129731.25<br />sale_price: 140000","pred.optimal: 150280.80<br />sale_price: 145500","pred.optimal: 145198.00<br />sale_price: 142000","pred.optimal: 121750.80<br />sale_price: 119000","pred.optimal: 219622.17<br />sale_price: 201800","pred.optimal: 156965.92<br />sale_price: 165000","pred.optimal: 231819.14<br />sale_price: 227000","pred.optimal: 160991.94<br />sale_price: 163000","pred.optimal: 133308.84<br />sale_price: 133000","pred.optimal: 115209.18<br />sale_price: 116000","pred.optimal: 107105.43<br />sale_price: 130000","pred.optimal: 104116.33<br />sale_price: 110000","pred.optimal: 126222.26<br />sale_price: 100000","pred.optimal: 115286.02<br />sale_price: 118500","pred.optimal:  77970.47<br />sale_price:  89900","pred.optimal: 163085.81<br />sale_price: 171000","pred.optimal: 132251.14<br />sale_price: 138000","pred.optimal: 115321.58<br />sale_price: 112500","pred.optimal: 110804.41<br />sale_price: 105500","pred.optimal: 123665.35<br />sale_price: 127500","pred.optimal: 126486.63<br />sale_price: 136870","pred.optimal: 131926.89<br />sale_price: 122600","pred.optimal:  77038.43<br />sale_price:  64000","pred.optimal: 130729.27<br />sale_price: 139500","pred.optimal: 103064.74<br />sale_price:  95000","pred.optimal: 148071.58<br />sale_price: 147000","pred.optimal: 134216.52<br />sale_price: 140000","pred.optimal:  53917.54<br />sale_price:  46500","pred.optimal: 109783.66<br />sale_price: 107900","pred.optimal: 102840.60<br />sale_price:  65000","pred.optimal: 118769.67<br />sale_price: 132000","pred.optimal:  90140.62<br />sale_price:  98000","pred.optimal: 139457.47<br />sale_price: 129400","pred.optimal: 152949.34<br />sale_price: 163000","pred.optimal: 131515.11<br />sale_price: 128000","pred.optimal: 117237.59<br />sale_price: 116900","pred.optimal:  73413.41<br />sale_price:  55000","pred.optimal: 195672.27<br />sale_price: 184000","pred.optimal: 145431.47<br />sale_price: 138000","pred.optimal: 141225.11<br />sale_price: 108000","pred.optimal: 107372.23<br />sale_price:  79500","pred.optimal: 147150.42<br />sale_price: 153000","pred.optimal: 123158.09<br />sale_price: 105000","pred.optimal: 115735.45<br />sale_price: 113000","pred.optimal:  78487.42<br />sale_price:  81300","pred.optimal: 151174.19<br />sale_price: 162900","pred.optimal: 194004.33<br />sale_price: 207000","pred.optimal: 123368.13<br />sale_price: 130000","pred.optimal: 117507.15<br />sale_price: 127500","pred.optimal: 116050.85<br />sale_price: 120000","pred.optimal: 125682.95<br />sale_price: 127500","pred.optimal: 105146.27<br />sale_price:  89500","pred.optimal: 120799.99<br />sale_price: 125000","pred.optimal: 104629.38<br />sale_price:  79900","pred.optimal: 116881.52<br />sale_price: 124000","pred.optimal: 115892.00<br />sale_price: 109900","pred.optimal: 139384.27<br />sale_price: 145000","pred.optimal: 147891.09<br />sale_price: 153000","pred.optimal: 176121.41<br />sale_price: 185000","pred.optimal: 106935.62<br />sale_price: 108000","pred.optimal: 152457.81<br />sale_price: 152400","pred.optimal: 160460.34<br />sale_price: 144000","pred.optimal: 199565.45<br />sale_price: 241500","pred.optimal: 165896.38<br />sale_price: 177000","pred.optimal: 148154.28<br />sale_price: 155000","pred.optimal: 207830.39<br />sale_price: 235000","pred.optimal: 118626.29<br />sale_price: 125000","pred.optimal: 244537.42<br />sale_price: 262280","pred.optimal: 223760.91<br />sale_price: 225000","pred.optimal: 175129.12<br />sale_price: 177439","pred.optimal: 241441.81<br />sale_price: 248500","pred.optimal: 199805.61<br />sale_price: 207500","pred.optimal: 208373.62<br />sale_price: 193000","pred.optimal: 186915.98<br />sale_price: 188000","pred.optimal: 226950.73<br />sale_price: 221000","pred.optimal: 210998.58<br />sale_price: 231500","pred.optimal: 154750.47<br />sale_price: 158000","pred.optimal: 121598.57<br />sale_price: 127000","pred.optimal: 235729.44<br />sale_price: 230000","pred.optimal: 263923.97<br />sale_price: 287000","pred.optimal: 279993.09<br />sale_price: 274000","pred.optimal: 233299.44<br />sale_price: 240000","pred.optimal: 189527.36<br />sale_price: 183000","pred.optimal: 141543.56<br />sale_price: 136500","pred.optimal: 202388.12<br />sale_price: 210000","pred.optimal: 148372.83<br />sale_price: 149300","pred.optimal: 118823.03<br />sale_price: 108000","pred.optimal: 184482.92<br />sale_price: 165400","pred.optimal: 140780.61<br />sale_price: 148000","pred.optimal:  76623.12<br />sale_price:  82500","pred.optimal: 119225.55<br />sale_price:  99000","pred.optimal:  86211.47<br />sale_price:  98000","pred.optimal: 189470.84<br />sale_price: 179500","pred.optimal: 156046.11<br />sale_price: 136500","pred.optimal: 143180.64<br />sale_price: 168000","pred.optimal: 118756.88<br />sale_price: 130000","pred.optimal: 176997.67<br />sale_price: 161900","pred.optimal: 172568.70<br />sale_price: 177000","pred.optimal: 126951.52<br />sale_price: 110000","pred.optimal: 257460.75<br />sale_price: 263400","pred.optimal: 133333.69<br />sale_price: 126000","pred.optimal: 110698.52<br />sale_price:  99500","pred.optimal: 295261.28<br />sale_price: 392500","pred.optimal: 269972.69<br />sale_price: 271000","pred.optimal: 212724.77<br />sale_price: 213000","pred.optimal: 204302.67<br />sale_price: 228500","pred.optimal: 237609.83<br />sale_price: 228950","pred.optimal: 211695.27<br />sale_price: 241500","pred.optimal: 281012.44<br />sale_price: 287000","pred.optimal: 263084.50<br />sale_price: 294000","pred.optimal: 250197.53<br />sale_price: 264966","pred.optimal: 166674.45<br />sale_price: 167500","pred.optimal: 219817.56<br />sale_price: 218689","pred.optimal: 231291.36<br />sale_price: 195000","pred.optimal: 325964.19<br />sale_price: 305000","pred.optimal: 269582.91<br />sale_price: 298751","pred.optimal: 330993.69<br />sale_price: 370000","pred.optimal: 128758.84<br />sale_price: 124000","pred.optimal: 109638.28<br />sale_price: 115000","pred.optimal: 140177.34<br />sale_price: 152500","pred.optimal: 112310.17<br />sale_price:  98000","pred.optimal:  85464.65<br />sale_price:  81000","pred.optimal: 142182.59<br />sale_price: 138000","pred.optimal: 132086.17<br />sale_price: 103000","pred.optimal: 196387.69<br />sale_price: 225000","pred.optimal: 170528.44<br />sale_price: 168000","pred.optimal: 160415.05<br />sale_price: 160000","pred.optimal: 162803.83<br />sale_price: 160000","pred.optimal: 343199.78<br />sale_price: 349265","pred.optimal: 236800.00<br />sale_price: 392000","pred.optimal: 379497.12<br />sale_price: 441929","pred.optimal: 195962.94<br />sale_price: 192000","pred.optimal: 147521.78<br />sale_price: 148000","pred.optimal: 180318.95<br />sale_price: 197000","pred.optimal: 143471.78<br />sale_price: 152000","pred.optimal: 119410.60<br />sale_price: 123000","pred.optimal: 116043.02<br />sale_price: 120500","pred.optimal: 104776.05<br />sale_price: 113500","pred.optimal: 140681.89<br />sale_price: 142500","pred.optimal: 371284.75<br />sale_price: 356000","pred.optimal: 381954.84<br />sale_price: 314813","pred.optimal: 271368.88<br />sale_price: 318000","pred.optimal: 286709.88<br />sale_price: 322400","pred.optimal: 320856.53<br />sale_price: 318000","pred.optimal: 317895.94<br />sale_price: 338931","pred.optimal: 496971.28<br />sale_price: 479069","pred.optimal: 283634.12<br />sale_price: 260116","pred.optimal: 298690.38<br />sale_price: 317000","pred.optimal: 292908.69<br />sale_price: 285000","pred.optimal: 254474.44<br />sale_price: 250000","pred.optimal: 187748.14<br />sale_price: 194700","pred.optimal: 190408.89<br />sale_price: 204000","pred.optimal: 189984.95<br />sale_price: 200000","pred.optimal: 196941.73<br />sale_price: 207000","pred.optimal: 221516.98<br />sale_price: 209500","pred.optimal: 279625.94<br />sale_price: 277500","pred.optimal: 300599.06<br />sale_price: 318061","pred.optimal: 179451.80<br />sale_price: 178900","pred.optimal: 176547.67<br />sale_price: 168165","pred.optimal: 176663.98<br />sale_price: 171925","pred.optimal: 220071.81<br />sale_price: 198444","pred.optimal: 194520.31<br />sale_price: 181134","pred.optimal: 170503.31<br />sale_price: 156932","pred.optimal: 197813.81<br />sale_price: 172500","pred.optimal: 237195.39<br />sale_price: 226500","pred.optimal: 244479.17<br />sale_price: 259000","pred.optimal: 180147.53<br />sale_price: 188500","pred.optimal: 169322.06<br />sale_price: 165500","pred.optimal: 207027.69<br />sale_price: 211000","pred.optimal: 567872.00<br />sale_price: 745000","pred.optimal: 313987.06<br />sale_price: 322500","pred.optimal: 296083.44<br />sale_price: 290000","pred.optimal: 399155.47<br />sale_price: 419005","pred.optimal: 313675.94<br />sale_price: 147000","pred.optimal: 341440.84<br />sale_price: 311872","pred.optimal: 234843.05<br />sale_price: 250000","pred.optimal: 145826.19<br />sale_price: 147000","pred.optimal: 186825.62<br />sale_price: 181000","pred.optimal: 190970.41<br />sale_price: 201000","pred.optimal: 162419.73<br />sale_price: 179000","pred.optimal: 135268.80<br />sale_price: 128000","pred.optimal: 124727.58<br />sale_price: 125500","pred.optimal: 275641.88<br />sale_price: 300000","pred.optimal: 142164.00<br />sale_price: 129000","pred.optimal: 309569.66<br />sale_price: 285000","pred.optimal: 187713.86<br />sale_price: 166000","pred.optimal: 230297.61<br />sale_price: 193800","pred.optimal: 192966.28<br />sale_price: 208900","pred.optimal: 219302.25<br />sale_price: 207500","pred.optimal: 176008.34<br />sale_price: 177000","pred.optimal: 255229.72<br />sale_price: 239000","pred.optimal: 277408.59<br />sale_price: 301000","pred.optimal: 187596.17<br />sale_price: 194000","pred.optimal: 201146.38<br />sale_price: 213750","pred.optimal: 175574.69<br />sale_price: 187000","pred.optimal: 193799.75<br />sale_price: 190000","pred.optimal: 239388.22<br />sale_price: 226000","pred.optimal: 162650.77<br />sale_price: 164000","pred.optimal: 193892.25<br />sale_price: 188500","pred.optimal: 244522.31<br />sale_price: 255000","pred.optimal: 148426.39<br />sale_price: 156000","pred.optimal: 132863.70<br />sale_price: 139900","pred.optimal: 140861.33<br />sale_price: 151500","pred.optimal: 133945.61<br />sale_price: 145000","pred.optimal: 128057.56<br />sale_price: 124000","pred.optimal: 151114.39<br />sale_price: 140500","pred.optimal: 143514.56<br />sale_price: 147000","pred.optimal:  64417.50<br />sale_price:  64000","pred.optimal: 131830.34<br />sale_price: 137000","pred.optimal: 128350.49<br />sale_price: 105000","pred.optimal: 127487.48<br />sale_price: 126000","pred.optimal: 155927.67<br />sale_price: 175000","pred.optimal: 135072.38<br />sale_price: 144000","pred.optimal: 136356.67<br />sale_price: 141000","pred.optimal: 132716.48<br />sale_price: 120000","pred.optimal: 162981.75<br />sale_price: 163500","pred.optimal: 144008.09<br />sale_price: 142000","pred.optimal: 140032.08<br />sale_price: 134500","pred.optimal: 123294.19<br />sale_price: 115000","pred.optimal: 148176.12<br />sale_price: 153000","pred.optimal: 130698.70<br />sale_price: 135000","pred.optimal: 282161.84<br />sale_price: 301600","pred.optimal: 114934.52<br />sale_price: 109000","pred.optimal: 129396.80<br />sale_price: 128500","pred.optimal: 106450.23<br />sale_price:  64500","pred.optimal: 107172.87<br />sale_price: 102000","pred.optimal: 145042.58<br />sale_price: 152000","pred.optimal: 127731.39<br />sale_price: 141000","pred.optimal: 111664.62<br />sale_price:  89471","pred.optimal:  89753.67<br />sale_price: 108500","pred.optimal: 113267.39<br />sale_price: 114000","pred.optimal: 126494.28<br />sale_price: 124500","pred.optimal:  92214.66<br />sale_price: 104500","pred.optimal: 131224.98<br />sale_price: 137500","pred.optimal: 107158.05<br />sale_price: 102000","pred.optimal: 119548.56<br />sale_price:  90000","pred.optimal: 146179.78<br />sale_price: 153575","pred.optimal:  74055.35<br />sale_price: 113000","pred.optimal: 154805.64<br />sale_price: 169500","pred.optimal: 130022.58<br />sale_price: 139400","pred.optimal: 125243.80<br />sale_price: 127000","pred.optimal: 125761.78<br />sale_price: 128500","pred.optimal: 129285.64<br />sale_price: 122000","pred.optimal: 258356.16<br />sale_price: 200500","pred.optimal: 114463.51<br />sale_price: 126000","pred.optimal: 113479.60<br />sale_price: 118500","pred.optimal: 159887.75<br />sale_price: 165000","pred.optimal: 124717.40<br />sale_price: 123000","pred.optimal: 115747.64<br />sale_price: 108000","pred.optimal: 127322.97<br />sale_price: 119900","pred.optimal: 117471.23<br />sale_price: 115000","pred.optimal: 124556.89<br />sale_price: 134500","pred.optimal: 130945.36<br />sale_price: 129000","pred.optimal: 169585.34<br />sale_price: 112000","pred.optimal: 154066.41<br />sale_price: 165250","pred.optimal: 156753.30<br />sale_price: 150000","pred.optimal: 152819.30<br />sale_price: 137000","pred.optimal: 147973.38<br />sale_price: 130000","pred.optimal:  81538.06<br />sale_price: 109900","pred.optimal: 213754.58<br />sale_price: 243000","pred.optimal: 107679.66<br />sale_price: 118000","pred.optimal: 146621.14<br />sale_price: 150000","pred.optimal:  94879.81<br />sale_price:  86000","pred.optimal: 137719.80<br />sale_price: 130000","pred.optimal:  95878.44<br />sale_price:  96000","pred.optimal: 181420.80<br />sale_price: 228500","pred.optimal: 195153.44<br />sale_price: 179900","pred.optimal: 280939.03<br />sale_price: 301000","pred.optimal: 223674.28<br />sale_price: 220000","pred.optimal: 186907.66<br />sale_price: 200141","pred.optimal: 259747.11<br />sale_price: 246500","pred.optimal: 206188.22<br />sale_price: 203000","pred.optimal: 215375.48<br />sale_price: 212999","pred.optimal: 178614.39<br />sale_price: 176432","pred.optimal: 282516.00<br />sale_price: 277000","pred.optimal: 213788.12<br />sale_price: 187500","pred.optimal: 208440.19<br />sale_price: 204750","pred.optimal: 186690.91<br />sale_price: 190550","pred.optimal: 193961.16<br />sale_price: 200000","pred.optimal: 234467.83<br />sale_price: 211000","pred.optimal: 142548.48<br />sale_price: 131500","pred.optimal: 189085.64<br />sale_price: 185000","pred.optimal: 182018.83<br />sale_price: 179400","pred.optimal: 177825.73<br />sale_price: 168675","pred.optimal: 175121.20<br />sale_price: 167000","pred.optimal: 131258.67<br />sale_price: 118500","pred.optimal: 135809.05<br />sale_price: 133500","pred.optimal: 169713.72<br />sale_price: 171500","pred.optimal: 143398.14<br />sale_price: 140000","pred.optimal: 125783.52<br />sale_price: 131750","pred.optimal:  89397.53<br />sale_price:  79000","pred.optimal: 105047.15<br />sale_price: 110000","pred.optimal: 140787.62<br />sale_price: 148500","pred.optimal: 142991.55<br />sale_price: 139000","pred.optimal: 196122.61<br />sale_price: 238000","pred.optimal: 144291.38<br />sale_price: 140000","pred.optimal: 294967.97<br />sale_price: 315000","pred.optimal: 209514.73<br />sale_price: 215000","pred.optimal: 122692.05<br />sale_price: 123900","pred.optimal: 201533.97<br />sale_price: 170000","pred.optimal: 125568.16<br />sale_price: 115000","pred.optimal: 134748.77<br />sale_price: 129850","pred.optimal: 165395.86<br />sale_price: 150909","pred.optimal: 129255.43<br />sale_price: 118400","pred.optimal:  61046.20<br />sale_price:  68104","pred.optimal: 287144.28<br />sale_price: 375000","pred.optimal: 168954.12<br />sale_price: 183500","pred.optimal: 140323.50<br />sale_price: 134000","pred.optimal: 220513.38<br />sale_price: 245000","pred.optimal: 201368.56<br />sale_price: 210000","pred.optimal: 232433.22<br />sale_price: 244000","pred.optimal: 243802.92<br />sale_price: 267300","pred.optimal: 172565.67<br />sale_price: 174000","pred.optimal: 338061.84<br />sale_price: 392000","pred.optimal: 295648.81<br />sale_price: 281213","pred.optimal:  83466.57<br />sale_price:  85500","pred.optimal:  87363.32<br />sale_price:  93900","pred.optimal:  78508.99<br />sale_price:  75190","pred.optimal: 185133.03<br />sale_price: 196000","pred.optimal: 131600.70<br />sale_price: 129500","pred.optimal: 126639.10<br />sale_price: 129500","pred.optimal: 180214.73<br />sale_price: 192100","pred.optimal: 183871.64<br />sale_price: 185000","pred.optimal: 294832.81<br />sale_price: 320000","pred.optimal: 241543.69<br />sale_price: 250000","pred.optimal: 241958.05<br />sale_price: 274725","pred.optimal: 179477.05<br />sale_price: 165600","pred.optimal: 425047.34<br />sale_price: 457347","pred.optimal: 493195.03<br />sale_price: 545224","pred.optimal: 460147.00<br />sale_price: 535000","pred.optimal: 406190.44<br />sale_price: 438780","pred.optimal: 180380.97<br />sale_price: 169000","pred.optimal: 316014.84<br />sale_price: 318000","pred.optimal: 429645.78<br />sale_price: 470000","pred.optimal: 241540.61<br />sale_price: 235000","pred.optimal: 213910.89<br />sale_price: 241500","pred.optimal: 295853.28<br />sale_price: 250000","pred.optimal: 168216.67<br />sale_price: 179900","pred.optimal: 267411.53<br />sale_price: 294323","pred.optimal: 176933.06<br />sale_price: 181000","pred.optimal: 151952.47<br />sale_price: 155000","pred.optimal: 133082.52<br />sale_price: 138800","pred.optimal: 107971.30<br />sale_price:  97500","pred.optimal:  94553.25<br />sale_price:  91500","pred.optimal:  98655.42<br />sale_price:  89000","pred.optimal: 153615.06<br />sale_price: 145000","pred.optimal: 423206.81<br />sale_price: 412083","pred.optimal: 244889.67<br />sale_price: 252000","pred.optimal: 275170.00<br />sale_price: 293000","pred.optimal: 404508.66<br />sale_price: 415000","pred.optimal: 313883.91<br />sale_price: 300000","pred.optimal: 274664.56<br />sale_price: 275500","pred.optimal: 365766.31<br />sale_price: 345000","pred.optimal: 303676.28<br />sale_price: 298236","pred.optimal: 358861.06<br />sale_price: 360000","pred.optimal: 190505.53<br />sale_price: 202500","pred.optimal: 344049.56<br />sale_price: 332200","pred.optimal: 179210.88<br />sale_price: 169990","pred.optimal: 177157.72<br />sale_price: 169985","pred.optimal: 171499.91<br />sale_price: 172785","pred.optimal: 284833.91<br />sale_price: 258000","pred.optimal: 195476.66<br />sale_price: 234000","pred.optimal: 273309.81<br />sale_price: 264561","pred.optimal: 191171.02<br />sale_price: 173000","pred.optimal: 326439.84<br />sale_price: 350000","pred.optimal: 328552.53<br />sale_price: 321000","pred.optimal: 255487.88<br />sale_price: 252000","pred.optimal: 297300.78<br />sale_price: 290000","pred.optimal: 192205.05<br />sale_price: 186500","pred.optimal: 137542.78<br />sale_price: 132000","pred.optimal: 145061.88<br />sale_price: 142500","pred.optimal: 154506.72<br />sale_price: 150000","pred.optimal: 133579.91<br />sale_price: 125000","pred.optimal: 116389.37<br />sale_price: 116000","pred.optimal: 129590.66<br />sale_price: 137000","pred.optimal: 314293.22<br />sale_price: 310000","pred.optimal: 258662.61<br />sale_price: 262000","pred.optimal: 153029.66<br />sale_price: 147400","pred.optimal: 168997.03<br />sale_price: 183900","pred.optimal: 145095.33<br />sale_price: 139000","pred.optimal: 203091.14<br />sale_price: 200000","pred.optimal: 190436.73<br />sale_price: 170000","pred.optimal: 132457.44<br />sale_price: 135500","pred.optimal: 141434.83<br />sale_price: 157500","pred.optimal: 160205.61<br />sale_price: 146000","pred.optimal: 192404.47<br />sale_price: 190000","pred.optimal: 121600.67<br />sale_price: 129900","pred.optimal: 156067.81<br />sale_price: 157500","pred.optimal: 127062.53<br />sale_price: 140000","pred.optimal: 125856.70<br />sale_price: 127000","pred.optimal: 140398.91<br />sale_price: 146500","pred.optimal: 124180.96<br />sale_price: 121000","pred.optimal: 280587.03<br />sale_price: 235000","pred.optimal: 145892.67<br />sale_price: 143000","pred.optimal: 141591.78<br />sale_price: 145250","pred.optimal: 138770.09<br />sale_price: 156000","pred.optimal: 217428.86<br />sale_price: 167000","pred.optimal: 146333.73<br />sale_price: 159000","pred.optimal: 297376.12<br />sale_price: 180000","pred.optimal: 106922.60<br />sale_price: 105000","pred.optimal: 170570.55<br />sale_price: 159000","pred.optimal: 146087.58<br />sale_price: 147000","pred.optimal: 115117.44<br />sale_price: 115000","pred.optimal: 145515.83<br />sale_price: 159500","pred.optimal: 115143.77<br />sale_price: 120000","pred.optimal: 186823.31<br />sale_price: 183000","pred.optimal: 153022.22<br />sale_price: 157500","pred.optimal: 278870.75<br />sale_price: 277500","pred.optimal: 211741.34<br />sale_price: 207500","pred.optimal: 131048.69<br />sale_price: 147500","pred.optimal:  77560.90<br />sale_price: 135000","pred.optimal:  89190.05<br />sale_price:  87500","pred.optimal: 136220.31<br />sale_price: 146000","pred.optimal: 137051.03<br />sale_price: 120000","pred.optimal: 117955.51<br />sale_price: 124000","pred.optimal: 172122.05<br />sale_price: 169000","pred.optimal: 139933.86<br />sale_price: 156500","pred.optimal: 167321.64<br />sale_price: 178000","pred.optimal: 101342.18<br />sale_price: 105000","pred.optimal: 131426.48<br />sale_price: 111500","pred.optimal:  94333.70<br />sale_price: 108000","pred.optimal: 116663.32<br />sale_price:  96900","pred.optimal: 169587.91<br />sale_price: 155000","pred.optimal: 149415.78<br />sale_price: 144000","pred.optimal: 144640.00<br />sale_price: 157000","pred.optimal:  64047.88<br />sale_price:  64500","pred.optimal: 152577.09<br />sale_price: 159000","pred.optimal: 125785.92<br />sale_price: 114500","pred.optimal:  97703.34<br />sale_price:  88000","pred.optimal: 129268.63<br />sale_price: 149000","pred.optimal:  97724.74<br />sale_price:  89000","pred.optimal: 103458.33<br />sale_price: 109000","pred.optimal: 214828.06<br />sale_price: 220000","pred.optimal: 124432.52<br />sale_price: 116500","pred.optimal: 142277.41<br />sale_price: 148000","pred.optimal: 132651.64<br />sale_price: 142500","pred.optimal: 166702.75<br />sale_price: 180000","pred.optimal: 158844.12<br />sale_price: 156500","pred.optimal: 133345.50<br />sale_price: 157000","pred.optimal: 134796.20<br />sale_price: 145000","pred.optimal: 164449.97<br />sale_price: 168000","pred.optimal: 161876.11<br />sale_price: 164000","pred.optimal: 132920.97<br />sale_price: 130000","pred.optimal: 143849.05<br />sale_price: 142500","pred.optimal: 137230.47<br />sale_price: 136900","pred.optimal: 131140.14<br />sale_price: 149900","pred.optimal: 226274.89<br />sale_price: 209000","pred.optimal: 235905.27<br />sale_price: 221500","pred.optimal: 256967.27<br />sale_price: 233555","pred.optimal: 239647.78<br />sale_price: 260000","pred.optimal: 267880.34<br />sale_price: 294900","pred.optimal: 215708.73<br />sale_price: 209700","pred.optimal: 223279.72<br />sale_price: 220000","pred.optimal: 151737.61<br />sale_price: 145000","pred.optimal: 194959.12<br />sale_price: 193000","pred.optimal: 210324.39<br />sale_price: 217000","pred.optimal: 221314.58<br />sale_price: 217000","pred.optimal: 225817.41<br />sale_price: 205000","pred.optimal: 133013.39<br />sale_price: 132500","pred.optimal: 150876.92<br />sale_price: 157500","pred.optimal: 135448.16<br />sale_price: 128500","pred.optimal: 277078.28<br />sale_price: 275000","pred.optimal: 255182.09<br />sale_price: 230000","pred.optimal: 215176.59<br />sale_price: 210900","pred.optimal: 239893.95<br />sale_price: 274300","pred.optimal: 237239.84<br />sale_price: 216837","pred.optimal: 143125.50<br />sale_price: 133000","pred.optimal: 147443.03<br />sale_price: 155900","pred.optimal: 240000.36<br />sale_price: 233230","pred.optimal: 196979.02<br />sale_price: 203160","pred.optimal: 117273.51<br />sale_price:  98000","pred.optimal: 140370.91<br />sale_price: 145000","pred.optimal: 146548.11<br />sale_price: 140000","pred.optimal: 142440.81<br />sale_price: 130000","pred.optimal: 142487.91<br />sale_price: 137500","pred.optimal:  98286.47<br />sale_price:  92000","pred.optimal:  97476.66<br />sale_price: 107000","pred.optimal: 133302.34<br />sale_price: 104900","pred.optimal: 120465.03<br />sale_price: 125000","pred.optimal:  97600.93<br />sale_price: 121000","pred.optimal: 107850.92<br />sale_price: 128000","pred.optimal: 117131.56<br />sale_price: 102000","pred.optimal: 145921.83<br />sale_price: 131000","pred.optimal: 193662.00<br />sale_price: 140000","pred.optimal: 255313.36<br />sale_price: 250000","pred.optimal: 197672.48<br />sale_price: 218000","pred.optimal: 229106.81<br />sale_price: 239000","pred.optimal: 242246.97<br />sale_price: 257000","pred.optimal:  94497.48<br />sale_price: 102000","pred.optimal:  45369.69<br />sale_price:  72000","pred.optimal:  91805.28<br />sale_price: 106500","pred.optimal:  67834.95<br />sale_price:  78000","pred.optimal: 222752.06<br />sale_price: 228000","pred.optimal: 171263.27<br />sale_price: 176000","pred.optimal: 214494.75<br />sale_price: 250000","pred.optimal: 194313.94<br />sale_price: 202000","pred.optimal: 317802.72<br />sale_price: 312500","pred.optimal: 202277.80<br />sale_price: 215000","pred.optimal: 160477.19<br />sale_price: 164000","pred.optimal:  78144.11<br />sale_price:  71000","pred.optimal: 167069.58<br />sale_price: 131000","pred.optimal: 143612.41<br />sale_price: 142500","pred.optimal: 208354.78<br />sale_price: 188000"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[45369.69140625,51983.6446795886,58597.5979529272,65211.5512262658,71825.5044996044,78439.457772943,85053.4110462816,91667.3643196202,98281.3175929589,104895.270866297,111509.224139636,118123.177412975,124737.130686313,131351.083959652,137965.03723299,144578.990506329,151192.943779668,157806.897053006,164420.850326345,171034.803599684,177648.756873022,184262.710146361,190876.663419699,197490.616693038,204104.569966377,210718.523239715,217332.476513054,223946.429786392,230560.383059731,237174.33633307,243788.289606408,250402.242879747,257016.196153085,263630.149426424,270244.102699763,276858.055973101,283472.00924644,290085.962519779,296699.915793117,303313.869066456,309927.822339794,316541.775613133,323155.728886472,329769.68215981,336383.635433149,342997.588706487,349611.541979826,356225.495253165,362839.448526503,369453.401799842,376067.35507318,382681.308346519,389295.261619858,395909.214893196,402523.168166535,409137.121439873,415751.074713212,422365.027986551,428978.981259889,435592.934533228,442206.887806566,448820.841079905,455434.794353244,462048.747626582,468662.700899921,475276.65417326,481890.607446598,488504.560719937,495118.513993275,501732.467266614,508346.420539953,514960.373813291,521574.32708663,528188.280359968,534802.233633307,541416.186906646,548030.140179984,554644.093453323,561258.046726661,567872],"y":[41318.9466390204,48141.1583931865,54963.3701473527,61785.5819015188,68607.793655685,75430.0054098511,82252.2171640173,89074.4289181834,95896.6406723496,102718.852426516,109541.064180682,116363.275934848,123185.487689014,130007.69944318,136829.911197346,143652.122951513,150474.334705679,157296.546459845,164118.758214011,170940.969968177,177763.181722343,184585.39347651,191407.605230676,198229.816984842,205052.028739008,211874.240493174,218696.45224734,225518.664001506,232340.875755673,239163.087509839,245985.299264005,252807.511018171,259629.722772337,266451.934526503,273274.146280669,280096.358034836,286918.569789002,293740.781543168,300562.993297334,307385.2050515,314207.416805666,321029.628559833,327851.840313999,334674.052068165,341496.263822331,348318.475576497,355140.687330663,361962.899084829,368785.110838996,375607.322593162,382429.534347328,389251.746101494,396073.95785566,402896.169609826,409718.381363992,416540.593118159,423362.804872325,430185.016626491,437007.228380657,443829.440134823,450651.651888989,457473.863643155,464296.075397322,471118.287151488,477940.498905654,484762.71065982,491584.922413986,498407.134168152,505229.345922319,512051.557676485,518873.769430651,525695.981184817,532518.192938983,539340.404693149,546162.616447316,552984.828201482,559807.039955648,566629.251709814,573451.46346398,580273.675218146],"text":"","type":"scatter","mode":"lines","name":"fitted values","line":{"width":3.77952755905512,"color":"rgba(51,102,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[45369.69140625,51983.6446795886,58597.5979529272,65211.5512262658,71825.5044996044,78439.457772943,85053.4110462816,91667.3643196202,98281.3175929589,104895.270866297,111509.224139636,118123.177412975,124737.130686313,131351.083959652,137965.03723299,144578.990506329,151192.943779668,157806.897053006,164420.850326345,171034.803599684,177648.756873022,184262.710146361,190876.663419699,197490.616693038,204104.569966377,210718.523239715,217332.476513054,223946.429786392,230560.383059731,237174.33633307,243788.289606408,250402.242879747,257016.196153085,263630.149426424,270244.102699763,276858.055973101,283472.00924644,290085.962519779,296699.915793117,303313.869066456,309927.822339794,316541.775613133,323155.728886472,329769.68215981,336383.635433149,342997.588706487,349611.541979826,356225.495253165,362839.448526503,369453.401799842,376067.35507318,382681.308346519,389295.261619858,395909.214893196,402523.168166535,409137.121439873,415751.074713212,422365.027986551,428978.981259889,435592.934533228,442206.887806566,448820.841079905,455434.794353244,462048.747626582,468662.700899921,475276.65417326,481890.607446598,488504.560719937,495118.513993275,501732.467266614,508346.420539953,514960.373813291,521574.32708663,528188.280359968,534802.233633307,541416.186906646,548030.140179984,554644.093453323,561258.046726661,567872,567872,567872,561258.046726661,554644.093453323,548030.140179984,541416.186906646,534802.233633307,528188.280359968,521574.32708663,514960.373813291,508346.420539953,501732.467266614,495118.513993275,488504.560719937,481890.607446598,475276.65417326,468662.700899921,462048.747626582,455434.794353244,448820.841079905,442206.887806566,435592.934533228,428978.981259889,422365.027986551,415751.074713212,409137.121439873,402523.168166535,395909.214893196,389295.261619858,382681.308346519,376067.35507318,369453.401799842,362839.448526503,356225.495253165,349611.541979826,342997.588706487,336383.635433149,329769.68215981,323155.728886472,316541.775613133,309927.822339794,303313.869066456,296699.915793117,290085.962519779,283472.00924644,276858.055973101,270244.102699763,263630.149426424,257016.196153085,250402.242879747,243788.289606408,237174.33633307,230560.383059731,223946.429786392,217332.476513054,210718.523239715,204104.569966377,197490.616693038,190876.663419699,184262.710146361,177648.756873022,171034.803599684,164420.850326345,157806.897053006,151192.943779668,144578.990506329,137965.03723299,131351.083959652,124737.130686313,118123.177412975,111509.224139636,104895.270866297,98281.3175929589,91667.3643196202,85053.4110462816,78439.457772943,71825.5044996044,65211.5512262658,58597.5979529272,51983.6446795886,45369.69140625,45369.69140625],"y":[38348.4802177588,45279.6067817182,52209.3169340957,59137.4381763965,66063.7720583989,72988.0899327899,79910.128114074,86829.5824535635,93746.1024158498,100659.284863477,107568.66794466,114473.725753423,121373.864804561,128268.423825595,135156.678851413,142037.855964881,148911.153994476,155775.778700216,162630.988126707,169476.145849818,176310.775383238,183134.606358727,189947.602847297,196749.967239456,203542.118745178,210324.651605399,217098.28208623,223863.794031476,230621.990656937,237373.656885673,244119.533323205,250860.300771666,257596.573109022,264328.896143929,271057.750331063,277783.555698259,284506.677813432,291227.434021104,297946.099481642,304662.912758294,311378.080836257,318091.783544003,324804.177396716,331515.398907588,338225.5674238,344934.787546402,351643.151190786,358350.739339526,365057.623533513,371763.867141284,378469.526440807,385174.651542871,391879.287180709,398583.473386611,405287.246072978,411990.637532451,418693.676869405,425396.390373119,432098.801841284,438800.932861142,445502.803054386,452204.430290997,458905.830876414,465607.019715715,472308.010457983,479008.815623518,485709.446716179,492409.914322811,499110.228201414,505810.397359498,512510.430123851,519210.334202773,525910.116741705,532609.78437304,539309.343260801,546008.799140789,552708.15735672,559407.422892804,566106.600403165,572805.694238446,572805.694238446,587741.656197846,580796.326524795,573851.080526824,566905.922554576,559960.857262174,553015.88963383,546071.025013258,539126.269136261,532181.628166861,525237.10873745,518292.717993471,511348.463643223,504404.354013494,497460.398111793,490516.605696122,483572.987353325,476629.554587261,469686.319918229,462743.296995314,455800.500723593,448857.947408504,441915.65492003,434973.642879863,428031.932875244,421090.548703866,414149.516655007,407208.865833041,400268.628530611,393328.840660117,386389.542253849,379450.77804504,372512.598144478,365575.058830133,358638.223470541,351702.163606592,344766.960220862,337832.705228742,330899.503231281,323967.473575663,317036.752775076,310107.497344707,303179.887113026,296254.129065232,289330.461764572,282409.160371412,275490.542230276,268574.972909078,261662.872435653,254754.721264676,247851.065204805,240952.518134005,234059.760854408,227173.533971537,220294.62240845,213423.829380949,206561.938732838,199709.666730228,192867.607614054,186036.180594292,179215.588061449,172405.794086536,165606.528301315,158817.314219474,152037.515416882,145266.389938144,138503.14354328,131746.975060765,124997.110573468,118252.826116273,111513.460416704,104778.419989554,98047.1789288493,91319.2753828033,84594.3062139605,77871.9208869123,71151.815252971,64433.7256266411,57717.4233606096,51002.7100046548,44289.4130602819,38348.4802177588],"text":"","type":"scatter","mode":"lines","line":{"width":3.77952755905512,"color":"transparent","dash":"solid"},"fill":"toself","fillcolor":"rgba(153,153,153,0.4)","hoveron":"points","hoverinfo":"x+y","showlegend":false,"xaxis":"x","yaxis":"y","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"Predicted sale price vs sales price","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[19244.5759765625,593997.115429688],"tickmode":"array","ticktext":["100000","200000","300000","400000","500000"],"tickvals":[100000,200000,300000,400000,500000],"categoryorder":"array","categoryarray":["100000","200000","300000","400000","500000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Predicted sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-605,780505],"tickmode":"array","ticktext":["0","200000","400000","600000"],"tickvals":[0,200000,400000,600000],"categoryorder":"array","categoryarray":["0","200000","400000","600000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Test sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"87c72af51a0":{"x":{},"y":{},"Predicted":{},"Tested":{},"type":"scatter"},"87c56065f8d":{"x":{},"y":{},"Predicted":{},"Tested":{}}},"cur_data":"87c72af51a0","visdat":{"87c72af51a0":["function (y) ","x"],"87c56065f8d":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="questionnaire-design.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exercises.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
