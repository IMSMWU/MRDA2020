<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14 XGBoost | Marketing Research Design &amp; Analysis 2020</title>
  <meta name="description" content="An Introduction to Statistics Using R" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="14 XGBoost | Marketing Research Design &amp; Analysis 2020" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An Introduction to Statistics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14 XGBoost | Marketing Research Design &amp; Analysis 2020" />
  
  <meta name="twitter:description" content="An Introduction to Statistics Using R" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="questionnaire-design.html"/>
<link rel="next" href="exercises.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">MRDA 2019</a></strong></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#finding-your-way-to-r"><i class="fa fa-check"></i>Finding Your Way To R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#datacamp"><i class="fa fa-check"></i>DataCamp</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#getting-help"><i class="fa fa-check"></i><b>1.2</b> Getting help</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#functions"><i class="fa fa-check"></i><b>1.3</b> Functions</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#packages"><i class="fa fa-check"></i><b>1.4</b> Packages</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#a-typical-r-session"><i class="fa fa-check"></i><b>1.5</b> A typical R session</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>2</b> Data handling</a><ul>
<li class="chapter" data-level="2.1" data-path="data-handling.html"><a href="data-handling.html#basic-data-handling"><i class="fa fa-check"></i><b>2.1</b> Basic data handling</a><ul>
<li class="chapter" data-level="2.1.1" data-path="data-handling.html"><a href="data-handling.html#creating-objects"><i class="fa fa-check"></i><b>2.1.1</b> Creating objects</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-handling.html"><a href="data-handling.html#data-types"><i class="fa fa-check"></i><b>2.1.2</b> Data types</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-handling.html"><a href="data-handling.html#data-structures"><i class="fa fa-check"></i><b>2.1.3</b> Data structures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><i class="fa fa-check"></i><b>3</b> Selects all columns from top10_track_explicit to top_10_track_release_date</a><ul>
<li class="chapter" data-level="3.1" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#advanced-data-handling"><i class="fa fa-check"></i><b>3.1</b> Advanced data handling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#the-dplyr-package"><i class="fa fa-check"></i><b>3.1.1</b> The dplyr package</a></li>
<li class="chapter" data-level="3.1.2" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#dealing-with-strings"><i class="fa fa-check"></i><b>3.1.2</b> Dealing with strings</a></li>
<li class="chapter" data-level="3.1.3" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#case-study"><i class="fa fa-check"></i><b>3.1.3</b> Case study</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#data-import-and-export"><i class="fa fa-check"></i><b>3.2</b> Data import and export</a><ul>
<li class="chapter" data-level="3.2.1" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>3.2.1</b> Getting data for this course</a></li>
<li class="chapter" data-level="3.2.2" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#import-data-created-by-other-software-packages"><i class="fa fa-check"></i><b>3.2.2</b> Import data created by other software packages</a></li>
<li class="chapter" data-level="3.2.3" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#export-data"><i class="fa fa-check"></i><b>3.2.3</b> Export data</a></li>
<li class="chapter" data-level="3.2.4" data-path="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html"><a href="selects-all-columns-from-top10-track-explicit-to-top-10-track-release-date.html#import-data-from-the-web"><i class="fa fa-check"></i><b>3.2.4</b> Import data from the Web</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>4</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.1" data-path="summarizing-data.html"><a href="summarizing-data.html#summary-statistics"><i class="fa fa-check"></i><b>4.1</b> Summary statistics</a><ul>
<li class="chapter" data-level="4.1.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>4.1.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.1.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>4.1.2</b> Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="summarizing-data.html"><a href="summarizing-data.html#data-visualization"><i class="fa fa-check"></i><b>4.2</b> Data visualization</a><ul>
<li class="chapter" data-level="4.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>4.2.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>4.2.2</b> Continuous variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#saving-plots"><i class="fa fa-check"></i><b>4.2.3</b> Saving plots</a></li>
<li class="chapter" data-level="4.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#additional-options"><i class="fa fa-check"></i><b>4.2.4</b> Additional options</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="summarizing-data.html"><a href="summarizing-data.html#writing-reports-using-r-markdown"><i class="fa fa-check"></i><b>4.3</b> Writing reports using R-Markdown</a><ul>
<li class="chapter" data-level="4.3.1" data-path="summarizing-data.html"><a href="summarizing-data.html#creating-a-new-r-markdown-document"><i class="fa fa-check"></i><b>4.3.1</b> Creating a new R-Markdown document</a></li>
<li class="chapter" data-level="4.3.2" data-path="summarizing-data.html"><a href="summarizing-data.html#text-and-equations"><i class="fa fa-check"></i><b>4.3.2</b> Text and Equations</a></li>
<li class="chapter" data-level="4.3.3" data-path="summarizing-data.html"><a href="summarizing-data.html#r-code"><i class="fa fa-check"></i><b>4.3.3</b> R-Code</a></li>
<li class="chapter" data-level="4.3.4" data-path="summarizing-data.html"><a href="summarizing-data.html#latex-math"><i class="fa fa-check"></i><b>4.3.4</b> LaTeX Math</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html"><i class="fa fa-check"></i><b>5</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#if-we-knew-it-all"><i class="fa fa-check"></i><b>5.1</b> If we knew it all</a><ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#sampling-from-a-known-population"><i class="fa fa-check"></i><b>5.1.1</b> Sampling from a known population</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>5.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#using-what-we-actually-know"><i class="fa fa-check"></i><b>5.3</b> Using what we actually know</a><ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#confidence-intervals-for-the-sample-mean"><i class="fa fa-check"></i><b>5.3.1</b> Confidence Intervals for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#summary"><i class="fa fa-check"></i><b>5.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-null-hypothesis"><i class="fa fa-check"></i><b>6.1.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="6.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#statistical-inference-on-a-sample"><i class="fa fa-check"></i><b>6.1.2</b> Statistical inference on a sample</a></li>
<li class="chapter" data-level="6.1.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#choosing-the-right-test"><i class="fa fa-check"></i><b>6.1.3</b> Choosing the right test</a></li>
<li class="chapter" data-level="6.1.4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#summary"><i class="fa fa-check"></i><b>6.1.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-t-test"><i class="fa fa-check"></i><b>6.2</b> One sample t-test</a></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-two-means"><i class="fa fa-check"></i><b>6.3</b> Comparing two means</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#independent-means-t-test"><i class="fa fa-check"></i><b>6.3.1</b> Independent-means t-test</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#dependent-means-t-test"><i class="fa fa-check"></i><b>6.3.2</b> Dependent-means t-test</a></li>
<li class="chapter" data-level="6.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#further-considerations"><i class="fa fa-check"></i><b>6.3.3</b> Further considerations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-several-means"><i class="fa fa-check"></i><b>6.4</b> Comparing several means</a><ul>
<li class="chapter" data-level="6.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#decomposing-variance"><i class="fa fa-check"></i><b>6.4.2</b> Decomposing variance</a></li>
<li class="chapter" data-level="6.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-way-anova"><i class="fa fa-check"></i><b>6.4.3</b> One-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>6.5</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="6.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#mann-whitney-u-test-a.k.a.-wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>6.5.1</b> Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test)</a></li>
<li class="chapter" data-level="6.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>6.5.2</b> Wilcoxon signed-rank test</a></li>
<li class="chapter" data-level="6.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.5.3</b> Kruskal-Wallis test</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#categorical-data"><i class="fa fa-check"></i><b>6.6</b> Categorical data</a><ul>
<li class="chapter" data-level="6.6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#confidence-intervals-for-proportions"><i class="fa fa-check"></i><b>6.6.1</b> Confidence intervals for proportions</a></li>
<li class="chapter" data-level="6.6.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#chi-square-test"><i class="fa fa-check"></i><b>6.6.2</b> Chi-square test</a></li>
<li class="chapter" data-level="6.6.3" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#sample-size"><i class="fa fa-check"></i><b>6.6.3</b> Sample size</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#summary-of-hypothesis-testing"><i class="fa fa-check"></i><b>6.7</b> Summary of hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>7</b> Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="regression.html"><a href="regression.html#correlation"><i class="fa fa-check"></i><b>7.1</b> Correlation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="regression.html"><a href="regression.html#correlation-coefficient"><i class="fa fa-check"></i><b>7.1.1</b> Correlation coefficient</a></li>
<li class="chapter" data-level="7.1.2" data-path="regression.html"><a href="regression.html#significance-testing"><i class="fa fa-check"></i><b>7.1.2</b> Significance testing</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="regression.html"><a href="regression.html#regression-1"><i class="fa fa-check"></i><b>7.2</b> Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.2.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.2.2</b> Multiple linear regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="regression.html"><a href="regression.html#estimation-of-the-regression-plane"><i class="fa fa-check"></i><b>7.2.3</b> Estimation of the regression plane</a></li>
<li class="chapter" data-level="7.2.4" data-path="regression.html"><a href="regression.html#calculate-z-on-a-grid-of-x-y-values"><i class="fa fa-check"></i><b>7.2.4</b> Calculate z on a grid of x-y values</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regression.html"><a href="regression.html#potential-problems"><i class="fa fa-check"></i><b>7.3</b> Potential problems</a><ul>
<li class="chapter" data-level="7.3.1" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>7.3.1</b> Outliers</a></li>
<li class="chapter" data-level="7.3.2" data-path="regression.html"><a href="regression.html#influential-observations"><i class="fa fa-check"></i><b>7.3.2</b> Influential observations</a></li>
<li class="chapter" data-level="7.3.3" data-path="regression.html"><a href="regression.html#non-linearity"><i class="fa fa-check"></i><b>7.3.3</b> Non-linearity</a></li>
<li class="chapter" data-level="7.3.4" data-path="regression.html"><a href="regression.html#non-constant-error-variance"><i class="fa fa-check"></i><b>7.3.4</b> Non-constant error variance</a></li>
<li class="chapter" data-level="7.3.5" data-path="regression.html"><a href="regression.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>7.3.5</b> Non-normally distributed errors</a></li>
<li class="chapter" data-level="7.3.6" data-path="regression.html"><a href="regression.html#correlation-of-errors"><i class="fa fa-check"></i><b>7.3.6</b> Correlation of errors</a></li>
<li class="chapter" data-level="7.3.7" data-path="regression.html"><a href="regression.html#collinearity"><i class="fa fa-check"></i><b>7.3.7</b> Collinearity</a></li>
<li class="chapter" data-level="7.3.8" data-path="regression.html"><a href="regression.html#omitted-variables"><i class="fa fa-check"></i><b>7.3.8</b> Omitted Variables</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regression.html"><a href="regression.html#categorical-predictors"><i class="fa fa-check"></i><b>7.4</b> Categorical predictors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regression.html"><a href="regression.html#two-categories"><i class="fa fa-check"></i><b>7.4.1</b> Two categories</a></li>
<li class="chapter" data-level="7.4.2" data-path="regression.html"><a href="regression.html#more-than-two-categories"><i class="fa fa-check"></i><b>7.4.2</b> More than two categories</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regression.html"><a href="regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>7.5</b> Extensions of the linear model</a><ul>
<li class="chapter" data-level="7.5.1" data-path="regression.html"><a href="regression.html#interaction-effects"><i class="fa fa-check"></i><b>7.5.1</b> Interaction effects</a></li>
<li class="chapter" data-level="7.5.2" data-path="regression.html"><a href="regression.html#non-linear-relationships"><i class="fa fa-check"></i><b>7.5.2</b> Non-linear relationships</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="regression.html"><a href="regression.html#summary-of-regression"><i class="fa fa-check"></i><b>7.6</b> Summary of regression</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="inspect-data.html"><a href="inspect-data.html"><i class="fa fa-check"></i><b>8</b> Inspect data</a><ul>
<li class="chapter" data-level="8.1" data-path="inspect-data.html"><a href="inspect-data.html#logistic-regression"><i class="fa fa-check"></i><b>8.1</b> Logistic regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="inspect-data.html"><a href="inspect-data.html#motivation-and-intuition"><i class="fa fa-check"></i><b>8.1.1</b> Motivation and intuition</a></li>
<li class="chapter" data-level="8.1.2" data-path="inspect-data.html"><a href="inspect-data.html#technical-details-of-the-model"><i class="fa fa-check"></i><b>8.1.2</b> Technical details of the model</a></li>
<li class="chapter" data-level="8.1.3" data-path="inspect-data.html"><a href="inspect-data.html#estimation-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Estimation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html"><i class="fa fa-check"></i><b>9</b> Exploratory factor analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#steps-in-factor-analysis"><i class="fa fa-check"></i><b>9.2</b> Steps in factor analysis</a><ul>
<li class="chapter" data-level="9.2.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#are-the-assumptions-satisfied"><i class="fa fa-check"></i><b>9.2.1</b> Are the assumptions satisfied?</a></li>
<li class="chapter" data-level="9.2.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#deriving-factors"><i class="fa fa-check"></i><b>9.2.2</b> Deriving factors</a></li>
<li class="chapter" data-level="9.2.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#factor-interpretation"><i class="fa fa-check"></i><b>9.2.3</b> Factor interpretation</a></li>
<li class="chapter" data-level="9.2.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#creating-new-variables"><i class="fa fa-check"></i><b>9.2.4</b> Creating new variables</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#reliability-analysis"><i class="fa fa-check"></i><b>9.3</b> Reliability analysis</a></li>
<li class="chapter" data-level="9.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#summary-of-factor-analysis"><i class="fa fa-check"></i><b>9.4</b> Summary of factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>10</b> Appendix</a><ul>
<li class="chapter" data-level="10.1" data-path="appendix.html"><a href="appendix.html#random-variables-probability-distributions"><i class="fa fa-check"></i><b>10.1</b> Random Variables &amp; Probability Distributions</a><ul>
<li class="chapter" data-level="10.1.1" data-path="appendix.html"><a href="appendix.html#random-variables"><i class="fa fa-check"></i><b>10.1.1</b> Random variables</a></li>
<li class="chapter" data-level="10.1.2" data-path="appendix.html"><a href="appendix.html#probability-distributions"><i class="fa fa-check"></i><b>10.1.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="10.1.3" data-path="appendix.html"><a href="appendix.html#appendix"><i class="fa fa-check"></i><b>10.1.3</b> Appendix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="this-is-the-same-as-avplot.html"><a href="this-is-the-same-as-avplot.html"><i class="fa fa-check"></i><b>11</b> This is the same as avPlot</a><ul>
<li class="chapter" data-level="11.1" data-path="regression.html"><a href="regression.html#regression"><i class="fa fa-check"></i><b>11.1</b> Regression</a><ul>
<li class="chapter" data-level="11.1.1" data-path="this-is-the-same-as-avplot.html"><a href="this-is-the-same-as-avplot.html#linear-regression"><i class="fa fa-check"></i><b>11.1.1</b> Linear regression</a></li>
<li class="chapter" data-level="11.1.2" data-path="inspect-data.html"><a href="inspect-data.html#logistic-regression"><i class="fa fa-check"></i><b>11.1.2</b> Logistic regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="assignments.html"><a href="assignments.html"><i class="fa fa-check"></i><b>12</b> Assignments</a><ul>
<li class="chapter" data-level="12.1" data-path="assignments.html"><a href="assignments.html#assignment-2-hypothesis-testing"><i class="fa fa-check"></i><b>12.1</b> Assignment 2 (Hypothesis Testing)</a></li>
<li class="chapter" data-level="12.2" data-path="assignments.html"><a href="assignments.html#assignment-3-hypothesis-testing-2"><i class="fa fa-check"></i><b>12.2</b> Assignment 3 (Hypothesis Testing 2)</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="questionnaire-design.html"><a href="questionnaire-design.html"><i class="fa fa-check"></i><b>13</b> Questionnaire design</a><ul>
<li class="chapter" data-level="13.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-design-process"><i class="fa fa-check"></i><b>13.1</b> Questionnaire design process</a><ul>
<li class="chapter" data-level="13.1.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#specification-of-the-information-needed"><i class="fa fa-check"></i><b>13.1.1</b> Specification of the information needed</a></li>
<li class="chapter" data-level="13.1.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#specify-the-interviewing-method"><i class="fa fa-check"></i><b>13.1.2</b> Specify the interviewing method</a></li>
<li class="chapter" data-level="13.1.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#determine-the-content-of-questions"><i class="fa fa-check"></i><b>13.1.3</b> Determine the content of questions</a></li>
<li class="chapter" data-level="13.1.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#inability-and-unwillingness-to-answer"><i class="fa fa-check"></i><b>13.1.4</b> Inability and unwillingness to answer</a></li>
<li class="chapter" data-level="13.1.5" data-path="questionnaire-design.html"><a href="questionnaire-design.html#decide-on-measurement-scales-and-scaling-techniques"><i class="fa fa-check"></i><b>13.1.5</b> Decide on measurement scales and scaling techniques</a></li>
<li class="chapter" data-level="13.1.6" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-structure"><i class="fa fa-check"></i><b>13.1.6</b> Questionnaire structure</a></li>
<li class="chapter" data-level="13.1.7" data-path="questionnaire-design.html"><a href="questionnaire-design.html#question-wording"><i class="fa fa-check"></i><b>13.1.7</b> Question wording</a></li>
<li class="chapter" data-level="13.1.8" data-path="questionnaire-design.html"><a href="questionnaire-design.html#choose-adequate-order"><i class="fa fa-check"></i><b>13.1.8</b> Choose adequate order</a></li>
<li class="chapter" data-level="13.1.9" data-path="questionnaire-design.html"><a href="questionnaire-design.html#test-your-questionnaire"><i class="fa fa-check"></i><b>13.1.9</b> Test your questionnaire</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#questionnaire-in-qualtrics"><i class="fa fa-check"></i><b>13.2</b> Questionnaire in Qualtrics</a></li>
<li class="chapter" data-level="13.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#question-types-and-data-analysis"><i class="fa fa-check"></i><b>13.3</b> Question types and data analysis</a><ul>
<li class="chapter" data-level="13.3.1" data-path="questionnaire-design.html"><a href="questionnaire-design.html#multiple-choice-with-a-single-answer"><i class="fa fa-check"></i><b>13.3.1</b> Multiple choice with a single answer</a></li>
<li class="chapter" data-level="13.3.2" data-path="questionnaire-design.html"><a href="questionnaire-design.html#multiple-choice-with-multiple-answers"><i class="fa fa-check"></i><b>13.3.2</b> Multiple choice with multiple answers</a></li>
<li class="chapter" data-level="13.3.3" data-path="questionnaire-design.html"><a href="questionnaire-design.html#rank-order-question"><i class="fa fa-check"></i><b>13.3.3</b> Rank order question</a></li>
<li class="chapter" data-level="13.3.4" data-path="questionnaire-design.html"><a href="questionnaire-design.html#constant-sum-question"><i class="fa fa-check"></i><b>13.3.4</b> Constant Sum question</a></li>
<li class="chapter" data-level="13.3.5" data-path="questionnaire-design.html"><a href="questionnaire-design.html#text-or-number-entry-question"><i class="fa fa-check"></i><b>13.3.5</b> Text or number entry question</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>14</b> XGBoost</a><ul>
<li class="chapter" data-level="14.1" data-path="xgboost.html"><a href="xgboost.html#what-is-machine-learning"><i class="fa fa-check"></i><b>14.1</b> What is machine learning?</a></li>
<li class="chapter" data-level="14.2" data-path="xgboost.html"><a href="xgboost.html#gradient-boosting"><i class="fa fa-check"></i><b>14.2</b> Gradient Boosting</a><ul>
<li class="chapter" data-level="14.2.1" data-path="xgboost.html"><a href="xgboost.html#elements-of-supervised-machine-learning"><i class="fa fa-check"></i><b>14.2.1</b> Elements of supervised machine learning</a></li>
<li class="chapter" data-level="14.2.2" data-path="xgboost.html"><a href="xgboost.html#principle-behind-boosting"><i class="fa fa-check"></i><b>14.2.2</b> Principle behind boosting</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="xgboost.html"><a href="xgboost.html#xgboost-package"><i class="fa fa-check"></i><b>14.3</b> xgboost package</a><ul>
<li class="chapter" data-level="14.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>14.3.1</b> Introduction</a></li>
<li class="chapter" data-level="14.3.2" data-path="xgboost.html"><a href="xgboost.html#data-preparation"><i class="fa fa-check"></i><b>14.3.2</b> Data preparation</a></li>
<li class="chapter" data-level="14.3.3" data-path="xgboost.html"><a href="xgboost.html#engineering"><i class="fa fa-check"></i><b>14.3.3</b> Engineering</a></li>
<li class="chapter" data-level="14.3.4" data-path="xgboost.html"><a href="xgboost.html#strategy-for-tuning"><i class="fa fa-check"></i><b>14.3.4</b> [STRATEGY FOR TUNING]</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>15</b> Exercises</a><ul>
<li class="chapter" data-level="15.1" data-path="exercises.html"><a href="exercises.html#exercise-in-machine-learning-in-progress"><i class="fa fa-check"></i><b>15.1</b> Exercise in Machine learning (IN PROGRESS)</a><ul>
<li class="chapter" data-level="15.1.1" data-path="exercises.html"><a href="exercises.html#exercise-to-download-in-progress"><i class="fa fa-check"></i><b>15.1.1</b> Exercise to download (IN PROGRESS)</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Marketing Research Design &amp; Analysis 2020</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="xgboost" class="section level1">
<h1><span class="header-section-number">14</span> XGBoost</h1>
<p>References:</p>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/abs/1603.02754">XGBoost: A Scalable Tree Boosting System</a></li>
<li><a href="https://bradleyboehmke.github.io/HOML/gbm.html#xgboost">Hands on machine-learning with R</a></li>
<li><a href="https://towardsdatascience.com/predicting-marketing-performance-with-machine-learning-c8472bc7807">Predicting marketing performance with ML</a></li>
<li><a href="https://doi.org/10.1016/j.dss.2014.03.001">A data-driven approach to predict the success of bank telemarketing</a></li>
<li><a href="http://uc-r.github.io/gbm_regression">Extensive tutorial</a></li>
<li><a href="http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/139-gradient-boosting-essentials-in-r-using-xgboost/">Gradient Boosting Essentials in R Using XGBOOST</a></li>
<li><a href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/">A Gentle Introduction to XGBoost for Applied Machine Learning</a></li>
<li><a href="https://datascienceplus.com/gradient-boosting-in-r/">Gradient Boosting in R</a></li>
</ol>
<div id="what-is-machine-learning" class="section level2">
<h2><span class="header-section-number">14.1</span> What is machine learning?</h2>
<p>In essence, the road to machine learning starts with regression. Some typical use cases of machine learning in marketing are:</p>
<ul>
<li>Segmenting customers based on common attributes or purchasing behavior for targeted marketing</li>
<li>Predicting coupon redemption rates for a given marketing campaign</li>
<li>Predicting customer churn so an organization can perform preventative intervention</li>
</ul>
<p>In its core, these tasks all seek to learn from data. In order to do so, we use a given set of features to train an algorithm and extract information we need. These algorithms, known as learners too, can be divided according to the amount and type of supervision needed during training. We distinguish supervised learners which construct predictive models, and unsupervised learners which build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish.</p>
<p>In this chapter we will focus on supervised machine learning, more specifically, on one method called extreme gradient boosting.</p>
</div>
<div id="gradient-boosting" class="section level2">
<h2><span class="header-section-number">14.2</span> Gradient Boosting</h2>
<p>When we talk about machine learning, there are two quite important factors that drive successful application: effective statistical models that are capable of capturing the complex data dependencies and how scalable are learning systems that learn the model from large datasets. Among the machine learning methods commonly used in practice are gradient tree boosting methods. Gradient boosting machines (GBMs) are a very popular machine learning method, and in this chapter we will introduce you R package “xgboost” and show how it can be used for marketing purposes. It is a scalable machine learning system for tree boosting and GMBs have proven successful across many domains and are one of the leading methods you can find across Kaggle competitions.</p>
<p>When it comes to marketing, it can be used in uplift modeling, i.e. can help a company to identify those who are likely to buy products as a result of receiving a discount or a personalized advertisement. Consequently, it helps a company to maximize profits by keeping advertising costs and overall efforts to the minimum. In the perspective of data analysis and marketing, performance of a marketing campaign can be predicted using algoritham such as GBMs. For instance, in the banking industry optimizing targeting for telemarketing used to be one of the main issues, especially under a growing pressure induced by financial crisis in 2008. A commercial bank from Portugal used data-driven model to predict the result of a telemarketing phone call to sell long term deposits is a valuable tool to support client selection decisions of bank campaign managers. As a result, they identified that inbound calls and an increase in other attributes identified as a highly relevant (such as agent experience or duration of pre-vious calls) enhance the probability for a successful deposit sell.</p>
<div id="elements-of-supervised-machine-learning" class="section level3">
<h3><span class="header-section-number">14.2.1</span> Elements of supervised machine learning</h3>
<p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" class="uri">https://xgboost.readthedocs.io/en/latest/tutorials/model.html</a>
<a href="https://bradleyboehmke.github.io/HOML/intro.html#supervised-learning" class="uri">https://bradleyboehmke.github.io/HOML/intro.html#supervised-learning</a></p>
<p>XGBoost is used in supervised machine learning. Let us decompose the meaning of supervised machine learning.</p>
<p>Supervised machine learning can be described as a process in which training data with multiple <strong>features </strong>(also called: predictor variables, independent variables, attributes, predictor) is used to predict a <strong>target variable</strong> (also called, dependent variable, response, outcome measurement).</p>
<p>The final outcome of supervised machine learning is a predictive model, so it is important to define what a model is. <strong>A model</strong>, in the context of supervised machine learning, contains a mathematical structure or algorithm by which the prediction of a target variable is made from the multiple features used as input. For instance, algorithm that helps you to predict the sale price of your house based on the house attributes.</p>
<p>Another important term in the context of machine learning are <strong>parameters</strong>. They denote undetermined part that we need to learn from data. For instance, in case of linear regression, the parameters are the <strong>coefficients </strong>.</p>
<p>Finally, as we said, models we build with supervised machine learning are predictive models. “Supervised” refers to a supervisory role of the target variable, which indicates the task that model needs to learn. More specifically, it means that the data which is used for training a model contains target variable. Given a set of training data, the learning algorithm attempts to find the combination of feature values that results in a predicted value as close to the actual target variable as possible.</p>
</div>
<div id="principle-behind-boosting" class="section level3">
<h3><span class="header-section-number">14.2.2</span> Principle behind boosting</h3>
<p>Boosting can be explained as a sequential process. That means that at each particular iteration, a new weak model is trained with respect to the error of the whole ensemble learned by that time. A weak model is one whose predictions (error rate) are only slightly better than random guessing. In simple words, in each iteration a better model is created by adding a new weak model to the existing one, where the purpose of the weak model is to slightly improve the remaining errors of the existing model. This process slowly learns from data and tries to improve its prediction in subsequent iterations.</p>
<p>Among other, boosting is used for solving both regression and classification problems.</p>
<p>Below you can find an illustrative example and explanation for each of them.
<img src="bigd.png" width="276" style="display: block; margin: auto;" /></p>
<p>In the illustration above you can see 4 boxes with pluses and minuses within them representing observations. The ultimate goal of a model we need to develop is of classification nature, i.e. to classify pluses (“+”) and minuses (-) within a box as accurate as possible.</p>
<p>It is important to mention that at the begining all observations are assigned equal weights. However, weights are subject to change after each iteration as misclassified observations in one iteration will be assigned higher weight in the next one. Opposingly, observations that are correctly classified in one iteration will be assigned lower weight in the subsequent iteration.</p>
<p>In the box 1, the first weak learner identified “+” signs just on the left side of the box. It simply missclassified three “+” signs in the middle “-” upper part and recognized only the two on the left side. Consequently, it split the box in two parts (blue and light red), meaning that everything that appears in the blue “-” marked area is classified as “+”, while the rest is classified as “-”.</p>
<p>Although our prediction model at this point does not do great job, it contains information useful for the next weak lerner that is being added in the box 2. Next weak learner assigns more weight to three “+” signs that were previously missclassified. Similarly to the previous split, the weak learner split the box 2 again in blue “-” and red “-” marked area. Again, everything in the blue area (left from the splitting line) was classified as “+”, including three minus signs being missclassified. The rest was classified as -". Even though our predicting model looks a bit better, its classification is still incorrect.</p>
<p>In the box 3, our model is becoming even better in classifying. It splitted the box horizontally, so that everything below the line was classified as “-” and above the line as “+”. Despite the progress we still have some missclassified “-” in the blue-marked area as well as wrongly classified “+” below the splitting line (circled signs in the box 3).</p>
<p>Finally, in the box 4 we see the result from combining information obtained from numerous weak learners. It is a weighted combination of the weak classifiers resulting in a strong classifier.Each classifier individually proved pretty poor performance in predicting, as they all show certain misclassification error. However, after combining them, the ultimate goal to classify all points correctly is reached and strong classifier created.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="boosted_stumps.gif" alt="Boosted regression tree predictions (courtesy of [Brandon Greenwell](https://github.com/bgreenwell))"  />
<p class="caption">
Figure 14.1: Boosted regression tree predictions (courtesy of <a href="https://github.com/bgreenwell">Brandon Greenwell</a>)
</p>
</div>
<p>To understand the whole concept easier, try to follow the image above. On the one hand, the blue curve depicts the real underlying function, while the points depict observations. Moreover, observations include some noise, i.e. errors. On the other hand, you can observe red curve representing constantly improving boosted prediction. More specifically, it illustrates the adjusted predictions after each additional sequential tree is added to the algorithm. At the beginning, you can observe large errors (.i.e. big deviation of the red curve from the blue one) which the boosted algorithm reduces pretty fast. However, as the predictions (.i.e. red curve) get closer to the true underlying function (i.e.blue curve), the contribution to model improvement of each additional tree is smaller and smaller. In the end, the predicted values nearly match to the true underlying function.</p>
</div>
</div>
<div id="xgboost-package" class="section level2">
<h2><span class="header-section-number">14.3</span> xgboost package</h2>
<p>There is an extensive list of packages with GBMs and its variations. However, the most popular implementations which we will cover here is certainly xgboost, which is quite fast and efficient.</p>
<div id="introduction" class="section level3">
<h3><span class="header-section-number">14.3.1</span> Introduction</h3>
<p>XGboost stands for “Extreme Gradient Boosting”, which is an efficient implementation of gradient boosting framework. The xgboost package has been quite popular and successful on Kaggle for data mining competitions.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="xgboost.html#cb1-1"></a><span class="co"># Turn off scientific notation</span></span>
<span id="cb1-2"><a href="xgboost.html#cb1-2"></a><span class="kw">options</span>(<span class="dt">scipen =</span> <span class="dv">9999</span>)</span>
<span id="cb1-3"><a href="xgboost.html#cb1-3"></a></span>
<span id="cb1-4"><a href="xgboost.html#cb1-4"></a><span class="co"># Helper packages</span></span>
<span id="cb1-5"><a href="xgboost.html#cb1-5"></a><span class="kw">library</span>(dplyr)      <span class="co"># for general data wrangling needs</span></span>
<span id="cb1-6"><a href="xgboost.html#cb1-6"></a></span>
<span id="cb1-7"><a href="xgboost.html#cb1-7"></a><span class="co"># Modeling packages</span></span>
<span id="cb1-8"><a href="xgboost.html#cb1-8"></a><span class="kw">library</span>(xgboost)    <span class="co"># for fitting extreme gradient boosting</span></span>
<span id="cb1-9"><a href="xgboost.html#cb1-9"></a><span class="kw">library</span>(rsample)    <span class="co"># for split of data set in the training data and test data</span></span>
<span id="cb1-10"><a href="xgboost.html#cb1-10"></a><span class="kw">library</span>(AmesHousing)<span class="co"># data set</span></span>
<span id="cb1-11"><a href="xgboost.html#cb1-11"></a><span class="kw">library</span>(caret)      <span class="co"># for resampling and model training</span></span>
<span id="cb1-12"><a href="xgboost.html#cb1-12"></a><span class="kw">library</span>(plotly)</span>
<span id="cb1-13"><a href="xgboost.html#cb1-13"></a><span class="kw">library</span>(recipes)</span>
<span id="cb1-14"><a href="xgboost.html#cb1-14"></a><span class="kw">library</span>(pdp)</span>
<span id="cb1-15"><a href="xgboost.html#cb1-15"></a><span class="kw">library</span>(knitr)</span>
<span id="cb1-16"><a href="xgboost.html#cb1-16"></a><span class="kw">library</span>(gbm)</span>
<span id="cb1-17"><a href="xgboost.html#cb1-17"></a><span class="kw">library</span>(mlr)</span>
<span id="cb1-18"><a href="xgboost.html#cb1-18"></a><span class="kw">library</span>(ggplot2)</span></code></pre></div>
<p>To explain gradient boosting with xgboost, we will use typical Ames Iowa Housing data set and try to build a model that predict sale price for houses.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="xgboost.html#cb2-1"></a><span class="co"># Ames housing data</span></span>
<span id="cb2-2"><a href="xgboost.html#cb2-2"></a>ames &lt;-<span class="st"> </span>AmesHousing<span class="op">::</span><span class="kw">make_ames</span>()</span>
<span id="cb2-3"><a href="xgboost.html#cb2-3"></a></span>
<span id="cb2-4"><a href="xgboost.html#cb2-4"></a><span class="co"># Ensure correct naming</span></span>
<span id="cb2-5"><a href="xgboost.html#cb2-5"></a><span class="kw">library</span>(janitor)</span>
<span id="cb2-6"><a href="xgboost.html#cb2-6"></a>ames&lt;-<span class="st"> </span>ames<span class="op">%&gt;%</span><span class="kw">clean_names</span>()</span>
<span id="cb2-7"><a href="xgboost.html#cb2-7"></a></span>
<span id="cb2-8"><a href="xgboost.html#cb2-8"></a><span class="co"># Use mlr package to get an overview of your data</span></span>
<span id="cb2-9"><a href="xgboost.html#cb2-9"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">head</span>(<span class="kw">summarizeColumns</span>(ames)))</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">name</th>
<th align="left">type</th>
<th align="right">na</th>
<th align="right">mean</th>
<th align="right">disp</th>
<th align="right">median</th>
<th align="right">mad</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">nlevs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ms_sub_class</td>
<td align="left">factor</td>
<td align="right">0</td>
<td align="right">NA</td>
<td align="right">0.6317406</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">1</td>
<td align="right">1079</td>
<td align="right">16</td>
</tr>
<tr class="even">
<td align="left">ms_zoning</td>
<td align="left">factor</td>
<td align="right">0</td>
<td align="right">NA</td>
<td align="right">0.2242321</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">2</td>
<td align="right">2273</td>
<td align="right">7</td>
</tr>
<tr class="odd">
<td align="left">lot_frontage</td>
<td align="left">numeric</td>
<td align="right">0</td>
<td align="right">57.64778</td>
<td align="right">33.4994408</td>
<td align="right">63.0</td>
<td align="right">25.2042</td>
<td align="right">0</td>
<td align="right">313</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">lot_area</td>
<td align="left">integer</td>
<td align="right">0</td>
<td align="right">10147.92184</td>
<td align="right">7880.0177594</td>
<td align="right">9436.5</td>
<td align="right">3024.5040</td>
<td align="right">1300</td>
<td align="right">215245</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">street</td>
<td align="left">factor</td>
<td align="right">0</td>
<td align="right">NA</td>
<td align="right">0.0040956</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">12</td>
<td align="right">2918</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">alley</td>
<td align="left">factor</td>
<td align="right">0</td>
<td align="right">NA</td>
<td align="right">0.0675768</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">78</td>
<td align="right">2732</td>
<td align="right">3</td>
</tr>
</tbody>
</table>
</div>
<div id="data-preparation" class="section level3">
<h3><span class="header-section-number">14.3.2</span> Data preparation</h3>
<p>First, we need to deal with data preparation. When using xgboost package, it is necessary to convert the categorical variables into numeric using one hot encoding. What is one hot encoding?
Usually, when between several categories exist ordinal relationship (e.g. variable “place” can be “1st”, “2nd” and so on), all you need to do is so called the integer encoding. In the ordinal variable (such as “place”) the integer values have a natural ordered relationship between each other, so machine learning algorithms are able to understand this relationship. However, for categorical variables where no ordinal relationship exists (e.g. variable “pet” with “dog”, “cat” and “rabbit”), the integer encoding is not sufficient and you need one hot encoding. In the “pet” example, there are 3 categories, thus 3 binary variables are required. Therefore, “1” value is placed in the binary variable for the respective “color” and “0” values for the other colors.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="xgboost.html#cb3-1"></a><span class="kw">cat</span>(tabl)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th>dog</th>
<th>cat</th>
<th>rabbit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can apply one hot encoding to our data set by using R’s base function <code>model.matrix</code>. In the code below, ~.+0 leads to encoding of all categorical variables without producing an intercept.</p>
<pre><code>## $contrasts
##         unordered           ordered 
## &quot;contr.treatment&quot;      &quot;contr.poly&quot;</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="xgboost.html#cb5-1"></a><span class="co"># One hot encoding (turning the test data into matrix with all numerical values)</span></span>
<span id="cb5-2"><a href="xgboost.html#cb5-2"></a>ames.he &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>.<span class="op">+</span><span class="dv">0</span>,<span class="dt">data =</span> ames)</span>
<span id="cb5-3"><a href="xgboost.html#cb5-3"></a></span>
<span id="cb5-4"><a href="xgboost.html#cb5-4"></a><span class="co"># Save variable names due to more practical addressing columns later on</span></span>
<span id="cb5-5"><a href="xgboost.html#cb5-5"></a>setcol &lt;-<span class="st"> </span><span class="kw">colnames</span>(ames.he)</span></code></pre></div>
<p>We need to break our data set into training and test data, while ensuring we have consistent distributions between the training and test sets.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="xgboost.html#cb6-1"></a><span class="co"># Data split on test and train data</span></span>
<span id="cb6-2"><a href="xgboost.html#cb6-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb6-3"><a href="xgboost.html#cb6-3"></a></span>
<span id="cb6-4"><a href="xgboost.html#cb6-4"></a><span class="co"># Create partition</span></span>
<span id="cb6-5"><a href="xgboost.html#cb6-5"></a><span class="kw">library</span>(caret)</span>
<span id="cb6-6"><a href="xgboost.html#cb6-6"></a>index &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">createDataPartition</span>(ames<span class="op">$</span>sale_price, <span class="dt">p =</span> <span class="fl">0.7</span>, <span class="dt">list =</span> F)</span></code></pre></div>
<p><code>index</code> is a matrix with just one column that contains approximately 70% of rows from our original data set.<br />
Now we use <code>index</code> to address columns we want to assign to our train data (<code>ames_train</code>), i.e. columns that we want to assign to our test data (<code>ames_test</code>).</p>
<p>Note that by using “-” in front of <code>index</code> we assign to <code>ames_test</code> all observations <strong>except</strong> those that are in <code>index</code>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="xgboost.html#cb7-1"></a><span class="co"># Split data set in train and test data</span></span>
<span id="cb7-2"><a href="xgboost.html#cb7-2"></a>ames_train &lt;-<span class="st"> </span>ames.he[index, ]</span>
<span id="cb7-3"><a href="xgboost.html#cb7-3"></a>ames_test &lt;-<span class="st"> </span>ames.he[<span class="op">-</span>index, ]</span></code></pre></div>
<p>If we take lake look at the dimensions of our test and train data, we can see that our split was successful.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="xgboost.html#cb8-1"></a><span class="kw">dim</span>(ames.he)   <span class="co"># 2930 observations in total</span></span></code></pre></div>
<pre><code>## [1] 2930  310</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="xgboost.html#cb10-1"></a><span class="kw">dim</span>(ames_train)<span class="co"># 2053 observations for training data</span></span></code></pre></div>
<pre><code>## [1] 2053  310</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="xgboost.html#cb12-1"></a><span class="kw">dim</span>(ames_test) <span class="co"># 877 observations for testing data</span></span></code></pre></div>
<pre><code>## [1] 877 310</code></pre>
<p>We are still not done with preparation of data. Since our task is to build a predictive model for house pricing based on multiple features, our target variable (sale price) needs to be excluded from the test data. Newly created variable will be used at the end to test accuracy of our final model.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="xgboost.html#cb14-1"></a><span class="co"># Test data </span></span>
<span id="cb14-2"><a href="xgboost.html#cb14-2"></a><span class="co">## Matrix containing all columns from the test data except dependent variable &quot;Sale_Price&quot;</span></span>
<span id="cb14-3"><a href="xgboost.html#cb14-3"></a><span class="kw">colnames</span>(ames_test[,<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>])<span class="co"># &quot;sale_price&quot; is a column number 308</span></span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;sale_price&quot;</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="xgboost.html#cb16-1"></a>ames_x_test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ames_test[,<span class="op">-</span><span class="dv">308</span>]) <span class="co"># addressing &quot;Sale Price&quot; column in matrix and excluding it</span></span>
<span id="cb16-2"><a href="xgboost.html#cb16-2"></a><span class="kw">colnames</span>(ames_x_test)[<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>] <span class="co"># No &quot;Sale Price&quot; anymore here! It used to be among last columns.</span></span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;longitude&quot;</code></pre>
<p>For training purposes, target variable (sale price) needs to be excluded from the train data set as well.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="xgboost.html#cb18-1"></a><span class="co"># Train data set</span></span>
<span id="cb18-2"><a href="xgboost.html#cb18-2"></a><span class="co"># Matrix containing all columns from the train data except dependent variable &quot;Sale_Price&quot;</span></span>
<span id="cb18-3"><a href="xgboost.html#cb18-3"></a></span>
<span id="cb18-4"><a href="xgboost.html#cb18-4"></a><span class="kw">colnames</span>(ames_train[,<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>])<span class="co"># &quot;sale_price&quot; is a column number 308</span></span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;sale_price&quot;</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="xgboost.html#cb20-1"></a>ames_x_train &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ames_train[,<span class="op">-</span><span class="dv">308</span>])<span class="co"># addressing &quot;Sale Price&quot; column in matrix and excluding it</span></span>
<span id="cb20-2"><a href="xgboost.html#cb20-2"></a><span class="kw">colnames</span>(ames_x_train[,<span class="dv">300</span><span class="op">:</span><span class="dv">308</span>])<span class="co"># No &quot;Sale Price&quot; anymore here!</span></span></code></pre></div>
<pre><code>## [1] &quot;sale_typeOth&quot;          &quot;sale_typeVWD&quot;          &quot;sale_typeWD &quot;         
## [4] &quot;sale_conditionAdjLand&quot; &quot;sale_conditionAlloca&quot;  &quot;sale_conditionFamily&quot; 
## [7] &quot;sale_conditionNormal&quot;  &quot;sale_conditionPartial&quot; &quot;longitude&quot;</code></pre>
<p>However, the target variable is going to be stored separately because the learning algorithm in a predictive model attempts to discover and model the relationships among the target variable and the other features.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="xgboost.html#cb22-1"></a><span class="co"># Dependent/Target variable &quot;Sales_Price&quot; from the train data in a from of a vector</span></span>
<span id="cb22-2"><a href="xgboost.html#cb22-2"></a>ames_y_train &lt;-<span class="st"> </span>ames_train[,<span class="dv">308</span>]</span></code></pre></div>
</div>
<div id="engineering" class="section level3">
<h3><span class="header-section-number">14.3.3</span> Engineering</h3>
<p>In order to create a good predictive model, usually the most of time is spent optimizing parameters. Before we start training our model, let us take a closer look at what parameters we need to handle.</p>
<p>There are 3 categories XGBoost parameters can be divided into:</p>
<ol style="list-style-type: decimal">
<li>General parameters</li>
<li>Boosting parameters</li>
<li>Tree-specific paramters</li>
</ol>
<p>General parameters will not be discussed in further details, but it consists of 3 parameters:</p>
<ol style="list-style-type: decimal">
<li><code>booster</code> - determines the booster type (gbtree, gblinear or dart) to use. For regression, you can use any. By default it is gbtree (which we will use as well).<br />
</li>
<li><code>nthread</code> - refers to the number of cores activated when computing. By default it uses maximum cores available, which leads to the fastest computation.<br />
</li>
<li><code>silent</code> - refers to turning on (“1”) running messages in R console. By default “0” is set, so that console does not get flooded with messages.</li>
</ol>
<p>For general parameters we will be using default options.</p>
<p>Next, booster parameters control the performance of the selected booster(gbtree in our case). At this moment we will introduce just the main ones:</p>
<ol style="list-style-type: decimal">
<li><code>nrounds</code> - set the maximum number of iterations.<br />
</li>
<li><code>eta</code> - stands for the learning rate. It determines the rate at which our model learns patterns in data. After every iteration, it shrinks the feature weights to reach the best optimum. Smaller learning rates lead to longer computation time. It is important to note that smaller learning rates should be supported by increasing number of iterations. Otherwise, the risk of reaching the optimum is more likely. Usually, it lies between 0.01 - 0.3.</li>
<li><code>max_depth</code> - which determines the maximum depth of each tree. Generally, it is stands that larger the depth, more complex the model and consequently higher chances of overfitting.
4.<code>min_child_weight</code> - minimum number of observations required in each terminal node</li>
<li><code>subsample</code> - percent of training data to sample for each tree</li>
<li><code>colsample_bytrees</code> - percent of columns to sample from for each tree</li>
<li><code>early_stopping_rounds</code> - stopping the training model as soon as evaluation metric (for regression that is “RMSE”) does not improve for a given number of rounds</li>
</ol>
<p>Finally, learning task parameters define methods for the loss function and model evaluation:</p>
<ol style="list-style-type: decimal">
<li><code>objective</code>- for linear regression it should be set to “reg:linear”.</li>
<li><code>eval_metric</code> - this parameter depends on <code>objective</code>. Here we set metrics used to evaluate a model’s accuracy on validation data. When “reg:linear” set as objective, default metric is RMSE.</li>
</ol>
<p>A package with useful tools for parameter optimization is <code>mlr</code>. It includes extensive list of parameters for any type of algorithm. We can take a look at list with parameters for regression and check parameters we just discussed.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="xgboost.html#cb23-1"></a><span class="co"># Parameters for regression</span></span>
<span id="cb23-2"><a href="xgboost.html#cb23-2"></a><span class="kw">getParamSet</span>(<span class="st">&quot;regr.xgboost&quot;</span>)</span></code></pre></div>
<pre><code>##                                 Type  len           Def
## booster                     discrete    -        gbtree
## watchlist                    untyped    -        &lt;NULL&gt;
## eta                          numeric    -           0.3
## gamma                        numeric    -             0
## max_depth                    integer    -             6
## min_child_weight             numeric    -             1
## subsample                    numeric    -             1
## colsample_bytree             numeric    -             1
## colsample_bylevel            numeric    -             1
## colsample_bynode             numeric    -             1
## num_parallel_tree            integer    -             1
## lambda                       numeric    -             1
## lambda_bias                  numeric    -             0
## alpha                        numeric    -             0
## objective                    untyped    -    reg:linear
## eval_metric                  untyped    -          rmse
## base_score                   numeric    -           0.5
## max_delta_step               numeric    -             0
## missing                      numeric    -              
## monotone_constraints   integervector &lt;NA&gt;             0
## tweedie_variance_power       numeric    -           1.5
## nthread                      integer    -             -
## nrounds                      integer    -             -
## feval                        untyped    -        &lt;NULL&gt;
## verbose                      integer    -             1
## print_every_n                integer    -             1
## early_stopping_rounds        integer    -        &lt;NULL&gt;
## maximize                     logical    -        &lt;NULL&gt;
## sample_type                 discrete    -       uniform
## normalize_type              discrete    -          tree
## rate_drop                    numeric    -             0
## skip_drop                    numeric    -             0
## scale_pos_weight             numeric    -             1
## refresh_leaf                 logical    -          TRUE
## feature_selector            discrete    -        cyclic
## top_k                        integer    -             0
## predictor                   discrete    - cpu_predictor
## updater                      untyped    -             -
## sketch_eps                   numeric    -          0.03
## one_drop                     logical    -         FALSE
## tree_method                 discrete    -          auto
## grow_policy                 discrete    -     depthwise
## max_leaves                   integer    -             0
## max_bin                      integer    -           256
## callbacks                    untyped    -              
##                                                      Constr Req Tunable Trafo
## booster                                gbtree,gblinear,dart   -    TRUE     -
## watchlist                                                 -   -   FALSE     -
## eta                                                  0 to 1   -    TRUE     -
## gamma                                              0 to Inf   -    TRUE     -
## max_depth                                          0 to Inf   -    TRUE     -
## min_child_weight                                   0 to Inf   -    TRUE     -
## subsample                                            0 to 1   -    TRUE     -
## colsample_bytree                                     0 to 1   -    TRUE     -
## colsample_bylevel                                    0 to 1   -    TRUE     -
## colsample_bynode                                     0 to 1   -    TRUE     -
## num_parallel_tree                                  1 to Inf   -    TRUE     -
## lambda                                             0 to Inf   -    TRUE     -
## lambda_bias                                        0 to Inf   -    TRUE     -
## alpha                                              0 to Inf   -    TRUE     -
## objective                                                 -   -   FALSE     -
## eval_metric                                               -   -   FALSE     -
## base_score                                      -Inf to Inf   -   FALSE     -
## max_delta_step                                     0 to Inf   -    TRUE     -
## missing                                         -Inf to Inf   -   FALSE     -
## monotone_constraints                                -1 to 1   -    TRUE     -
## tweedie_variance_power                               1 to 2   Y    TRUE     -
## nthread                                            1 to Inf   -   FALSE     -
## nrounds                                            1 to Inf   -    TRUE     -
## feval                                                     -   -   FALSE     -
## verbose                                              0 to 2   -   FALSE     -
## print_every_n                                      1 to Inf   Y   FALSE     -
## early_stopping_rounds                              1 to Inf   -   FALSE     -
## maximize                                                  -   -   FALSE     -
## sample_type                                uniform,weighted   Y    TRUE     -
## normalize_type                                  tree,forest   Y    TRUE     -
## rate_drop                                            0 to 1   Y    TRUE     -
## skip_drop                                            0 to 1   Y    TRUE     -
## scale_pos_weight                                -Inf to Inf   -    TRUE     -
## refresh_leaf                                              -   -    TRUE     -
## feature_selector       cyclic,shuffle,random,greedy,thrifty   -    TRUE     -
## top_k                                              0 to Inf   -    TRUE     -
## predictor                       cpu_predictor,gpu_predictor   -    TRUE     -
## updater                                                   -   -    TRUE     -
## sketch_eps                                           0 to 1   -    TRUE     -
## one_drop                                                  -   Y    TRUE     -
## tree_method                 auto,exact,approx,hist,gpu_hist   Y    TRUE     -
## grow_policy                             depthwise,lossguide   Y    TRUE     -
## max_leaves                                         0 to Inf   Y    TRUE     -
## max_bin                                            2 to Inf   Y    TRUE     -
## callbacks                                                 -   -   FALSE     -</code></pre>
<p>To build a well-performing predictive model many iterations are necessary. Therefore, in order to determine how good or bad one model predicts the target variable, performance evaluation needs to be conducted. A technique that will be used to help us in evaluating performance of our future machine learning models is called <strong>k-fold cross-validation technique.</strong> K-fold cross-validation evaluates a model by training a couple of models on subsets of the available input data and evaluating them on the complementary subset of the data. In this process, training data is split into k groups (i.e. folds) of approximately equal size. Then the model is fit on k−1 folds and the remaining fold is used in computation of the model performance. This procedure is repeated k times, where each time, a different fold is treated as the validation set (i.e. used in computation of the model performance). Thus, the final cross-validation k-fold estimate is computed by averaging the k test errors. The final output provided is an approximation of the error we may expect on unseen data.</p>
<p>The first model to pass to the k-fold cross validation will be built using default parameters. As default for number of iterations (<code>nrounds</code>) is zero, we will set it on 200.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="xgboost.html#cb25-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb25-2"><a href="xgboost.html#cb25-2"></a>ames_xgb &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</span>
<span id="cb25-3"><a href="xgboost.html#cb25-3"></a>  <span class="dt">data =</span> ames_x_train,      <span class="co"># matrix with train data without sale price</span></span>
<span id="cb25-4"><a href="xgboost.html#cb25-4"></a>  <span class="dt">label =</span> ames_y_train,     <span class="co"># numerical vector with sale price with train data </span></span>
<span id="cb25-5"><a href="xgboost.html#cb25-5"></a>  <span class="dt">nrounds =</span> <span class="dv">200</span>,            <span class="co"># number of iterations </span></span>
<span id="cb25-6"><a href="xgboost.html#cb25-6"></a>  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>, <span class="co"># parameter referring to the function to be me minimised (RMSE)</span></span>
<span id="cb25-7"><a href="xgboost.html#cb25-7"></a>  <span class="dt">nfold =</span> <span class="dv">10</span>,                <span class="co"># data is randomly partitioned into n-fold equal size subsamples.</span></span>
<span id="cb25-8"><a href="xgboost.html#cb25-8"></a>  <span class="dt">params =</span> <span class="kw">list</span>(            <span class="co"># defining the list of parameters</span></span>
<span id="cb25-9"><a href="xgboost.html#cb25-9"></a>    <span class="dt">eta =</span> <span class="fl">0.3</span>,              <span class="co"># learning rate </span></span>
<span id="cb25-10"><a href="xgboost.html#cb25-10"></a>    <span class="dt">max_depth =</span> <span class="dv">6</span>,          <span class="co"># maximal depth of tree</span></span>
<span id="cb25-11"><a href="xgboost.html#cb25-11"></a>    <span class="dt">min_child_weight =</span> <span class="dv">1</span>,   <span class="co"># minimum number of observations required in each terminal node</span></span>
<span id="cb25-12"><a href="xgboost.html#cb25-12"></a>    <span class="dt">subsample =</span> <span class="dv">1</span>,          <span class="co"># percent of training data to sample for each tree</span></span>
<span id="cb25-13"><a href="xgboost.html#cb25-13"></a>    <span class="dt">colsample_bytree =</span> <span class="dv">1</span>    <span class="co"># percent of columns to sample from for each tree</span></span>
<span id="cb25-14"><a href="xgboost.html#cb25-14"></a>    ),</span>
<span id="cb25-15"><a href="xgboost.html#cb25-15"></a>  <span class="dt">verbose =</span> <span class="dv">0</span>               <span class="co"># print the statistics during the process (1 or 0)</span></span>
<span id="cb25-16"><a href="xgboost.html#cb25-16"></a>  )  </span></code></pre></div>
<pre><code>## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:04] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="xgboost.html#cb27-1"></a>(eval &lt;-<span class="st"> </span>ames_xgb<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span></span>
<span id="cb27-2"><a href="xgboost.html#cb27-2"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(</span>
<span id="cb27-3"><a href="xgboost.html#cb27-3"></a>    <span class="dt">ntrees.train =</span> <span class="kw">which</span>(train_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(train_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb27-4"><a href="xgboost.html#cb27-4"></a>    <span class="dt">rmse.train   =</span> <span class="kw">min</span>(train_rmse_mean),</span>
<span id="cb27-5"><a href="xgboost.html#cb27-5"></a>    <span class="dt">ntrees.test  =</span> <span class="kw">which</span>(test_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(test_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb27-6"><a href="xgboost.html#cb27-6"></a>    <span class="dt">rmse.test   =</span> <span class="kw">min</span>(test_rmse_mean)))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["ntrees.train"],"name":[1],"type":["int"],"align":["right"]},{"label":["rmse.train"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["ntrees.test"],"name":[3],"type":["int"],"align":["right"]},{"label":["rmse.test"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"200","2":"596.9639","3":"180","4":"25324.12"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="xgboost.html#cb28-1"></a><span class="kw">print</span>(eval<span class="op">$</span>rmse.train)</span></code></pre></div>
<pre><code>## [1] 596.9639</code></pre>
<p>After conducting the 10-fold cross validation, we sorted the output so that it shows us at what iteration (round) our model reached the lowest error (<code>eval$rmse.train</code>=) when fitted to the seen part of training data and unseen part of the training data (<code>eval$rmse.test</code>=).</p>
<p><em>Side note: unseen part of the training data (rmse.test) has nothing to do with test data from the initial split we did at the very beginNing of the chapter and named as <code>ames_test</code> . Here we talk about unseen data in the process of k-fold cross-validation.</em></p>
<p>Unsurprisingly, our model performed very well when fitted to the seen data, suggesting the RMSE of <code>eval$rmse.train</code>. Here we see an evidence of overfitting. In other words, our model fits the training part of the training data very well, but is not generalizable, i.e. when confronted with unseen data, its predictions are not as good as for the trained part.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="xgboost.html#cb30-1"></a><span class="co"># Plot error vs number trees</span></span>
<span id="cb30-2"><a href="xgboost.html#cb30-2"></a>pe&lt;-<span class="kw">ggplot</span>(ames_xgb<span class="op">$</span>evaluation_log) <span class="op">+</span></span>
<span id="cb30-3"><a href="xgboost.html#cb30-3"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, train_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb30-4"><a href="xgboost.html#cb30-4"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, test_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb30-5"><a href="xgboost.html#cb30-5"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="dt">label=</span> <span class="st">&quot;Iteration (round)&quot;</span>)<span class="op">+</span></span>
<span id="cb30-6"><a href="xgboost.html#cb30-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="dt">label =</span> <span class="st">&quot;Root Mean Square Error&quot;</span>)<span class="op">+</span></span>
<span id="cb30-7"><a href="xgboost.html#cb30-7"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="dt">label =</span> <span class="st">&quot;10-fold Cross-validation&quot;</span>)</span>
<span id="cb30-8"><a href="xgboost.html#cb30-8"></a><span class="kw">ggplotly</span>(pe)</span></code></pre></div>
<div id="htmlwidget-3ac916349136fc93fcf5" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-3ac916349136fc93fcf5">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[140594.584375,101286.6867191,73807.2992187,54337.7453124,40785.1941407,31380.5322266,24898.7701174,20446.4482421,17473.6291017,15400.1717773,13967.5166992,12978.5577148,12269.1211913,11734.381543,11249.5087889,10852.1914063,10512.8967773,10225.834375,9994.7420898,9751.9199219,9519.4707031,9315.4803711,9117.5376953,8910.4476562,8750.9583982,8593.2,8448.3758789,8278.2055175,8106.6849121,7939.7208497,7771.9758787,7613.2281249,7462.2565431,7337.0039061,7215.4793945,7085.5856446,6910.9421874,6796.2040525,6677.9723143,6574.5823733,6457.2341309,6338.7859863,6233.3975586,6160.2472169,6055.0294922,5962.9359374,5841.8112305,5740.5357911,5645.0680177,5540.2312012,5444.6583007,5364.8390137,5288.4353515,5201.0837403,5128.1606445,5069.8740233,4996.0522463,4919.8980958,4822.187793,4739.6396485,4670.4356447,4613.4902832,4552.1994141,4489.4088377,4403.4343262,4304.1591552,4231.0445556,4165.1487548,4090.1140381,4008.9814942,3944.6739745,3891.482251,3829.4938722,3761.4834472,3726.1801758,3671.3132814,3630.2940919,3580.0728515,3520.8024657,3471.8132812,3411.6532958,3349.0892578,3293.5904052,3253.8236328,3192.9784668,3148.8376954,3108.5681396,3052.2455811,3007.3830078,2954.0198487,2911.3963135,2867.298999,2819.4181153,2776.9638428,2727.8215576,2693.3881592,2663.077539,2617.4836182,2573.2193848,2532.8037842,2499.63125,2462.9624513,2432.0398438,2401.1086425,2370.0609131,2341.9923708,2304.8312989,2267.9302124,2229.6167969,2202.0569825,2165.8003297,2138.5861695,2106.602722,2074.371228,2047.8173338,2014.0364135,1993.7470216,1967.0413207,1941.7842408,1918.6677857,1894.6833861,1864.8548583,1835.1145019,1795.4808105,1763.2951782,1738.6042603,1710.9897583,1687.8432495,1661.0685059,1631.4690186,1613.644336,1588.7129029,1563.7790406,1531.9783569,1506.9308106,1487.3664307,1460.6477905,1441.3762329,1419.9146241,1397.0803223,1378.6616577,1361.2244995,1344.8429687,1329.2414673,1308.6594483,1282.1494265,1259.7858643,1240.2073486,1221.02854,1205.4603698,1190.8673888,1177.3331053,1162.7586548,1151.5119873,1140.4366821,1125.1754579,1107.6707276,1092.8359314,1075.6617982,1061.7394897,1044.5710511,1027.747229,1014.5498536,1002.2487854,983.644104,969.4414,952.2909852,936.2260744,922.0340517,907.7742554,898.2338135,886.1614807,872.0424926,862.0117431,847.1053529,835.2450867,826.1704223,817.2422912,807.975293,794.2547303,783.8483766,773.7636108,760.4451111,749.0615723,738.7089599,728.3018922,718.2620666,709.0963683,697.2377988,688.6634644,678.9270875,668.306317,658.0195071,649.9507569,639.8951108,631.664569,622.9106049,612.372888,602.3153779,596.9639069],"text":["iter:   1<br />train_rmse_mean: 140594.5844","iter:   2<br />train_rmse_mean: 101286.6867","iter:   3<br />train_rmse_mean:  73807.2992","iter:   4<br />train_rmse_mean:  54337.7453","iter:   5<br />train_rmse_mean:  40785.1941","iter:   6<br />train_rmse_mean:  31380.5322","iter:   7<br />train_rmse_mean:  24898.7701","iter:   8<br />train_rmse_mean:  20446.4482","iter:   9<br />train_rmse_mean:  17473.6291","iter:  10<br />train_rmse_mean:  15400.1718","iter:  11<br />train_rmse_mean:  13967.5167","iter:  12<br />train_rmse_mean:  12978.5577","iter:  13<br />train_rmse_mean:  12269.1212","iter:  14<br />train_rmse_mean:  11734.3815","iter:  15<br />train_rmse_mean:  11249.5088","iter:  16<br />train_rmse_mean:  10852.1914","iter:  17<br />train_rmse_mean:  10512.8968","iter:  18<br />train_rmse_mean:  10225.8344","iter:  19<br />train_rmse_mean:   9994.7421","iter:  20<br />train_rmse_mean:   9751.9199","iter:  21<br />train_rmse_mean:   9519.4707","iter:  22<br />train_rmse_mean:   9315.4804","iter:  23<br />train_rmse_mean:   9117.5377","iter:  24<br />train_rmse_mean:   8910.4477","iter:  25<br />train_rmse_mean:   8750.9584","iter:  26<br />train_rmse_mean:   8593.2000","iter:  27<br />train_rmse_mean:   8448.3759","iter:  28<br />train_rmse_mean:   8278.2055","iter:  29<br />train_rmse_mean:   8106.6849","iter:  30<br />train_rmse_mean:   7939.7208","iter:  31<br />train_rmse_mean:   7771.9759","iter:  32<br />train_rmse_mean:   7613.2281","iter:  33<br />train_rmse_mean:   7462.2565","iter:  34<br />train_rmse_mean:   7337.0039","iter:  35<br />train_rmse_mean:   7215.4794","iter:  36<br />train_rmse_mean:   7085.5856","iter:  37<br />train_rmse_mean:   6910.9422","iter:  38<br />train_rmse_mean:   6796.2041","iter:  39<br />train_rmse_mean:   6677.9723","iter:  40<br />train_rmse_mean:   6574.5824","iter:  41<br />train_rmse_mean:   6457.2341","iter:  42<br />train_rmse_mean:   6338.7860","iter:  43<br />train_rmse_mean:   6233.3976","iter:  44<br />train_rmse_mean:   6160.2472","iter:  45<br />train_rmse_mean:   6055.0295","iter:  46<br />train_rmse_mean:   5962.9359","iter:  47<br />train_rmse_mean:   5841.8112","iter:  48<br />train_rmse_mean:   5740.5358","iter:  49<br />train_rmse_mean:   5645.0680","iter:  50<br />train_rmse_mean:   5540.2312","iter:  51<br />train_rmse_mean:   5444.6583","iter:  52<br />train_rmse_mean:   5364.8390","iter:  53<br />train_rmse_mean:   5288.4354","iter:  54<br />train_rmse_mean:   5201.0837","iter:  55<br />train_rmse_mean:   5128.1606","iter:  56<br />train_rmse_mean:   5069.8740","iter:  57<br />train_rmse_mean:   4996.0522","iter:  58<br />train_rmse_mean:   4919.8981","iter:  59<br />train_rmse_mean:   4822.1878","iter:  60<br />train_rmse_mean:   4739.6396","iter:  61<br />train_rmse_mean:   4670.4356","iter:  62<br />train_rmse_mean:   4613.4903","iter:  63<br />train_rmse_mean:   4552.1994","iter:  64<br />train_rmse_mean:   4489.4088","iter:  65<br />train_rmse_mean:   4403.4343","iter:  66<br />train_rmse_mean:   4304.1592","iter:  67<br />train_rmse_mean:   4231.0446","iter:  68<br />train_rmse_mean:   4165.1488","iter:  69<br />train_rmse_mean:   4090.1140","iter:  70<br />train_rmse_mean:   4008.9815","iter:  71<br />train_rmse_mean:   3944.6740","iter:  72<br />train_rmse_mean:   3891.4823","iter:  73<br />train_rmse_mean:   3829.4939","iter:  74<br />train_rmse_mean:   3761.4834","iter:  75<br />train_rmse_mean:   3726.1802","iter:  76<br />train_rmse_mean:   3671.3133","iter:  77<br />train_rmse_mean:   3630.2941","iter:  78<br />train_rmse_mean:   3580.0729","iter:  79<br />train_rmse_mean:   3520.8025","iter:  80<br />train_rmse_mean:   3471.8133","iter:  81<br />train_rmse_mean:   3411.6533","iter:  82<br />train_rmse_mean:   3349.0893","iter:  83<br />train_rmse_mean:   3293.5904","iter:  84<br />train_rmse_mean:   3253.8236","iter:  85<br />train_rmse_mean:   3192.9785","iter:  86<br />train_rmse_mean:   3148.8377","iter:  87<br />train_rmse_mean:   3108.5681","iter:  88<br />train_rmse_mean:   3052.2456","iter:  89<br />train_rmse_mean:   3007.3830","iter:  90<br />train_rmse_mean:   2954.0198","iter:  91<br />train_rmse_mean:   2911.3963","iter:  92<br />train_rmse_mean:   2867.2990","iter:  93<br />train_rmse_mean:   2819.4181","iter:  94<br />train_rmse_mean:   2776.9638","iter:  95<br />train_rmse_mean:   2727.8216","iter:  96<br />train_rmse_mean:   2693.3882","iter:  97<br />train_rmse_mean:   2663.0775","iter:  98<br />train_rmse_mean:   2617.4836","iter:  99<br />train_rmse_mean:   2573.2194","iter: 100<br />train_rmse_mean:   2532.8038","iter: 101<br />train_rmse_mean:   2499.6312","iter: 102<br />train_rmse_mean:   2462.9625","iter: 103<br />train_rmse_mean:   2432.0398","iter: 104<br />train_rmse_mean:   2401.1086","iter: 105<br />train_rmse_mean:   2370.0609","iter: 106<br />train_rmse_mean:   2341.9924","iter: 107<br />train_rmse_mean:   2304.8313","iter: 108<br />train_rmse_mean:   2267.9302","iter: 109<br />train_rmse_mean:   2229.6168","iter: 110<br />train_rmse_mean:   2202.0570","iter: 111<br />train_rmse_mean:   2165.8003","iter: 112<br />train_rmse_mean:   2138.5862","iter: 113<br />train_rmse_mean:   2106.6027","iter: 114<br />train_rmse_mean:   2074.3712","iter: 115<br />train_rmse_mean:   2047.8173","iter: 116<br />train_rmse_mean:   2014.0364","iter: 117<br />train_rmse_mean:   1993.7470","iter: 118<br />train_rmse_mean:   1967.0413","iter: 119<br />train_rmse_mean:   1941.7842","iter: 120<br />train_rmse_mean:   1918.6678","iter: 121<br />train_rmse_mean:   1894.6834","iter: 122<br />train_rmse_mean:   1864.8549","iter: 123<br />train_rmse_mean:   1835.1145","iter: 124<br />train_rmse_mean:   1795.4808","iter: 125<br />train_rmse_mean:   1763.2952","iter: 126<br />train_rmse_mean:   1738.6043","iter: 127<br />train_rmse_mean:   1710.9898","iter: 128<br />train_rmse_mean:   1687.8432","iter: 129<br />train_rmse_mean:   1661.0685","iter: 130<br />train_rmse_mean:   1631.4690","iter: 131<br />train_rmse_mean:   1613.6443","iter: 132<br />train_rmse_mean:   1588.7129","iter: 133<br />train_rmse_mean:   1563.7790","iter: 134<br />train_rmse_mean:   1531.9784","iter: 135<br />train_rmse_mean:   1506.9308","iter: 136<br />train_rmse_mean:   1487.3664","iter: 137<br />train_rmse_mean:   1460.6478","iter: 138<br />train_rmse_mean:   1441.3762","iter: 139<br />train_rmse_mean:   1419.9146","iter: 140<br />train_rmse_mean:   1397.0803","iter: 141<br />train_rmse_mean:   1378.6617","iter: 142<br />train_rmse_mean:   1361.2245","iter: 143<br />train_rmse_mean:   1344.8430","iter: 144<br />train_rmse_mean:   1329.2415","iter: 145<br />train_rmse_mean:   1308.6594","iter: 146<br />train_rmse_mean:   1282.1494","iter: 147<br />train_rmse_mean:   1259.7859","iter: 148<br />train_rmse_mean:   1240.2073","iter: 149<br />train_rmse_mean:   1221.0285","iter: 150<br />train_rmse_mean:   1205.4604","iter: 151<br />train_rmse_mean:   1190.8674","iter: 152<br />train_rmse_mean:   1177.3331","iter: 153<br />train_rmse_mean:   1162.7587","iter: 154<br />train_rmse_mean:   1151.5120","iter: 155<br />train_rmse_mean:   1140.4367","iter: 156<br />train_rmse_mean:   1125.1755","iter: 157<br />train_rmse_mean:   1107.6707","iter: 158<br />train_rmse_mean:   1092.8359","iter: 159<br />train_rmse_mean:   1075.6618","iter: 160<br />train_rmse_mean:   1061.7395","iter: 161<br />train_rmse_mean:   1044.5711","iter: 162<br />train_rmse_mean:   1027.7472","iter: 163<br />train_rmse_mean:   1014.5499","iter: 164<br />train_rmse_mean:   1002.2488","iter: 165<br />train_rmse_mean:    983.6441","iter: 166<br />train_rmse_mean:    969.4414","iter: 167<br />train_rmse_mean:    952.2910","iter: 168<br />train_rmse_mean:    936.2261","iter: 169<br />train_rmse_mean:    922.0341","iter: 170<br />train_rmse_mean:    907.7743","iter: 171<br />train_rmse_mean:    898.2338","iter: 172<br />train_rmse_mean:    886.1615","iter: 173<br />train_rmse_mean:    872.0425","iter: 174<br />train_rmse_mean:    862.0117","iter: 175<br />train_rmse_mean:    847.1054","iter: 176<br />train_rmse_mean:    835.2451","iter: 177<br />train_rmse_mean:    826.1704","iter: 178<br />train_rmse_mean:    817.2423","iter: 179<br />train_rmse_mean:    807.9753","iter: 180<br />train_rmse_mean:    794.2547","iter: 181<br />train_rmse_mean:    783.8484","iter: 182<br />train_rmse_mean:    773.7636","iter: 183<br />train_rmse_mean:    760.4451","iter: 184<br />train_rmse_mean:    749.0616","iter: 185<br />train_rmse_mean:    738.7090","iter: 186<br />train_rmse_mean:    728.3019","iter: 187<br />train_rmse_mean:    718.2621","iter: 188<br />train_rmse_mean:    709.0964","iter: 189<br />train_rmse_mean:    697.2378","iter: 190<br />train_rmse_mean:    688.6635","iter: 191<br />train_rmse_mean:    678.9271","iter: 192<br />train_rmse_mean:    668.3063","iter: 193<br />train_rmse_mean:    658.0195","iter: 194<br />train_rmse_mean:    649.9508","iter: 195<br />train_rmse_mean:    639.8951","iter: 196<br />train_rmse_mean:    631.6646","iter: 197<br />train_rmse_mean:    622.9106","iter: 198<br />train_rmse_mean:    612.3729","iter: 199<br />train_rmse_mean:    602.3154","iter: 200<br />train_rmse_mean:    596.9639"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[140830.0953125,102345.2250001,75785.7101561,57672.9679688,45915.3250002,38311.3929687,33616.81875,30879.5787108,29185.3,28172.7208985,27607.0394532,27279.8892578,27061.8238281,26881.2214844,26793.7441406,26636.1140624,26526.2492185,26461.0828124,26375.2814454,26277.4503905,26193.3626954,26127.6943359,26053.3458984,26004.0568359,25962.8052734,25943.5228515,25885.6171876,25876.8292969,25862.4749998,25857.2537109,25851.8718751,25804.6171874,25744.5046873,25729.4595704,25712.5009763,25687.5533203,25652.7568361,25644.3394531,25622.3550782,25614.5439454,25587.1650391,25588.391797,25553.5650392,25563.8392578,25565.597461,25558.894922,25559.4990234,25530.3472655,25533.0353515,25538.9937499,25521.7060546,25510.8697266,25501.2828125,25491.6111329,25494.9958985,25482.5375,25471.4603516,25468.3007812,25475.3955078,25477.728125,25466.0451171,25463.3246095,25463.7367187,25464.5193361,25450.4017579,25440.8671874,25428.3402343,25427.5576172,25416.8119139,25423.8158204,25421.6410157,25434.1550782,25437.7244141,25419.9603515,25420.550586,25410.667578,25407.8673829,25412.2046875,25400.1414062,25398.93125,25388.6587889,25387.0603516,25393.4564452,25380.0558592,25385.3869141,25393.2226562,25389.4341797,25386.0212893,25382.3292969,25379.0605469,25377.0248046,25374.8541014,25370.3861328,25361.2626954,25357.2740234,25356.1664061,25359.553125,25354.3246095,25350.5222656,25350.6003907,25355.1400391,25350.830664,25349.5009766,25350.7478516,25349.5787109,25351.8269532,25348.1710939,25344.3224608,25349.6769532,25346.9449221,25340.8806641,25343.3119139,25344.7365235,25345.3154296,25347.8003907,25343.7220703,25344.9958983,25347.4574219,25347.4783203,25347.051953,25348.3916016,25350.9445311,25347.7277343,25349.5507813,25345.4978514,25342.9316407,25344.1292969,25343.3724609,25342.9531251,25336.3380859,25337.2625001,25336.9708984,25337.0501952,25335.7666016,25333.378711,25332.696289,25332.6228515,25330.8492188,25330.9353516,25326.6519532,25326.8830078,25326.2708984,25327.1251953,25326.4398438,25326.8103516,25324.4328124,25327.0125001,25327.3517578,25328.8878907,25330.1875001,25331.8197265,25333.0693358,25332.1882813,25331.2546874,25330.8001954,25334.4599608,25334.1427734,25333.5734376,25334.2324219,25330.0501954,25328.7503904,25328.5691407,25328.2818359,25330.2099609,25329.8851564,25329.3031252,25331.8753905,25330.8138672,25329.659961,25329.7888672,25330.2810547,25330.6986328,25329.5767577,25329.8845702,25329.5523437,25327.6296875,25325.6216797,25324.7634766,25326.0648438,25324.1212892,25325.8197266,25325.7359375,25325.784375,25326.0169923,25326.1857421,25326.2892577,25327.3462891,25328.4289063,25329.7216797,25327.9978514,25327.6835936,25329.2515626,25330.9572266,25330.2986328,25328.8724608,25328.0289063,25328.3859376,25328.9691407,25326.942969,25327.2769531],"text":["iter:   1<br />test_rmse_mean: 140830.10","iter:   2<br />test_rmse_mean: 102345.23","iter:   3<br />test_rmse_mean:  75785.71","iter:   4<br />test_rmse_mean:  57672.97","iter:   5<br />test_rmse_mean:  45915.33","iter:   6<br />test_rmse_mean:  38311.39","iter:   7<br />test_rmse_mean:  33616.82","iter:   8<br />test_rmse_mean:  30879.58","iter:   9<br />test_rmse_mean:  29185.30","iter:  10<br />test_rmse_mean:  28172.72","iter:  11<br />test_rmse_mean:  27607.04","iter:  12<br />test_rmse_mean:  27279.89","iter:  13<br />test_rmse_mean:  27061.82","iter:  14<br />test_rmse_mean:  26881.22","iter:  15<br />test_rmse_mean:  26793.74","iter:  16<br />test_rmse_mean:  26636.11","iter:  17<br />test_rmse_mean:  26526.25","iter:  18<br />test_rmse_mean:  26461.08","iter:  19<br />test_rmse_mean:  26375.28","iter:  20<br />test_rmse_mean:  26277.45","iter:  21<br />test_rmse_mean:  26193.36","iter:  22<br />test_rmse_mean:  26127.69","iter:  23<br />test_rmse_mean:  26053.35","iter:  24<br />test_rmse_mean:  26004.06","iter:  25<br />test_rmse_mean:  25962.81","iter:  26<br />test_rmse_mean:  25943.52","iter:  27<br />test_rmse_mean:  25885.62","iter:  28<br />test_rmse_mean:  25876.83","iter:  29<br />test_rmse_mean:  25862.47","iter:  30<br />test_rmse_mean:  25857.25","iter:  31<br />test_rmse_mean:  25851.87","iter:  32<br />test_rmse_mean:  25804.62","iter:  33<br />test_rmse_mean:  25744.50","iter:  34<br />test_rmse_mean:  25729.46","iter:  35<br />test_rmse_mean:  25712.50","iter:  36<br />test_rmse_mean:  25687.55","iter:  37<br />test_rmse_mean:  25652.76","iter:  38<br />test_rmse_mean:  25644.34","iter:  39<br />test_rmse_mean:  25622.36","iter:  40<br />test_rmse_mean:  25614.54","iter:  41<br />test_rmse_mean:  25587.17","iter:  42<br />test_rmse_mean:  25588.39","iter:  43<br />test_rmse_mean:  25553.57","iter:  44<br />test_rmse_mean:  25563.84","iter:  45<br />test_rmse_mean:  25565.60","iter:  46<br />test_rmse_mean:  25558.89","iter:  47<br />test_rmse_mean:  25559.50","iter:  48<br />test_rmse_mean:  25530.35","iter:  49<br />test_rmse_mean:  25533.04","iter:  50<br />test_rmse_mean:  25538.99","iter:  51<br />test_rmse_mean:  25521.71","iter:  52<br />test_rmse_mean:  25510.87","iter:  53<br />test_rmse_mean:  25501.28","iter:  54<br />test_rmse_mean:  25491.61","iter:  55<br />test_rmse_mean:  25495.00","iter:  56<br />test_rmse_mean:  25482.54","iter:  57<br />test_rmse_mean:  25471.46","iter:  58<br />test_rmse_mean:  25468.30","iter:  59<br />test_rmse_mean:  25475.40","iter:  60<br />test_rmse_mean:  25477.73","iter:  61<br />test_rmse_mean:  25466.05","iter:  62<br />test_rmse_mean:  25463.32","iter:  63<br />test_rmse_mean:  25463.74","iter:  64<br />test_rmse_mean:  25464.52","iter:  65<br />test_rmse_mean:  25450.40","iter:  66<br />test_rmse_mean:  25440.87","iter:  67<br />test_rmse_mean:  25428.34","iter:  68<br />test_rmse_mean:  25427.56","iter:  69<br />test_rmse_mean:  25416.81","iter:  70<br />test_rmse_mean:  25423.82","iter:  71<br />test_rmse_mean:  25421.64","iter:  72<br />test_rmse_mean:  25434.16","iter:  73<br />test_rmse_mean:  25437.72","iter:  74<br />test_rmse_mean:  25419.96","iter:  75<br />test_rmse_mean:  25420.55","iter:  76<br />test_rmse_mean:  25410.67","iter:  77<br />test_rmse_mean:  25407.87","iter:  78<br />test_rmse_mean:  25412.20","iter:  79<br />test_rmse_mean:  25400.14","iter:  80<br />test_rmse_mean:  25398.93","iter:  81<br />test_rmse_mean:  25388.66","iter:  82<br />test_rmse_mean:  25387.06","iter:  83<br />test_rmse_mean:  25393.46","iter:  84<br />test_rmse_mean:  25380.06","iter:  85<br />test_rmse_mean:  25385.39","iter:  86<br />test_rmse_mean:  25393.22","iter:  87<br />test_rmse_mean:  25389.43","iter:  88<br />test_rmse_mean:  25386.02","iter:  89<br />test_rmse_mean:  25382.33","iter:  90<br />test_rmse_mean:  25379.06","iter:  91<br />test_rmse_mean:  25377.02","iter:  92<br />test_rmse_mean:  25374.85","iter:  93<br />test_rmse_mean:  25370.39","iter:  94<br />test_rmse_mean:  25361.26","iter:  95<br />test_rmse_mean:  25357.27","iter:  96<br />test_rmse_mean:  25356.17","iter:  97<br />test_rmse_mean:  25359.55","iter:  98<br />test_rmse_mean:  25354.32","iter:  99<br />test_rmse_mean:  25350.52","iter: 100<br />test_rmse_mean:  25350.60","iter: 101<br />test_rmse_mean:  25355.14","iter: 102<br />test_rmse_mean:  25350.83","iter: 103<br />test_rmse_mean:  25349.50","iter: 104<br />test_rmse_mean:  25350.75","iter: 105<br />test_rmse_mean:  25349.58","iter: 106<br />test_rmse_mean:  25351.83","iter: 107<br />test_rmse_mean:  25348.17","iter: 108<br />test_rmse_mean:  25344.32","iter: 109<br />test_rmse_mean:  25349.68","iter: 110<br />test_rmse_mean:  25346.94","iter: 111<br />test_rmse_mean:  25340.88","iter: 112<br />test_rmse_mean:  25343.31","iter: 113<br />test_rmse_mean:  25344.74","iter: 114<br />test_rmse_mean:  25345.32","iter: 115<br />test_rmse_mean:  25347.80","iter: 116<br />test_rmse_mean:  25343.72","iter: 117<br />test_rmse_mean:  25345.00","iter: 118<br />test_rmse_mean:  25347.46","iter: 119<br />test_rmse_mean:  25347.48","iter: 120<br />test_rmse_mean:  25347.05","iter: 121<br />test_rmse_mean:  25348.39","iter: 122<br />test_rmse_mean:  25350.94","iter: 123<br />test_rmse_mean:  25347.73","iter: 124<br />test_rmse_mean:  25349.55","iter: 125<br />test_rmse_mean:  25345.50","iter: 126<br />test_rmse_mean:  25342.93","iter: 127<br />test_rmse_mean:  25344.13","iter: 128<br />test_rmse_mean:  25343.37","iter: 129<br />test_rmse_mean:  25342.95","iter: 130<br />test_rmse_mean:  25336.34","iter: 131<br />test_rmse_mean:  25337.26","iter: 132<br />test_rmse_mean:  25336.97","iter: 133<br />test_rmse_mean:  25337.05","iter: 134<br />test_rmse_mean:  25335.77","iter: 135<br />test_rmse_mean:  25333.38","iter: 136<br />test_rmse_mean:  25332.70","iter: 137<br />test_rmse_mean:  25332.62","iter: 138<br />test_rmse_mean:  25330.85","iter: 139<br />test_rmse_mean:  25330.94","iter: 140<br />test_rmse_mean:  25326.65","iter: 141<br />test_rmse_mean:  25326.88","iter: 142<br />test_rmse_mean:  25326.27","iter: 143<br />test_rmse_mean:  25327.13","iter: 144<br />test_rmse_mean:  25326.44","iter: 145<br />test_rmse_mean:  25326.81","iter: 146<br />test_rmse_mean:  25324.43","iter: 147<br />test_rmse_mean:  25327.01","iter: 148<br />test_rmse_mean:  25327.35","iter: 149<br />test_rmse_mean:  25328.89","iter: 150<br />test_rmse_mean:  25330.19","iter: 151<br />test_rmse_mean:  25331.82","iter: 152<br />test_rmse_mean:  25333.07","iter: 153<br />test_rmse_mean:  25332.19","iter: 154<br />test_rmse_mean:  25331.25","iter: 155<br />test_rmse_mean:  25330.80","iter: 156<br />test_rmse_mean:  25334.46","iter: 157<br />test_rmse_mean:  25334.14","iter: 158<br />test_rmse_mean:  25333.57","iter: 159<br />test_rmse_mean:  25334.23","iter: 160<br />test_rmse_mean:  25330.05","iter: 161<br />test_rmse_mean:  25328.75","iter: 162<br />test_rmse_mean:  25328.57","iter: 163<br />test_rmse_mean:  25328.28","iter: 164<br />test_rmse_mean:  25330.21","iter: 165<br />test_rmse_mean:  25329.89","iter: 166<br />test_rmse_mean:  25329.30","iter: 167<br />test_rmse_mean:  25331.88","iter: 168<br />test_rmse_mean:  25330.81","iter: 169<br />test_rmse_mean:  25329.66","iter: 170<br />test_rmse_mean:  25329.79","iter: 171<br />test_rmse_mean:  25330.28","iter: 172<br />test_rmse_mean:  25330.70","iter: 173<br />test_rmse_mean:  25329.58","iter: 174<br />test_rmse_mean:  25329.88","iter: 175<br />test_rmse_mean:  25329.55","iter: 176<br />test_rmse_mean:  25327.63","iter: 177<br />test_rmse_mean:  25325.62","iter: 178<br />test_rmse_mean:  25324.76","iter: 179<br />test_rmse_mean:  25326.06","iter: 180<br />test_rmse_mean:  25324.12","iter: 181<br />test_rmse_mean:  25325.82","iter: 182<br />test_rmse_mean:  25325.74","iter: 183<br />test_rmse_mean:  25325.78","iter: 184<br />test_rmse_mean:  25326.02","iter: 185<br />test_rmse_mean:  25326.19","iter: 186<br />test_rmse_mean:  25326.29","iter: 187<br />test_rmse_mean:  25327.35","iter: 188<br />test_rmse_mean:  25328.43","iter: 189<br />test_rmse_mean:  25329.72","iter: 190<br />test_rmse_mean:  25328.00","iter: 191<br />test_rmse_mean:  25327.68","iter: 192<br />test_rmse_mean:  25329.25","iter: 193<br />test_rmse_mean:  25330.96","iter: 194<br />test_rmse_mean:  25330.30","iter: 195<br />test_rmse_mean:  25328.87","iter: 196<br />test_rmse_mean:  25328.03","iter: 197<br />test_rmse_mean:  25328.39","iter: 198<br />test_rmse_mean:  25328.97","iter: 199<br />test_rmse_mean:  25326.94","iter: 200<br />test_rmse_mean:  25327.28"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"10-fold Cross-validation","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-8.95,209.95],"tickmode":"array","ticktext":["0","50","100","150","200"],"tickvals":[0,50,100,150,200],"categoryorder":"array","categoryarray":["0","50","100","150","200"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Iteration (round)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-6414.69266338,147841.75188278],"tickmode":"array","ticktext":["0","50000","100000"],"tickvals":[0,50000,100000],"categoryorder":"array","categoryarray":["0","50000","100000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Root Mean Square Error","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"235851cf384e":{"x":{},"y":{},"type":"scatter"},"235834d67c7d":{"x":{},"y":{}}},"cur_data":"235851cf384e","visdat":{"235851cf384e":["function (y) ","x"],"235834d67c7d":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The gap between the blue and the red line in the graph above depicts the performance difference of our model when fitted to the seen data (the red line) and when fitted to the unseen data (the blue line).
Our goal is to make our model perform with unseen data as good as possible, i.e. to minimize RMSE as much as possible.</p>
<p>To pursue that goal, parameters, that we discussed earlier, need to be as optimally tuned as possible.</p>
<p>Therefore, some parameters should be adapted. For the following cross-validation process, we will slightly adapt parameters:</p>
<ol style="list-style-type: decimal">
<li>Decrease the learning rate <code>eta</code> from 0.3 to 0.03</li>
<li>Set <code>early_stopping_rounds</code> at 50</li>
<li>Reduce maximum tree depth <code>max_depth</code> to 3</li>
<li>Increase minimum number of observations required in each terminal node <code>min_child_weight</code> to 3</li>
<li>Reduce <code>subsample</code> to 0.5</li>
<li><code>colsample_bytree</code> reduced to 0.5</li>
</ol>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="xgboost.html#cb31-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb31-2"><a href="xgboost.html#cb31-2"></a>ames_xgb1 &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</span>
<span id="cb31-3"><a href="xgboost.html#cb31-3"></a>  <span class="dt">data =</span> ames_x_train,           <span class="co"># matrix with train data without sale price</span></span>
<span id="cb31-4"><a href="xgboost.html#cb31-4"></a>  <span class="dt">label =</span> ames_y_train,          <span class="co"># numerical vector with sale price with train data </span></span>
<span id="cb31-5"><a href="xgboost.html#cb31-5"></a>  <span class="dt">nrounds =</span> <span class="dv">2301</span>,                <span class="co"># number of iterations </span></span>
<span id="cb31-6"><a href="xgboost.html#cb31-6"></a>  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,      <span class="co"># indicating regression model</span></span>
<span id="cb31-7"><a href="xgboost.html#cb31-7"></a>  <span class="dt">early_stopping_rounds =</span> <span class="dv">50</span>,    <span class="co"># stopping the training model as soon as evaluation metric (RMSE) does not improve for a given number of rounds</span></span>
<span id="cb31-8"><a href="xgboost.html#cb31-8"></a>  <span class="dt">nfold =</span> <span class="dv">10</span>,                     <span class="co"># data is randomly partitioned into nfold equal size subsamples</span></span>
<span id="cb31-9"><a href="xgboost.html#cb31-9"></a>  <span class="dt">params =</span> <span class="kw">list</span>(</span>
<span id="cb31-10"><a href="xgboost.html#cb31-10"></a>    <span class="dt">eta =</span> <span class="fl">0.03</span>,                  <span class="co"># learning rate </span></span>
<span id="cb31-11"><a href="xgboost.html#cb31-11"></a>    <span class="dt">max_depth =</span> <span class="dv">3</span>,               <span class="co"># maximal depth of tree</span></span>
<span id="cb31-12"><a href="xgboost.html#cb31-12"></a>    <span class="dt">min_child_weight =</span> <span class="dv">3</span>,        <span class="co"># minimum number of observations required in each terminal node</span></span>
<span id="cb31-13"><a href="xgboost.html#cb31-13"></a>    <span class="dt">subsample =</span> <span class="fl">0.5</span>,             <span class="co"># percent of training data to sample for each tree</span></span>
<span id="cb31-14"><a href="xgboost.html#cb31-14"></a>    <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>       <span class="co"># percent of columns to sample from for each tree</span></span>
<span id="cb31-15"><a href="xgboost.html#cb31-15"></a>    ),</span>
<span id="cb31-16"><a href="xgboost.html#cb31-16"></a>  <span class="dt">verbose =</span> <span class="dv">0</span></span>
<span id="cb31-17"><a href="xgboost.html#cb31-17"></a>) </span></code></pre></div>
<pre><code>## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [14:01:40] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="xgboost.html#cb33-1"></a><span class="co"># Checking results</span></span>
<span id="cb33-2"><a href="xgboost.html#cb33-2"></a>(eval1&lt;-ames_xgb1<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span></span>
<span id="cb33-3"><a href="xgboost.html#cb33-3"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(</span>
<span id="cb33-4"><a href="xgboost.html#cb33-4"></a>    <span class="dt">ntrees.train =</span> <span class="kw">which</span>(train_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(train_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb33-5"><a href="xgboost.html#cb33-5"></a>    <span class="dt">rmse.train   =</span> <span class="kw">min</span>(train_rmse_mean),</span>
<span id="cb33-6"><a href="xgboost.html#cb33-6"></a>    <span class="dt">ntrees.test  =</span> <span class="kw">which</span>(test_rmse_mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(test_rmse_mean))[<span class="dv">1</span>],</span>
<span id="cb33-7"><a href="xgboost.html#cb33-7"></a>    <span class="dt">rmse.test   =</span> <span class="kw">min</span>(test_rmse_mean),))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["ntrees.train"],"name":[1],"type":["int"],"align":["right"]},{"label":["rmse.train"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["ntrees.test"],"name":[3],"type":["int"],"align":["right"]},{"label":["rmse.test"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"1641","2":"8270.452","3":"1591","4":"22194.2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="xgboost.html#cb34-1"></a><span class="co"># Plot error vs number trees</span></span>
<span id="cb34-2"><a href="xgboost.html#cb34-2"></a>pr1 &lt;-<span class="kw">ggplot</span>(ames_xgb1<span class="op">$</span>evaluation_log) <span class="op">+</span></span>
<span id="cb34-3"><a href="xgboost.html#cb34-3"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, train_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb34-4"><a href="xgboost.html#cb34-4"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(iter, test_rmse_mean), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb34-5"><a href="xgboost.html#cb34-5"></a><span class="kw">ggplotly</span>(pr1)</span></code></pre></div>
<div id="htmlwidget-5f4aae55f69d7414061a" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-5f4aae55f69d7414061a">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641],"y":[191273.0484375,185933.8765625,180756.0921875,175737.2546875,170844.3,166088.346875,161518.5859375,157048.4171875,152737.0609375,148562.721875,144497.440625,140576.4328125,136761.7953125,133069.5015625,129493.8648437,126022.2117187,122639.8671875,119379.2015625,116202.6601564,113125.5226563,110167.3539063,107286.1757814,104474.5304687,101769.1281249,99138.7890625,96600.8906251,94115.0460935,91728.8437499,89424.6046876,87172.3570312,84999.6671875,82903.7109376,80853.9304687,78890.9335938,76957.809375,75101.7570313,73319.7359375,71600.8523438,69952.2921876,68336.4507813,66746.803125,65207.3195313,63737.2800782,62310.6460936,60927.4851561,59586.3546875,58307.0894532,57062.4929687,55852.7765624,54675.4761719,53572.740625,52491.9500001,51438.2679687,50435.7417968,49463.4226565,48524.4785157,47612.1804688,46734.8527343,45872.5085937,45049.4976562,44230.7769532,43455.3175783,42704.8624998,41976.58125,41276.7867187,40610.6078125,39977.7835938,39357.4671876,38761.153125,38173.3589842,37621.1484375,37076.6972656,36547.1332033,36046.00625,35554.9125001,35082.9191406,34631.9613282,34202.7960937,33761.6308592,33369.6363281,32981.6560547,32611.1220702,32241.5986329,31901.1425781,31565.5484374,31235.8171874,30925.0763672,30617.9236328,30323.6427735,30036.4955077,29774.6191408,29512.6923827,29262.7179688,29015.8236328,28761.7,28522.3066406,28294.8701171,28070.8529297,27851.8320313,27654.8003907,27465.9111327,27277.2046876,27096.9041015,26918.9457032,26744.5958985,26576.7283204,26403.3472656,26249.8636718,26093.4158202,25933.6716797,25798.4677734,25650.1224609,25515.8431641,25385.1949219,25250.6330076,25115.0818359,24982.1806641,24861.915039,24754.6792969,24642.4980468,24531.8460938,24417.1332029,24309.6511719,24204.7966798,24093.9117187,23993.9857422,23899.8728517,23796.8419922,23704.828711,23614.9507812,23519.3072265,23433.7015626,23353.7960937,23269.0666018,23183.3148439,23097.1292971,23015.8472657,22939.0371092,22868.2076172,22789.6560545,22720.5708984,22654.7691405,22588.5703124,22522.7076172,22456.9548828,22385.2117188,22317.0500002,22242.7796874,22174.0917969,22108.8271485,22048.5683595,21981.8945314,21919.3320312,21866.5556641,21814.0714843,21760.0740235,21704.0230467,21646.4666017,21593.906836,21536.0210936,21484.1273438,21432.9158203,21383.4949218,21333.0810546,21283.3486328,21228.0068359,21176.9837889,21125.1484374,21076.2896486,21024.6603515,20976.1761719,20930.3177733,20884.0132812,20834.5812501,20794.0748047,20749.3533203,20710.9208983,20667.6023439,20626.8300781,20586.2916015,20545.1945313,20506.8896483,20463.7101561,20423.7472656,20382.7000001,20342.3630858,20299.0679687,20259.003125,20216.8660158,20180.6539063,20138.3503906,20098.2169923,20066.291797,20030.9816405,19999.4927735,19967.678125,19936.0966798,19899.7687499,19866.06875,19825.2035157,19792.1789063,19759.0664061,19727.4541016,19694.1578126,19663.4345704,19630.1162109,19597.4634767,19562.3466797,19527.965039,19494.6537109,19462.9876955,19433.7111329,19406.2832031,19380.3779297,19352.2152343,19323.2191406,19295.3101563,19267.2740235,19239.0718749,19204.9375,19174.9441405,19149.0708985,19120.5433594,19091.4785156,19061.4552734,19033.3630861,19004.5826171,18972.1890624,18945.2908204,18915.8363281,18885.7335938,18857.5437501,18828.4511719,18800.3837888,18771.25625,18743.219922,18716.9755859,18691.3421877,18663.8583984,18636.0435546,18610.8373047,18585.1248049,18556.4234374,18530.909961,18506.1210936,18480.3273436,18460.3197267,18431.8279296,18408.2828126,18386.8197265,18364.2976561,18339.6947267,18313.6832031,18292.5529298,18267.777539,18243.9197265,18220.4535156,18200.1658203,18177.0544922,18150.1964844,18124.0337891,18103.5507812,18079.8748046,18057.8808594,18032.5371094,18011.8302735,17986.8703125,17964.7142579,17941.5312501,17919.2910157,17896.305664,17878.1482422,17856.2990235,17833.1199219,17810.1417968,17785.9060546,17764.4804689,17744.3189452,17723.3089843,17700.8806641,17679.274414,17661.5330079,17642.9246094,17618.7556641,17601.4228517,17579.7294921,17559.1105469,17536.9777343,17517.4921876,17500.1189453,17483.1082032,17465.3417969,17445.6427733,17424.7703125,17405.2968751,17388.330078,17365.998828,17346.1029295,17325.7537108,17305.7005859,17286.1250001,17262.9550782,17245.3771485,17226.7525391,17207.9457032,17187.5419922,17168.5343751,17149.1078127,17130.3867188,17111.9802733,17092.9162111,17072.759961,17052.6107423,17031.1980468,17011.7193359,16996.9417969,16975.322461,16954.5599607,16935.5253908,16920.1261719,16902.127539,16886.6138672,16870.2375,16846.723828,16826.3697265,16807.251953,16789.5496094,16769.4773437,16751.6189453,16730.934961,16714.3521484,16696.8257811,16678.9486329,16660.7796875,16644.5423827,16626.1402345,16610.7919922,16593.3751953,16574.3317382,16557.2140625,16541.1024414,16525.3838869,16507.7697265,16487.8483399,16472.9107421,16456.7827148,16440.8573242,16424.2257812,16409.1351562,16395.2939452,16379.5656251,16363.959375,16349.3381837,16335.65,16320.3214842,16303.2688476,16286.5243164,16268.9979492,16250.396582,16236.0696289,16223.2211914,16210.6370118,16196.9724611,16180.9021484,16163.6954102,16146.9090821,16130.3739257,16116.2223631,16104.0488281,16088.1957031,16073.4041017,16059.2402343,16043.7220704,16028.5441405,16013.7105467,16003.5365235,15990.7262695,15977.6920899,15961.4018555,15946.2060547,15932.1013671,15914.9150391,15902.8648438,15888.1525389,15873.95,15861.6841798,15848.6105469,15832.6450196,15816.7478515,15804.6751954,15789.522754,15772.8224609,15759.9494142,15744.3765625,15727.7083985,15713.8864259,15699.8203125,15687.2207031,15673.2846679,15659.5458985,15647.8077149,15633.8987303,15620.1185546,15606.7974608,15592.2111327,15578.9405273,15565.887207,15548.5611329,15535.2714843,15521.3028321,15510.0695312,15497.3239258,15484.5158201,15472.3686524,15457.1388672,15443.8787109,15431.7672852,15419.1674805,15407.0576171,15392.4144531,15378.379004,15365.8604493,15351.8521484,15339.6166992,15328.046582,15313.472461,15299.2111328,15286.0961915,15272.87041,15259.4154296,15247.5253905,15236.2936523,15225.1350586,15216.5726564,15202.6168946,15191.5330077,15177.4015625,15163.991504,15148.9411133,15137.1551758,15124.0464846,15114.295703,15101.1208008,15090.5407229,15078.1321289,15066.3478517,15051.8887696,15039.9623047,15026.9331054,15014.8970702,15004.126465,14990.3405273,14979.1464843,14966.7585938,14953.1158204,14939.586914,14928.9424804,14914.8075195,14904.2401368,14893.4883788,14879.1646483,14867.7236327,14857.7876952,14845.1459962,14833.2935547,14822.0724609,14807.9764648,14796.0132813,14784.5651367,14774.7843751,14761.9060549,14749.4519532,14738.6628907,14726.2575197,14717.2686522,14706.4986328,14694.3793946,14685.6532227,14676.1422853,14665.3949219,14652.8223632,14639.7353516,14629.4810547,14616.3894531,14606.7375977,14594.9755859,14583.8950194,14575.4579102,14563.9163087,14553.065625,14541.9241211,14531.3616212,14520.4407227,14509.1858399,14498.8831055,14488.7537109,14475.8915039,14465.9661132,14456.808789,14446.786914,14435.8063478,14424.9270508,14414.6041017,14402.6075195,14392.5742187,14381.8767578,14371.8661133,14359.5345702,14348.5486329,14338.6460938,14328.6459961,14318.215332,14308.4853517,14298.9006836,14291.0674804,14281.2105469,14270.622461,14259.634668,14247.6233398,14237.7279297,14228.365332,14217.3624024,14205.9204101,14197.6424806,14185.2121094,14176.887793,14168.0860353,14157.0083009,14146.9761719,14138.045703,14126.4767578,14117.2023439,14107.2535156,14096.0782226,14084.8851563,14075.5860353,14065.5765626,14054.011621,14045.1368164,14037.7083984,14027.6362306,14018.576465,14010.3529297,14000.8512696,13990.3156251,13979.2999023,13968.8438476,13959.4039061,13948.4785156,13939.9552735,13929.6823242,13920.1541993,13909.2102539,13901.5759767,13889.9520508,13879.8606445,13871.3128907,13860.8738281,13851.5831055,13841.1238281,13830.2613281,13822.3398436,13813.4841797,13805.9306641,13798.5786133,13787.822461,13778.5108398,13770.4270508,13761.4714842,13753.2857422,13744.0717773,13735.9770509,13727.401953,13716.8332031,13708.3648437,13699.9498048,13690.309082,13680.6999024,13671.7609375,13663.1916992,13653.4107421,13643.7963865,13634.0574218,13624.798047,13616.8571289,13609.1697267,13600.2916994,13591.8333008,13583.7169922,13574.0123048,13564.6404295,13554.7366209,13546.738965,13536.8267577,13528.4523437,13518.4833985,13510.9683594,13503.7742188,13494.8791016,13485.9110352,13477.9439453,13470.5354491,13462.5480468,13456.0685546,13449.0383788,13440.0551758,13430.0287109,13421.1280273,13412.5220704,13403.0490233,13393.1557616,13383.913086,13375.7239259,13367.9271485,13359.9927734,13350.897754,13342.73916,13335.7375,13328.9654297,13319.5104492,13311.113379,13304.0067382,13296.5408204,13289.0544921,13283.2428711,13273.5094726,13265.838672,13257.8791992,13252.4963867,13244.9581054,13236.5366211,13228.7804687,13221.4942384,13213.6642579,13202.0534181,13192.5632813,13183.787793,13177.2428713,13168.6488283,13159.2630859,13150.5844726,13142.104785,13134.0889648,13126.6257811,13119.0996094,13108.2967774,13099.3958983,13091.16875,13082.8332032,13074.625293,13067.9562499,13060.4037111,13051.2006837,13043.5125976,13036.9482422,13028.0190429,13019.4476562,13012.286621,13004.342969,12997.2591797,12991.5975586,12982.6208983,12975.8401366,12967.3001953,12957.7146485,12950.9524415,12943.5360352,12933.9343751,12927.9717775,12920.0101564,12912.136133,12904.6666992,12896.3387695,12887.6597657,12879.920703,12873.006836,12864.699121,12858.2216797,12850.5514649,12843.3847655,12833.8914063,12826.8171874,12818.9207029,12811.4073242,12804.8120118,12799.1898437,12791.8541992,12782.3768553,12775.2072265,12766.559082,12760.3365233,12752.636328,12746.2016602,12738.0006835,12731.7064454,12724.3060548,12716.6429689,12708.4344725,12700.2238282,12694.3572267,12686.3668945,12678.4621093,12671.1853516,12663.769043,12656.4323243,12649.5164062,12640.7433594,12632.1456054,12625.0062501,12617.2276367,12610.5745118,12604.3011719,12595.9916015,12588.6083984,12581.4545899,12573.2086915,12566.762793,12561.2474609,12554.8432617,12547.8399413,12540.1999999,12531.9302735,12523.8564451,12517.4641602,12510.6009766,12503.0888671,12497.1612305,12489.2887697,12482.5107423,12473.5102539,12466.108203,12459.2929689,12453.1055664,12446.0171876,12438.8180664,12433.2813475,12424.9545898,12417.9292968,12411.14209,12404.0894531,12396.7867187,12388.4385743,12382.4735351,12375.3878905,12367.9591798,12361.2461915,12353.644043,12346.1833983,12339.1890626,12331.0509766,12322.8722655,12316.6110352,12309.1063477,12300.6239258,12294.3045897,12286.5171875,12278.6508789,12271.963672,12265.328125,12257.9143555,12251.4076173,12243.6055665,12236.6208986,12230.5536132,12225.2867186,12217.7534179,12211.1719727,12205.8424804,12198.5277342,12193.3292968,12186.7627931,12180.3017577,12172.3431639,12165.0093748,12157.5313477,12150.009961,12144.6108398,12136.8942382,12130.7036132,12124.3753905,12117.7641601,12111.5117187,12104.7043946,12099.2041016,12093.816797,12087.1810546,12080.870215,12074.419043,12067.4161134,12059.7850586,12052.7119141,12048.1315429,12041.7,12035.4688476,12027.784668,12020.7597657,12014.7997069,12008.6856445,12002.2161132,11995.7390625,11988.7231445,11982.553418,11975.6019532,11969.706543,11962.544629,11956.6125977,11950.9847654,11945.2783203,11939.6328123,11934.3792968,11927.5801757,11919.6601562,11913.1350587,11905.8572265,11898.525293,11891.6173829,11884.6563478,11878.1353517,11872.5181639,11867.2348632,11860.300293,11854.4983398,11847.6467773,11841.6641601,11833.9402345,11827.960254,11821.4342772,11814.1423827,11808.0582031,11802.2645507,11796.7534179,11790.2622069,11784.7080079,11777.7664065,11771.3841796,11764.8676757,11758.05,11752.7160155,11747.0876953,11740.1954102,11736.0773437,11730.6500001,11724.8143554,11718.5375976,11712.3608398,11707.5041015,11701.6517578,11694.7666991,11688.5759766,11682.5826172,11676.7528322,11668.7360351,11661.2343749,11656.50166,11650.7458985,11645.1758788,11639.6100586,11632.5982421,11626.6259766,11620.3697266,11615.241211,11609.5195312,11602.7518555,11596.9673829,11590.6537109,11585.08125,11579.3675781,11573.5977538,11566.7946289,11559.9672852,11553.0791015,11546.9447266,11540.6353516,11535.1396485,11530.26416,11525.5600585,11519.035547,11513.2390624,11506.4286132,11501.2627929,11495.8214844,11489.6628907,11483.3170899,11477.1600588,11469.3874024,11464.0469726,11458.8391601,11453.0998046,11446.9091796,11440.8128907,11434.3861327,11427.6344726,11422.0486327,11416.2987306,11411.100293,11405.8346681,11399.2916993,11393.2899414,11388.1323243,11380.8312499,11376.4680663,11371.6288086,11365.2960939,11360.5492189,11354.2201171,11348.5349609,11344.4541017,11338.1798827,11332.6687501,11326.9966797,11322.2634765,11316.1920898,11312.1370118,11306.6985352,11299.2354492,11293.6974609,11287.075586,11281.6728515,11277.2475586,11270.9359376,11265.7552734,11259.7400389,11254.0026367,11248.5598634,11242.3162108,11235.5849608,11228.6432618,11222.9271484,11217.0243165,11212.3648438,11208.8854493,11202.7208985,11196.9552733,11192.4085939,11187.0125975,11180.0858397,11174.0143554,11169.4588867,11163.2826172,11158.1441407,11153.9835936,11148.7131837,11143.8336913,11136.8695313,11131.7997068,11127.655078,11123.0214846,11117.6423826,11111.2074219,11105.8830079,11100.6970703,11095.9606444,11089.5974608,11083.5782226,11077.0664062,11072.9149413,11067.5780273,11062.9968751,11056.9645507,11050.5748047,11045.7225585,11039.4910156,11033.7759765,11027.7584961,11022.3016601,11017.2082031,11012.4534179,11006.2955077,11001.0072265,10996.0203125,10990.8520507,10986.1397461,10981.1075195,10976.1582031,10970.5607423,10965.6222656,10960.630957,10955.5340821,10951.0802733,10945.4911133,10939.1466797,10932.5253907,10925.5994141,10918.9972656,10912.9528321,10906.749707,10900.925879,10895.1947265,10889.8012695,10884.4517579,10879.6702147,10874.5103514,10868.6737303,10862.8136719,10857.7453125,10851.9397462,10846.573828,10841.2108398,10835.5182618,10830.1363281,10823.7337891,10819.2047851,10813.8209962,10808.1564453,10802.1110352,10796.0032229,10790.2284179,10784.8186524,10779.1249999,10774.3801759,10768.6713865,10763.3091797,10757.9910157,10753.2322266,10748.4835936,10742.3358399,10737.5571289,10733.5998048,10729.1114257,10723.5205078,10718.7654296,10713.2402344,10708.234668,10702.3639648,10697.4382812,10691.7334961,10686.9855468,10682.3662111,10678.2991211,10674.0569337,10669.74375,10664.667871,10660.0124023,10655.1429687,10649.0081054,10643.545215,10638.3119141,10633.1738281,10627.8793945,10622.487207,10618.1041018,10612.9884765,10607.9279296,10602.8897461,10597.3668946,10592.2996092,10586.6479493,10580.7702147,10576.5253906,10570.6361328,10565.2023437,10559.7425781,10554.1627931,10548.2280275,10542.374121,10537.5589844,10533.0551758,10528.7277344,10524.259668,10518.9113282,10514.6754881,10509.4843751,10505.2621094,10500.5077147,10495.7844726,10490.9765625,10486.5217774,10480.1458984,10475.9597657,10471.975293,10467.8069337,10463.0179688,10457.5931642,10453.1813474,10448.1386719,10443.2193358,10438.606543,10433.1544922,10428.0100585,10422.7701171,10418.1320313,10414.2068359,10408.9249023,10403.5029297,10399.2092773,10395.008496,10390.7961913,10386.9081054,10382.4967774,10378.7470704,10372.9629883,10369.0401367,10364.7013672,10360.7516601,10356.0316406,10350.9477541,10346.2070312,10340.7419921,10336.937207,10331.6169922,10326.8943359,10323.2499023,10318.8061524,10313.3942384,10309.8447266,10304.8247072,10300.8116212,10296.1417969,10291.9816407,10286.88584,10282.0882811,10277.1088868,10273.0887697,10269.3509767,10263.9366212,10259.6824218,10255.4842773,10250.6237304,10245.9143554,10241.5264649,10237.6261719,10232.8821289,10227.6178712,10221.966211,10218.5708008,10214.3064452,10209.8895509,10204.6195313,10200.5369143,10195.5466798,10192.5063476,10188.3084963,10184.6933594,10179.1386718,10173.8219726,10169.1209959,10163.2920897,10158.6887695,10154.4503907,10151.0238279,10146.914746,10142.6577149,10138.2640625,10134.0682619,10129.2328125,10124.717871,10120.7701173,10116.4362305,10111.7110352,10107.8236327,10104.2887696,10100.5932616,10096.9688477,10091.0079102,10087.5198242,10082.745996,10078.7325197,10074.331836,10069.6338867,10064.4260743,10059.8538086,10056.2185546,10052.2132811,10047.511133,10042.5999999,10038.1784178,10034.7168946,10029.6154296,10024.9525392,10021.0005859,10016.6966799,10012.5800782,10007.753125,10002.5809572,9997.6729492,9993.2192381,9989.0591797,9984.3021484,9979.469922,9974.6929687,9970.7515626,9966.3585938,9961.495117,9956.6710937,9952.6243163,9948.6363282,9944.0358398,9939.8394532,9935.7042968,9931.9916017,9927.7328124,9923.5350586,9919.1387695,9915.5708983,9911.9227539,9907.5476562,9904.0256835,9900.090039,9895.7112305,9891.2556641,9886.6345702,9882.7385742,9878.3875976,9874.8143554,9869.3733399,9865.0372068,9860.3339844,9856.3107422,9852.2984375,9848.3700195,9843.5174805,9838.9121095,9834.1353516,9828.8893557,9824.6041993,9819.1498046,9813.6654297,9808.653711,9803.6671875,9799.9693359,9795.5821288,9791.9808592,9787.9803711,9784.09375,9779.3061524,9774.5801757,9770.6866213,9766.3853517,9760.7795898,9756.2064454,9753.373633,9749.5056639,9746.2663086,9742.0804689,9738.1283204,9733.7545897,9729.5763673,9725.3388671,9720.2896486,9715.7969726,9712.3234377,9707.2520507,9703.1846681,9699.2765625,9694.525,9690.5545898,9686.9860351,9682.5646484,9678.4070312,9674.4249023,9671.0023438,9666.2885741,9662.3831055,9657.8333984,9654.4992188,9650.5574219,9647.2318361,9643.4223633,9639.592383,9635.8815429,9631.6891603,9627.1593751,9622.3273437,9618.2489258,9614.3976562,9610.6211914,9606.2142577,9601.6732423,9597.074707,9592.8965821,9588.6847657,9585.302539,9581.2200196,9578.0884765,9572.9178711,9568.4525391,9563.9163086,9559.7106447,9555.9235351,9551.6052735,9548.0566405,9543.6219726,9540.0982422,9536.4252929,9532.8852539,9528.6717774,9524.7595703,9520.8387696,9516.7094726,9512.6869141,9508.1612305,9504.7683594,9500.4402342,9495.9058595,9492.2084961,9488.8384765,9484.8633789,9481.233203,9477.4308594,9473.5901367,9468.9374023,9464.5551756,9460.6810548,9455.9086915,9452.1219726,9448.2274413,9443.6925782,9439.8691407,9436.0065429,9432.254199,9428.0531248,9423.9828125,9420.3354493,9416.5090821,9413.2719725,9408.9917969,9404.5006838,9401.3632813,9397.3964845,9393.3391602,9389.3166015,9385.6562501,9382.172168,9377.5980469,9373.1319336,9368.4714843,9364.4201172,9359.7154298,9355.1958985,9350.5950196,9346.351172,9342.2923829,9338.4483398,9334.3578125,9331.4634766,9327.2550782,9323.4313476,9320.1499022,9316.5241211,9312.5967773,9308.6309571,9303.7669923,9299.6922852,9296.0230469,9291.8491211,9288.3513672,9283.8030273,9279.6753906,9275.1083983,9271.2620118,9266.1719727,9262.9418945,9259.377246,9256.1013671,9252.222168,9248.0228516,9244.0064453,9239.3560546,9234.9124025,9229.7899415,9225.6792969,9221.5456055,9217.7009767,9214.6220704,9211.2693359,9207.7211915,9203.8087891,9200.7685546,9197.2282227,9193.8114259,9190.2532227,9186.9646484,9183.0737303,9179.1238281,9174.8737305,9170.41875,9166.7317381,9162.534082,9158.5374999,9154.9034178,9151.5545901,9147.9790039,9144.0165039,9140.2765625,9136.3150392,9133.3271483,9129.6112306,9126.1232423,9121.7398438,9118.3032227,9114.7731445,9110.9005859,9105.9426759,9102.8570311,9098.5608399,9094.4460937,9090.8979492,9087.6174804,9084.5245118,9080.9725586,9077.1966798,9073.7653319,9070.2342774,9066.8019531,9063.5402344,9060.6638671,9057.4322265,9054.0867187,9050.9107421,9047.5954101,9043.8339844,9040.1479493,9035.8343749,9032.5445314,9029.1651367,9025.4888672,9022.1113281,9017.9394534,9014.763672,9010.297949,9005.6344727,9001.6172851,8998.155957,8994.8296874,8991.2102539,8987.5373047,8983.9741211,8980.6247072,8976.9285155,8973.2110352,8969.1153322,8965.4311524,8961.405957,8958.0104493,8954.4775391,8951.0079101,8948.2253906,8944.1493166,8940.1321288,8937.1769532,8934.1606446,8930.6120116,8927.303711,8924.559375,8921.0881837,8916.8111329,8912.9831055,8908.6122071,8904.6433593,8901.115332,8897.6604492,8894.5361329,8890.909375,8888.2708008,8884.5529296,8880.6094727,8877.0433593,8873.7541992,8869.6765625,8866.5992186,8863.5513672,8859.1900392,8855.8933592,8853.0724609,8849.3067382,8846.1560548,8843.0574219,8840.0697265,8836.6837892,8832.8335938,8828.9762694,8825.4662109,8821.9264649,8818.0683594,8814.6666993,8811.6762695,8808.0826172,8804.7350585,8801.2036133,8798.3880859,8795.2895507,8791.2810548,8788.0657226,8784.390332,8780.2191407,8776.4226562,8772.6880859,8769.0168946,8765.885547,8762.5181641,8759.5027344,8755.6433592,8752.447168,8749.0296876,8746.3249022,8742.6397461,8739.605371,8735.8365234,8731.7487307,8728.4410156,8725.5746093,8722.7520507,8719.4819337,8716.2262696,8712.6333983,8709.0460937,8706.8203125,8703.4074219,8699.6072266,8696.0004883,8692.9020508,8689.533789,8686.0097657,8683.0055664,8679.1355467,8675.3479493,8671.9102539,8669.0017579,8665.3959961,8661.8708007,8658.2657228,8654.6980469,8651.6375977,8647.6091796,8644.7395506,8641.5644531,8638.2059571,8634.6629884,8631.3784181,8628.6782227,8624.8655273,8621.3570314,8618.2654298,8613.9237305,8610.9240234,8607.7831056,8604.827832,8601.5244141,8598.1441407,8594.6427733,8591.3144531,8588.834668,8586.114453,8582.88125,8579.6512696,8576.0960937,8573.3043946,8569.3290039,8565.499707,8562.2493164,8559.1452149,8556.4190429,8553.1923828,8549.6049803,8545.9080077,8542.3890625,8538.5795898,8535.263086,8531.9936523,8529.3740235,8526.4134765,8523.4508787,8520.1082031,8516.8189452,8512.8239258,8510.1987303,8507.2279297,8504.3068359,8501.2423829,8498.8714844,8496.6041017,8493.3000976,8489.9455077,8486.1185546,8482.896582,8479.8479493,8476.5015625,8473.7133789,8470.8802734,8467.9375976,8464.4943359,8461.0661133,8458.0816405,8454.5962891,8451.8534179,8448.4605467,8444.6095703,8441.2961913,8437.5876953,8434.2835938,8431.1911133,8428.0887695,8424.6432617,8420.8870117,8417.659961,8414.7137696,8410.5921874,8407.130957,8404.3990235,8401.1901365,8398.6205078,8395.5261718,8392.4016602,8388.7774414,8385.4957032,8382.2943846,8378.3756835,8375.7971679,8372.7826171,8369.6576662,8367.2898439,8364.2095704,8360.9573241,8357.6848144,8354.4581056,8351.3082518,8347.8439451,8344.6586426,8341.8040039,8339.0198243,8335.8728513,8332.6301758,8329.1758789,8326.9011231,8323.8728516,8321.3982421,8317.7615234,8314.2266601,8311.0767577,8308.1091309,8304.1199708,8301.7109864,8298.3552734,8295.2639158,8291.1239746,8287.475586,8284.6132811,8281.452246,8277.4551757,8274.1467285,8270.4522951],"text":["iter:    1<br />train_rmse_mean: 191273.048","iter:    2<br />train_rmse_mean: 185933.877","iter:    3<br />train_rmse_mean: 180756.092","iter:    4<br />train_rmse_mean: 175737.255","iter:    5<br />train_rmse_mean: 170844.300","iter:    6<br />train_rmse_mean: 166088.347","iter:    7<br />train_rmse_mean: 161518.586","iter:    8<br />train_rmse_mean: 157048.417","iter:    9<br />train_rmse_mean: 152737.061","iter:   10<br />train_rmse_mean: 148562.722","iter:   11<br />train_rmse_mean: 144497.441","iter:   12<br />train_rmse_mean: 140576.433","iter:   13<br />train_rmse_mean: 136761.795","iter:   14<br />train_rmse_mean: 133069.502","iter:   15<br />train_rmse_mean: 129493.865","iter:   16<br />train_rmse_mean: 126022.212","iter:   17<br />train_rmse_mean: 122639.867","iter:   18<br />train_rmse_mean: 119379.202","iter:   19<br />train_rmse_mean: 116202.660","iter:   20<br />train_rmse_mean: 113125.523","iter:   21<br />train_rmse_mean: 110167.354","iter:   22<br />train_rmse_mean: 107286.176","iter:   23<br />train_rmse_mean: 104474.530","iter:   24<br />train_rmse_mean: 101769.128","iter:   25<br />train_rmse_mean:  99138.789","iter:   26<br />train_rmse_mean:  96600.891","iter:   27<br />train_rmse_mean:  94115.046","iter:   28<br />train_rmse_mean:  91728.844","iter:   29<br />train_rmse_mean:  89424.605","iter:   30<br />train_rmse_mean:  87172.357","iter:   31<br />train_rmse_mean:  84999.667","iter:   32<br />train_rmse_mean:  82903.711","iter:   33<br />train_rmse_mean:  80853.930","iter:   34<br />train_rmse_mean:  78890.934","iter:   35<br />train_rmse_mean:  76957.809","iter:   36<br />train_rmse_mean:  75101.757","iter:   37<br />train_rmse_mean:  73319.736","iter:   38<br />train_rmse_mean:  71600.852","iter:   39<br />train_rmse_mean:  69952.292","iter:   40<br />train_rmse_mean:  68336.451","iter:   41<br />train_rmse_mean:  66746.803","iter:   42<br />train_rmse_mean:  65207.320","iter:   43<br />train_rmse_mean:  63737.280","iter:   44<br />train_rmse_mean:  62310.646","iter:   45<br />train_rmse_mean:  60927.485","iter:   46<br />train_rmse_mean:  59586.355","iter:   47<br />train_rmse_mean:  58307.089","iter:   48<br />train_rmse_mean:  57062.493","iter:   49<br />train_rmse_mean:  55852.777","iter:   50<br />train_rmse_mean:  54675.476","iter:   51<br />train_rmse_mean:  53572.741","iter:   52<br />train_rmse_mean:  52491.950","iter:   53<br />train_rmse_mean:  51438.268","iter:   54<br />train_rmse_mean:  50435.742","iter:   55<br />train_rmse_mean:  49463.423","iter:   56<br />train_rmse_mean:  48524.479","iter:   57<br />train_rmse_mean:  47612.180","iter:   58<br />train_rmse_mean:  46734.853","iter:   59<br />train_rmse_mean:  45872.509","iter:   60<br />train_rmse_mean:  45049.498","iter:   61<br />train_rmse_mean:  44230.777","iter:   62<br />train_rmse_mean:  43455.318","iter:   63<br />train_rmse_mean:  42704.862","iter:   64<br />train_rmse_mean:  41976.581","iter:   65<br />train_rmse_mean:  41276.787","iter:   66<br />train_rmse_mean:  40610.608","iter:   67<br />train_rmse_mean:  39977.784","iter:   68<br />train_rmse_mean:  39357.467","iter:   69<br />train_rmse_mean:  38761.153","iter:   70<br />train_rmse_mean:  38173.359","iter:   71<br />train_rmse_mean:  37621.148","iter:   72<br />train_rmse_mean:  37076.697","iter:   73<br />train_rmse_mean:  36547.133","iter:   74<br />train_rmse_mean:  36046.006","iter:   75<br />train_rmse_mean:  35554.913","iter:   76<br />train_rmse_mean:  35082.919","iter:   77<br />train_rmse_mean:  34631.961","iter:   78<br />train_rmse_mean:  34202.796","iter:   79<br />train_rmse_mean:  33761.631","iter:   80<br />train_rmse_mean:  33369.636","iter:   81<br />train_rmse_mean:  32981.656","iter:   82<br />train_rmse_mean:  32611.122","iter:   83<br />train_rmse_mean:  32241.599","iter:   84<br />train_rmse_mean:  31901.143","iter:   85<br />train_rmse_mean:  31565.548","iter:   86<br />train_rmse_mean:  31235.817","iter:   87<br />train_rmse_mean:  30925.076","iter:   88<br />train_rmse_mean:  30617.924","iter:   89<br />train_rmse_mean:  30323.643","iter:   90<br />train_rmse_mean:  30036.496","iter:   91<br />train_rmse_mean:  29774.619","iter:   92<br />train_rmse_mean:  29512.692","iter:   93<br />train_rmse_mean:  29262.718","iter:   94<br />train_rmse_mean:  29015.824","iter:   95<br />train_rmse_mean:  28761.700","iter:   96<br />train_rmse_mean:  28522.307","iter:   97<br />train_rmse_mean:  28294.870","iter:   98<br />train_rmse_mean:  28070.853","iter:   99<br />train_rmse_mean:  27851.832","iter:  100<br />train_rmse_mean:  27654.800","iter:  101<br />train_rmse_mean:  27465.911","iter:  102<br />train_rmse_mean:  27277.205","iter:  103<br />train_rmse_mean:  27096.904","iter:  104<br />train_rmse_mean:  26918.946","iter:  105<br />train_rmse_mean:  26744.596","iter:  106<br />train_rmse_mean:  26576.728","iter:  107<br />train_rmse_mean:  26403.347","iter:  108<br />train_rmse_mean:  26249.864","iter:  109<br />train_rmse_mean:  26093.416","iter:  110<br />train_rmse_mean:  25933.672","iter:  111<br />train_rmse_mean:  25798.468","iter:  112<br />train_rmse_mean:  25650.122","iter:  113<br />train_rmse_mean:  25515.843","iter:  114<br />train_rmse_mean:  25385.195","iter:  115<br />train_rmse_mean:  25250.633","iter:  116<br />train_rmse_mean:  25115.082","iter:  117<br />train_rmse_mean:  24982.181","iter:  118<br />train_rmse_mean:  24861.915","iter:  119<br />train_rmse_mean:  24754.679","iter:  120<br />train_rmse_mean:  24642.498","iter:  121<br />train_rmse_mean:  24531.846","iter:  122<br />train_rmse_mean:  24417.133","iter:  123<br />train_rmse_mean:  24309.651","iter:  124<br />train_rmse_mean:  24204.797","iter:  125<br />train_rmse_mean:  24093.912","iter:  126<br />train_rmse_mean:  23993.986","iter:  127<br />train_rmse_mean:  23899.873","iter:  128<br />train_rmse_mean:  23796.842","iter:  129<br />train_rmse_mean:  23704.829","iter:  130<br />train_rmse_mean:  23614.951","iter:  131<br />train_rmse_mean:  23519.307","iter:  132<br />train_rmse_mean:  23433.702","iter:  133<br />train_rmse_mean:  23353.796","iter:  134<br />train_rmse_mean:  23269.067","iter:  135<br />train_rmse_mean:  23183.315","iter:  136<br />train_rmse_mean:  23097.129","iter:  137<br />train_rmse_mean:  23015.847","iter:  138<br />train_rmse_mean:  22939.037","iter:  139<br />train_rmse_mean:  22868.208","iter:  140<br />train_rmse_mean:  22789.656","iter:  141<br />train_rmse_mean:  22720.571","iter:  142<br />train_rmse_mean:  22654.769","iter:  143<br />train_rmse_mean:  22588.570","iter:  144<br />train_rmse_mean:  22522.708","iter:  145<br />train_rmse_mean:  22456.955","iter:  146<br />train_rmse_mean:  22385.212","iter:  147<br />train_rmse_mean:  22317.050","iter:  148<br />train_rmse_mean:  22242.780","iter:  149<br />train_rmse_mean:  22174.092","iter:  150<br />train_rmse_mean:  22108.827","iter:  151<br />train_rmse_mean:  22048.568","iter:  152<br />train_rmse_mean:  21981.895","iter:  153<br />train_rmse_mean:  21919.332","iter:  154<br />train_rmse_mean:  21866.556","iter:  155<br />train_rmse_mean:  21814.071","iter:  156<br />train_rmse_mean:  21760.074","iter:  157<br />train_rmse_mean:  21704.023","iter:  158<br />train_rmse_mean:  21646.467","iter:  159<br />train_rmse_mean:  21593.907","iter:  160<br />train_rmse_mean:  21536.021","iter:  161<br />train_rmse_mean:  21484.127","iter:  162<br />train_rmse_mean:  21432.916","iter:  163<br />train_rmse_mean:  21383.495","iter:  164<br />train_rmse_mean:  21333.081","iter:  165<br />train_rmse_mean:  21283.349","iter:  166<br />train_rmse_mean:  21228.007","iter:  167<br />train_rmse_mean:  21176.984","iter:  168<br />train_rmse_mean:  21125.148","iter:  169<br />train_rmse_mean:  21076.290","iter:  170<br />train_rmse_mean:  21024.660","iter:  171<br />train_rmse_mean:  20976.176","iter:  172<br />train_rmse_mean:  20930.318","iter:  173<br />train_rmse_mean:  20884.013","iter:  174<br />train_rmse_mean:  20834.581","iter:  175<br />train_rmse_mean:  20794.075","iter:  176<br />train_rmse_mean:  20749.353","iter:  177<br />train_rmse_mean:  20710.921","iter:  178<br />train_rmse_mean:  20667.602","iter:  179<br />train_rmse_mean:  20626.830","iter:  180<br />train_rmse_mean:  20586.292","iter:  181<br />train_rmse_mean:  20545.195","iter:  182<br />train_rmse_mean:  20506.890","iter:  183<br />train_rmse_mean:  20463.710","iter:  184<br />train_rmse_mean:  20423.747","iter:  185<br />train_rmse_mean:  20382.700","iter:  186<br />train_rmse_mean:  20342.363","iter:  187<br />train_rmse_mean:  20299.068","iter:  188<br />train_rmse_mean:  20259.003","iter:  189<br />train_rmse_mean:  20216.866","iter:  190<br />train_rmse_mean:  20180.654","iter:  191<br />train_rmse_mean:  20138.350","iter:  192<br />train_rmse_mean:  20098.217","iter:  193<br />train_rmse_mean:  20066.292","iter:  194<br />train_rmse_mean:  20030.982","iter:  195<br />train_rmse_mean:  19999.493","iter:  196<br />train_rmse_mean:  19967.678","iter:  197<br />train_rmse_mean:  19936.097","iter:  198<br />train_rmse_mean:  19899.769","iter:  199<br />train_rmse_mean:  19866.069","iter:  200<br />train_rmse_mean:  19825.204","iter:  201<br />train_rmse_mean:  19792.179","iter:  202<br />train_rmse_mean:  19759.066","iter:  203<br />train_rmse_mean:  19727.454","iter:  204<br />train_rmse_mean:  19694.158","iter:  205<br />train_rmse_mean:  19663.435","iter:  206<br />train_rmse_mean:  19630.116","iter:  207<br />train_rmse_mean:  19597.463","iter:  208<br />train_rmse_mean:  19562.347","iter:  209<br />train_rmse_mean:  19527.965","iter:  210<br />train_rmse_mean:  19494.654","iter:  211<br />train_rmse_mean:  19462.988","iter:  212<br />train_rmse_mean:  19433.711","iter:  213<br />train_rmse_mean:  19406.283","iter:  214<br />train_rmse_mean:  19380.378","iter:  215<br />train_rmse_mean:  19352.215","iter:  216<br />train_rmse_mean:  19323.219","iter:  217<br />train_rmse_mean:  19295.310","iter:  218<br />train_rmse_mean:  19267.274","iter:  219<br />train_rmse_mean:  19239.072","iter:  220<br />train_rmse_mean:  19204.938","iter:  221<br />train_rmse_mean:  19174.944","iter:  222<br />train_rmse_mean:  19149.071","iter:  223<br />train_rmse_mean:  19120.543","iter:  224<br />train_rmse_mean:  19091.479","iter:  225<br />train_rmse_mean:  19061.455","iter:  226<br />train_rmse_mean:  19033.363","iter:  227<br />train_rmse_mean:  19004.583","iter:  228<br />train_rmse_mean:  18972.189","iter:  229<br />train_rmse_mean:  18945.291","iter:  230<br />train_rmse_mean:  18915.836","iter:  231<br />train_rmse_mean:  18885.734","iter:  232<br />train_rmse_mean:  18857.544","iter:  233<br />train_rmse_mean:  18828.451","iter:  234<br />train_rmse_mean:  18800.384","iter:  235<br />train_rmse_mean:  18771.256","iter:  236<br />train_rmse_mean:  18743.220","iter:  237<br />train_rmse_mean:  18716.976","iter:  238<br />train_rmse_mean:  18691.342","iter:  239<br />train_rmse_mean:  18663.858","iter:  240<br />train_rmse_mean:  18636.044","iter:  241<br />train_rmse_mean:  18610.837","iter:  242<br />train_rmse_mean:  18585.125","iter:  243<br />train_rmse_mean:  18556.423","iter:  244<br />train_rmse_mean:  18530.910","iter:  245<br />train_rmse_mean:  18506.121","iter:  246<br />train_rmse_mean:  18480.327","iter:  247<br />train_rmse_mean:  18460.320","iter:  248<br />train_rmse_mean:  18431.828","iter:  249<br />train_rmse_mean:  18408.283","iter:  250<br />train_rmse_mean:  18386.820","iter:  251<br />train_rmse_mean:  18364.298","iter:  252<br />train_rmse_mean:  18339.695","iter:  253<br />train_rmse_mean:  18313.683","iter:  254<br />train_rmse_mean:  18292.553","iter:  255<br />train_rmse_mean:  18267.778","iter:  256<br />train_rmse_mean:  18243.920","iter:  257<br />train_rmse_mean:  18220.454","iter:  258<br />train_rmse_mean:  18200.166","iter:  259<br />train_rmse_mean:  18177.054","iter:  260<br />train_rmse_mean:  18150.196","iter:  261<br />train_rmse_mean:  18124.034","iter:  262<br />train_rmse_mean:  18103.551","iter:  263<br />train_rmse_mean:  18079.875","iter:  264<br />train_rmse_mean:  18057.881","iter:  265<br />train_rmse_mean:  18032.537","iter:  266<br />train_rmse_mean:  18011.830","iter:  267<br />train_rmse_mean:  17986.870","iter:  268<br />train_rmse_mean:  17964.714","iter:  269<br />train_rmse_mean:  17941.531","iter:  270<br />train_rmse_mean:  17919.291","iter:  271<br />train_rmse_mean:  17896.306","iter:  272<br />train_rmse_mean:  17878.148","iter:  273<br />train_rmse_mean:  17856.299","iter:  274<br />train_rmse_mean:  17833.120","iter:  275<br />train_rmse_mean:  17810.142","iter:  276<br />train_rmse_mean:  17785.906","iter:  277<br />train_rmse_mean:  17764.480","iter:  278<br />train_rmse_mean:  17744.319","iter:  279<br />train_rmse_mean:  17723.309","iter:  280<br />train_rmse_mean:  17700.881","iter:  281<br />train_rmse_mean:  17679.274","iter:  282<br />train_rmse_mean:  17661.533","iter:  283<br />train_rmse_mean:  17642.925","iter:  284<br />train_rmse_mean:  17618.756","iter:  285<br />train_rmse_mean:  17601.423","iter:  286<br />train_rmse_mean:  17579.729","iter:  287<br />train_rmse_mean:  17559.111","iter:  288<br />train_rmse_mean:  17536.978","iter:  289<br />train_rmse_mean:  17517.492","iter:  290<br />train_rmse_mean:  17500.119","iter:  291<br />train_rmse_mean:  17483.108","iter:  292<br />train_rmse_mean:  17465.342","iter:  293<br />train_rmse_mean:  17445.643","iter:  294<br />train_rmse_mean:  17424.770","iter:  295<br />train_rmse_mean:  17405.297","iter:  296<br />train_rmse_mean:  17388.330","iter:  297<br />train_rmse_mean:  17365.999","iter:  298<br />train_rmse_mean:  17346.103","iter:  299<br />train_rmse_mean:  17325.754","iter:  300<br />train_rmse_mean:  17305.701","iter:  301<br />train_rmse_mean:  17286.125","iter:  302<br />train_rmse_mean:  17262.955","iter:  303<br />train_rmse_mean:  17245.377","iter:  304<br />train_rmse_mean:  17226.753","iter:  305<br />train_rmse_mean:  17207.946","iter:  306<br />train_rmse_mean:  17187.542","iter:  307<br />train_rmse_mean:  17168.534","iter:  308<br />train_rmse_mean:  17149.108","iter:  309<br />train_rmse_mean:  17130.387","iter:  310<br />train_rmse_mean:  17111.980","iter:  311<br />train_rmse_mean:  17092.916","iter:  312<br />train_rmse_mean:  17072.760","iter:  313<br />train_rmse_mean:  17052.611","iter:  314<br />train_rmse_mean:  17031.198","iter:  315<br />train_rmse_mean:  17011.719","iter:  316<br />train_rmse_mean:  16996.942","iter:  317<br />train_rmse_mean:  16975.322","iter:  318<br />train_rmse_mean:  16954.560","iter:  319<br />train_rmse_mean:  16935.525","iter:  320<br />train_rmse_mean:  16920.126","iter:  321<br />train_rmse_mean:  16902.128","iter:  322<br />train_rmse_mean:  16886.614","iter:  323<br />train_rmse_mean:  16870.237","iter:  324<br />train_rmse_mean:  16846.724","iter:  325<br />train_rmse_mean:  16826.370","iter:  326<br />train_rmse_mean:  16807.252","iter:  327<br />train_rmse_mean:  16789.550","iter:  328<br />train_rmse_mean:  16769.477","iter:  329<br />train_rmse_mean:  16751.619","iter:  330<br />train_rmse_mean:  16730.935","iter:  331<br />train_rmse_mean:  16714.352","iter:  332<br />train_rmse_mean:  16696.826","iter:  333<br />train_rmse_mean:  16678.949","iter:  334<br />train_rmse_mean:  16660.780","iter:  335<br />train_rmse_mean:  16644.542","iter:  336<br />train_rmse_mean:  16626.140","iter:  337<br />train_rmse_mean:  16610.792","iter:  338<br />train_rmse_mean:  16593.375","iter:  339<br />train_rmse_mean:  16574.332","iter:  340<br />train_rmse_mean:  16557.214","iter:  341<br />train_rmse_mean:  16541.102","iter:  342<br />train_rmse_mean:  16525.384","iter:  343<br />train_rmse_mean:  16507.770","iter:  344<br />train_rmse_mean:  16487.848","iter:  345<br />train_rmse_mean:  16472.911","iter:  346<br />train_rmse_mean:  16456.783","iter:  347<br />train_rmse_mean:  16440.857","iter:  348<br />train_rmse_mean:  16424.226","iter:  349<br />train_rmse_mean:  16409.135","iter:  350<br />train_rmse_mean:  16395.294","iter:  351<br />train_rmse_mean:  16379.566","iter:  352<br />train_rmse_mean:  16363.959","iter:  353<br />train_rmse_mean:  16349.338","iter:  354<br />train_rmse_mean:  16335.650","iter:  355<br />train_rmse_mean:  16320.321","iter:  356<br />train_rmse_mean:  16303.269","iter:  357<br />train_rmse_mean:  16286.524","iter:  358<br />train_rmse_mean:  16268.998","iter:  359<br />train_rmse_mean:  16250.397","iter:  360<br />train_rmse_mean:  16236.070","iter:  361<br />train_rmse_mean:  16223.221","iter:  362<br />train_rmse_mean:  16210.637","iter:  363<br />train_rmse_mean:  16196.972","iter:  364<br />train_rmse_mean:  16180.902","iter:  365<br />train_rmse_mean:  16163.695","iter:  366<br />train_rmse_mean:  16146.909","iter:  367<br />train_rmse_mean:  16130.374","iter:  368<br />train_rmse_mean:  16116.222","iter:  369<br />train_rmse_mean:  16104.049","iter:  370<br />train_rmse_mean:  16088.196","iter:  371<br />train_rmse_mean:  16073.404","iter:  372<br />train_rmse_mean:  16059.240","iter:  373<br />train_rmse_mean:  16043.722","iter:  374<br />train_rmse_mean:  16028.544","iter:  375<br />train_rmse_mean:  16013.711","iter:  376<br />train_rmse_mean:  16003.537","iter:  377<br />train_rmse_mean:  15990.726","iter:  378<br />train_rmse_mean:  15977.692","iter:  379<br />train_rmse_mean:  15961.402","iter:  380<br />train_rmse_mean:  15946.206","iter:  381<br />train_rmse_mean:  15932.101","iter:  382<br />train_rmse_mean:  15914.915","iter:  383<br />train_rmse_mean:  15902.865","iter:  384<br />train_rmse_mean:  15888.153","iter:  385<br />train_rmse_mean:  15873.950","iter:  386<br />train_rmse_mean:  15861.684","iter:  387<br />train_rmse_mean:  15848.611","iter:  388<br />train_rmse_mean:  15832.645","iter:  389<br />train_rmse_mean:  15816.748","iter:  390<br />train_rmse_mean:  15804.675","iter:  391<br />train_rmse_mean:  15789.523","iter:  392<br />train_rmse_mean:  15772.822","iter:  393<br />train_rmse_mean:  15759.949","iter:  394<br />train_rmse_mean:  15744.377","iter:  395<br />train_rmse_mean:  15727.708","iter:  396<br />train_rmse_mean:  15713.886","iter:  397<br />train_rmse_mean:  15699.820","iter:  398<br />train_rmse_mean:  15687.221","iter:  399<br />train_rmse_mean:  15673.285","iter:  400<br />train_rmse_mean:  15659.546","iter:  401<br />train_rmse_mean:  15647.808","iter:  402<br />train_rmse_mean:  15633.899","iter:  403<br />train_rmse_mean:  15620.119","iter:  404<br />train_rmse_mean:  15606.797","iter:  405<br />train_rmse_mean:  15592.211","iter:  406<br />train_rmse_mean:  15578.941","iter:  407<br />train_rmse_mean:  15565.887","iter:  408<br />train_rmse_mean:  15548.561","iter:  409<br />train_rmse_mean:  15535.271","iter:  410<br />train_rmse_mean:  15521.303","iter:  411<br />train_rmse_mean:  15510.070","iter:  412<br />train_rmse_mean:  15497.324","iter:  413<br />train_rmse_mean:  15484.516","iter:  414<br />train_rmse_mean:  15472.369","iter:  415<br />train_rmse_mean:  15457.139","iter:  416<br />train_rmse_mean:  15443.879","iter:  417<br />train_rmse_mean:  15431.767","iter:  418<br />train_rmse_mean:  15419.167","iter:  419<br />train_rmse_mean:  15407.058","iter:  420<br />train_rmse_mean:  15392.414","iter:  421<br />train_rmse_mean:  15378.379","iter:  422<br />train_rmse_mean:  15365.860","iter:  423<br />train_rmse_mean:  15351.852","iter:  424<br />train_rmse_mean:  15339.617","iter:  425<br />train_rmse_mean:  15328.047","iter:  426<br />train_rmse_mean:  15313.472","iter:  427<br />train_rmse_mean:  15299.211","iter:  428<br />train_rmse_mean:  15286.096","iter:  429<br />train_rmse_mean:  15272.870","iter:  430<br />train_rmse_mean:  15259.415","iter:  431<br />train_rmse_mean:  15247.525","iter:  432<br />train_rmse_mean:  15236.294","iter:  433<br />train_rmse_mean:  15225.135","iter:  434<br />train_rmse_mean:  15216.573","iter:  435<br />train_rmse_mean:  15202.617","iter:  436<br />train_rmse_mean:  15191.533","iter:  437<br />train_rmse_mean:  15177.402","iter:  438<br />train_rmse_mean:  15163.992","iter:  439<br />train_rmse_mean:  15148.941","iter:  440<br />train_rmse_mean:  15137.155","iter:  441<br />train_rmse_mean:  15124.046","iter:  442<br />train_rmse_mean:  15114.296","iter:  443<br />train_rmse_mean:  15101.121","iter:  444<br />train_rmse_mean:  15090.541","iter:  445<br />train_rmse_mean:  15078.132","iter:  446<br />train_rmse_mean:  15066.348","iter:  447<br />train_rmse_mean:  15051.889","iter:  448<br />train_rmse_mean:  15039.962","iter:  449<br />train_rmse_mean:  15026.933","iter:  450<br />train_rmse_mean:  15014.897","iter:  451<br />train_rmse_mean:  15004.126","iter:  452<br />train_rmse_mean:  14990.341","iter:  453<br />train_rmse_mean:  14979.146","iter:  454<br />train_rmse_mean:  14966.759","iter:  455<br />train_rmse_mean:  14953.116","iter:  456<br />train_rmse_mean:  14939.587","iter:  457<br />train_rmse_mean:  14928.942","iter:  458<br />train_rmse_mean:  14914.808","iter:  459<br />train_rmse_mean:  14904.240","iter:  460<br />train_rmse_mean:  14893.488","iter:  461<br />train_rmse_mean:  14879.165","iter:  462<br />train_rmse_mean:  14867.724","iter:  463<br />train_rmse_mean:  14857.788","iter:  464<br />train_rmse_mean:  14845.146","iter:  465<br />train_rmse_mean:  14833.294","iter:  466<br />train_rmse_mean:  14822.072","iter:  467<br />train_rmse_mean:  14807.976","iter:  468<br />train_rmse_mean:  14796.013","iter:  469<br />train_rmse_mean:  14784.565","iter:  470<br />train_rmse_mean:  14774.784","iter:  471<br />train_rmse_mean:  14761.906","iter:  472<br />train_rmse_mean:  14749.452","iter:  473<br />train_rmse_mean:  14738.663","iter:  474<br />train_rmse_mean:  14726.258","iter:  475<br />train_rmse_mean:  14717.269","iter:  476<br />train_rmse_mean:  14706.499","iter:  477<br />train_rmse_mean:  14694.379","iter:  478<br />train_rmse_mean:  14685.653","iter:  479<br />train_rmse_mean:  14676.142","iter:  480<br />train_rmse_mean:  14665.395","iter:  481<br />train_rmse_mean:  14652.822","iter:  482<br />train_rmse_mean:  14639.735","iter:  483<br />train_rmse_mean:  14629.481","iter:  484<br />train_rmse_mean:  14616.389","iter:  485<br />train_rmse_mean:  14606.738","iter:  486<br />train_rmse_mean:  14594.976","iter:  487<br />train_rmse_mean:  14583.895","iter:  488<br />train_rmse_mean:  14575.458","iter:  489<br />train_rmse_mean:  14563.916","iter:  490<br />train_rmse_mean:  14553.066","iter:  491<br />train_rmse_mean:  14541.924","iter:  492<br />train_rmse_mean:  14531.362","iter:  493<br />train_rmse_mean:  14520.441","iter:  494<br />train_rmse_mean:  14509.186","iter:  495<br />train_rmse_mean:  14498.883","iter:  496<br />train_rmse_mean:  14488.754","iter:  497<br />train_rmse_mean:  14475.892","iter:  498<br />train_rmse_mean:  14465.966","iter:  499<br />train_rmse_mean:  14456.809","iter:  500<br />train_rmse_mean:  14446.787","iter:  501<br />train_rmse_mean:  14435.806","iter:  502<br />train_rmse_mean:  14424.927","iter:  503<br />train_rmse_mean:  14414.604","iter:  504<br />train_rmse_mean:  14402.608","iter:  505<br />train_rmse_mean:  14392.574","iter:  506<br />train_rmse_mean:  14381.877","iter:  507<br />train_rmse_mean:  14371.866","iter:  508<br />train_rmse_mean:  14359.535","iter:  509<br />train_rmse_mean:  14348.549","iter:  510<br />train_rmse_mean:  14338.646","iter:  511<br />train_rmse_mean:  14328.646","iter:  512<br />train_rmse_mean:  14318.215","iter:  513<br />train_rmse_mean:  14308.485","iter:  514<br />train_rmse_mean:  14298.901","iter:  515<br />train_rmse_mean:  14291.067","iter:  516<br />train_rmse_mean:  14281.211","iter:  517<br />train_rmse_mean:  14270.622","iter:  518<br />train_rmse_mean:  14259.635","iter:  519<br />train_rmse_mean:  14247.623","iter:  520<br />train_rmse_mean:  14237.728","iter:  521<br />train_rmse_mean:  14228.365","iter:  522<br />train_rmse_mean:  14217.362","iter:  523<br />train_rmse_mean:  14205.920","iter:  524<br />train_rmse_mean:  14197.642","iter:  525<br />train_rmse_mean:  14185.212","iter:  526<br />train_rmse_mean:  14176.888","iter:  527<br />train_rmse_mean:  14168.086","iter:  528<br />train_rmse_mean:  14157.008","iter:  529<br />train_rmse_mean:  14146.976","iter:  530<br />train_rmse_mean:  14138.046","iter:  531<br />train_rmse_mean:  14126.477","iter:  532<br />train_rmse_mean:  14117.202","iter:  533<br />train_rmse_mean:  14107.254","iter:  534<br />train_rmse_mean:  14096.078","iter:  535<br />train_rmse_mean:  14084.885","iter:  536<br />train_rmse_mean:  14075.586","iter:  537<br />train_rmse_mean:  14065.577","iter:  538<br />train_rmse_mean:  14054.012","iter:  539<br />train_rmse_mean:  14045.137","iter:  540<br />train_rmse_mean:  14037.708","iter:  541<br />train_rmse_mean:  14027.636","iter:  542<br />train_rmse_mean:  14018.576","iter:  543<br />train_rmse_mean:  14010.353","iter:  544<br />train_rmse_mean:  14000.851","iter:  545<br />train_rmse_mean:  13990.316","iter:  546<br />train_rmse_mean:  13979.300","iter:  547<br />train_rmse_mean:  13968.844","iter:  548<br />train_rmse_mean:  13959.404","iter:  549<br />train_rmse_mean:  13948.479","iter:  550<br />train_rmse_mean:  13939.955","iter:  551<br />train_rmse_mean:  13929.682","iter:  552<br />train_rmse_mean:  13920.154","iter:  553<br />train_rmse_mean:  13909.210","iter:  554<br />train_rmse_mean:  13901.576","iter:  555<br />train_rmse_mean:  13889.952","iter:  556<br />train_rmse_mean:  13879.861","iter:  557<br />train_rmse_mean:  13871.313","iter:  558<br />train_rmse_mean:  13860.874","iter:  559<br />train_rmse_mean:  13851.583","iter:  560<br />train_rmse_mean:  13841.124","iter:  561<br />train_rmse_mean:  13830.261","iter:  562<br />train_rmse_mean:  13822.340","iter:  563<br />train_rmse_mean:  13813.484","iter:  564<br />train_rmse_mean:  13805.931","iter:  565<br />train_rmse_mean:  13798.579","iter:  566<br />train_rmse_mean:  13787.822","iter:  567<br />train_rmse_mean:  13778.511","iter:  568<br />train_rmse_mean:  13770.427","iter:  569<br />train_rmse_mean:  13761.471","iter:  570<br />train_rmse_mean:  13753.286","iter:  571<br />train_rmse_mean:  13744.072","iter:  572<br />train_rmse_mean:  13735.977","iter:  573<br />train_rmse_mean:  13727.402","iter:  574<br />train_rmse_mean:  13716.833","iter:  575<br />train_rmse_mean:  13708.365","iter:  576<br />train_rmse_mean:  13699.950","iter:  577<br />train_rmse_mean:  13690.309","iter:  578<br />train_rmse_mean:  13680.700","iter:  579<br />train_rmse_mean:  13671.761","iter:  580<br />train_rmse_mean:  13663.192","iter:  581<br />train_rmse_mean:  13653.411","iter:  582<br />train_rmse_mean:  13643.796","iter:  583<br />train_rmse_mean:  13634.057","iter:  584<br />train_rmse_mean:  13624.798","iter:  585<br />train_rmse_mean:  13616.857","iter:  586<br />train_rmse_mean:  13609.170","iter:  587<br />train_rmse_mean:  13600.292","iter:  588<br />train_rmse_mean:  13591.833","iter:  589<br />train_rmse_mean:  13583.717","iter:  590<br />train_rmse_mean:  13574.012","iter:  591<br />train_rmse_mean:  13564.640","iter:  592<br />train_rmse_mean:  13554.737","iter:  593<br />train_rmse_mean:  13546.739","iter:  594<br />train_rmse_mean:  13536.827","iter:  595<br />train_rmse_mean:  13528.452","iter:  596<br />train_rmse_mean:  13518.483","iter:  597<br />train_rmse_mean:  13510.968","iter:  598<br />train_rmse_mean:  13503.774","iter:  599<br />train_rmse_mean:  13494.879","iter:  600<br />train_rmse_mean:  13485.911","iter:  601<br />train_rmse_mean:  13477.944","iter:  602<br />train_rmse_mean:  13470.535","iter:  603<br />train_rmse_mean:  13462.548","iter:  604<br />train_rmse_mean:  13456.069","iter:  605<br />train_rmse_mean:  13449.038","iter:  606<br />train_rmse_mean:  13440.055","iter:  607<br />train_rmse_mean:  13430.029","iter:  608<br />train_rmse_mean:  13421.128","iter:  609<br />train_rmse_mean:  13412.522","iter:  610<br />train_rmse_mean:  13403.049","iter:  611<br />train_rmse_mean:  13393.156","iter:  612<br />train_rmse_mean:  13383.913","iter:  613<br />train_rmse_mean:  13375.724","iter:  614<br />train_rmse_mean:  13367.927","iter:  615<br />train_rmse_mean:  13359.993","iter:  616<br />train_rmse_mean:  13350.898","iter:  617<br />train_rmse_mean:  13342.739","iter:  618<br />train_rmse_mean:  13335.737","iter:  619<br />train_rmse_mean:  13328.965","iter:  620<br />train_rmse_mean:  13319.510","iter:  621<br />train_rmse_mean:  13311.113","iter:  622<br />train_rmse_mean:  13304.007","iter:  623<br />train_rmse_mean:  13296.541","iter:  624<br />train_rmse_mean:  13289.054","iter:  625<br />train_rmse_mean:  13283.243","iter:  626<br />train_rmse_mean:  13273.509","iter:  627<br />train_rmse_mean:  13265.839","iter:  628<br />train_rmse_mean:  13257.879","iter:  629<br />train_rmse_mean:  13252.496","iter:  630<br />train_rmse_mean:  13244.958","iter:  631<br />train_rmse_mean:  13236.537","iter:  632<br />train_rmse_mean:  13228.780","iter:  633<br />train_rmse_mean:  13221.494","iter:  634<br />train_rmse_mean:  13213.664","iter:  635<br />train_rmse_mean:  13202.053","iter:  636<br />train_rmse_mean:  13192.563","iter:  637<br />train_rmse_mean:  13183.788","iter:  638<br />train_rmse_mean:  13177.243","iter:  639<br />train_rmse_mean:  13168.649","iter:  640<br />train_rmse_mean:  13159.263","iter:  641<br />train_rmse_mean:  13150.584","iter:  642<br />train_rmse_mean:  13142.105","iter:  643<br />train_rmse_mean:  13134.089","iter:  644<br />train_rmse_mean:  13126.626","iter:  645<br />train_rmse_mean:  13119.100","iter:  646<br />train_rmse_mean:  13108.297","iter:  647<br />train_rmse_mean:  13099.396","iter:  648<br />train_rmse_mean:  13091.169","iter:  649<br />train_rmse_mean:  13082.833","iter:  650<br />train_rmse_mean:  13074.625","iter:  651<br />train_rmse_mean:  13067.956","iter:  652<br />train_rmse_mean:  13060.404","iter:  653<br />train_rmse_mean:  13051.201","iter:  654<br />train_rmse_mean:  13043.513","iter:  655<br />train_rmse_mean:  13036.948","iter:  656<br />train_rmse_mean:  13028.019","iter:  657<br />train_rmse_mean:  13019.448","iter:  658<br />train_rmse_mean:  13012.287","iter:  659<br />train_rmse_mean:  13004.343","iter:  660<br />train_rmse_mean:  12997.259","iter:  661<br />train_rmse_mean:  12991.598","iter:  662<br />train_rmse_mean:  12982.621","iter:  663<br />train_rmse_mean:  12975.840","iter:  664<br />train_rmse_mean:  12967.300","iter:  665<br />train_rmse_mean:  12957.715","iter:  666<br />train_rmse_mean:  12950.952","iter:  667<br />train_rmse_mean:  12943.536","iter:  668<br />train_rmse_mean:  12933.934","iter:  669<br />train_rmse_mean:  12927.972","iter:  670<br />train_rmse_mean:  12920.010","iter:  671<br />train_rmse_mean:  12912.136","iter:  672<br />train_rmse_mean:  12904.667","iter:  673<br />train_rmse_mean:  12896.339","iter:  674<br />train_rmse_mean:  12887.660","iter:  675<br />train_rmse_mean:  12879.921","iter:  676<br />train_rmse_mean:  12873.007","iter:  677<br />train_rmse_mean:  12864.699","iter:  678<br />train_rmse_mean:  12858.222","iter:  679<br />train_rmse_mean:  12850.551","iter:  680<br />train_rmse_mean:  12843.385","iter:  681<br />train_rmse_mean:  12833.891","iter:  682<br />train_rmse_mean:  12826.817","iter:  683<br />train_rmse_mean:  12818.921","iter:  684<br />train_rmse_mean:  12811.407","iter:  685<br />train_rmse_mean:  12804.812","iter:  686<br />train_rmse_mean:  12799.190","iter:  687<br />train_rmse_mean:  12791.854","iter:  688<br />train_rmse_mean:  12782.377","iter:  689<br />train_rmse_mean:  12775.207","iter:  690<br />train_rmse_mean:  12766.559","iter:  691<br />train_rmse_mean:  12760.337","iter:  692<br />train_rmse_mean:  12752.636","iter:  693<br />train_rmse_mean:  12746.202","iter:  694<br />train_rmse_mean:  12738.001","iter:  695<br />train_rmse_mean:  12731.706","iter:  696<br />train_rmse_mean:  12724.306","iter:  697<br />train_rmse_mean:  12716.643","iter:  698<br />train_rmse_mean:  12708.434","iter:  699<br />train_rmse_mean:  12700.224","iter:  700<br />train_rmse_mean:  12694.357","iter:  701<br />train_rmse_mean:  12686.367","iter:  702<br />train_rmse_mean:  12678.462","iter:  703<br />train_rmse_mean:  12671.185","iter:  704<br />train_rmse_mean:  12663.769","iter:  705<br />train_rmse_mean:  12656.432","iter:  706<br />train_rmse_mean:  12649.516","iter:  707<br />train_rmse_mean:  12640.743","iter:  708<br />train_rmse_mean:  12632.146","iter:  709<br />train_rmse_mean:  12625.006","iter:  710<br />train_rmse_mean:  12617.228","iter:  711<br />train_rmse_mean:  12610.575","iter:  712<br />train_rmse_mean:  12604.301","iter:  713<br />train_rmse_mean:  12595.992","iter:  714<br />train_rmse_mean:  12588.608","iter:  715<br />train_rmse_mean:  12581.455","iter:  716<br />train_rmse_mean:  12573.209","iter:  717<br />train_rmse_mean:  12566.763","iter:  718<br />train_rmse_mean:  12561.247","iter:  719<br />train_rmse_mean:  12554.843","iter:  720<br />train_rmse_mean:  12547.840","iter:  721<br />train_rmse_mean:  12540.200","iter:  722<br />train_rmse_mean:  12531.930","iter:  723<br />train_rmse_mean:  12523.856","iter:  724<br />train_rmse_mean:  12517.464","iter:  725<br />train_rmse_mean:  12510.601","iter:  726<br />train_rmse_mean:  12503.089","iter:  727<br />train_rmse_mean:  12497.161","iter:  728<br />train_rmse_mean:  12489.289","iter:  729<br />train_rmse_mean:  12482.511","iter:  730<br />train_rmse_mean:  12473.510","iter:  731<br />train_rmse_mean:  12466.108","iter:  732<br />train_rmse_mean:  12459.293","iter:  733<br />train_rmse_mean:  12453.106","iter:  734<br />train_rmse_mean:  12446.017","iter:  735<br />train_rmse_mean:  12438.818","iter:  736<br />train_rmse_mean:  12433.281","iter:  737<br />train_rmse_mean:  12424.955","iter:  738<br />train_rmse_mean:  12417.929","iter:  739<br />train_rmse_mean:  12411.142","iter:  740<br />train_rmse_mean:  12404.089","iter:  741<br />train_rmse_mean:  12396.787","iter:  742<br />train_rmse_mean:  12388.439","iter:  743<br />train_rmse_mean:  12382.474","iter:  744<br />train_rmse_mean:  12375.388","iter:  745<br />train_rmse_mean:  12367.959","iter:  746<br />train_rmse_mean:  12361.246","iter:  747<br />train_rmse_mean:  12353.644","iter:  748<br />train_rmse_mean:  12346.183","iter:  749<br />train_rmse_mean:  12339.189","iter:  750<br />train_rmse_mean:  12331.051","iter:  751<br />train_rmse_mean:  12322.872","iter:  752<br />train_rmse_mean:  12316.611","iter:  753<br />train_rmse_mean:  12309.106","iter:  754<br />train_rmse_mean:  12300.624","iter:  755<br />train_rmse_mean:  12294.305","iter:  756<br />train_rmse_mean:  12286.517","iter:  757<br />train_rmse_mean:  12278.651","iter:  758<br />train_rmse_mean:  12271.964","iter:  759<br />train_rmse_mean:  12265.328","iter:  760<br />train_rmse_mean:  12257.914","iter:  761<br />train_rmse_mean:  12251.408","iter:  762<br />train_rmse_mean:  12243.606","iter:  763<br />train_rmse_mean:  12236.621","iter:  764<br />train_rmse_mean:  12230.554","iter:  765<br />train_rmse_mean:  12225.287","iter:  766<br />train_rmse_mean:  12217.753","iter:  767<br />train_rmse_mean:  12211.172","iter:  768<br />train_rmse_mean:  12205.842","iter:  769<br />train_rmse_mean:  12198.528","iter:  770<br />train_rmse_mean:  12193.329","iter:  771<br />train_rmse_mean:  12186.763","iter:  772<br />train_rmse_mean:  12180.302","iter:  773<br />train_rmse_mean:  12172.343","iter:  774<br />train_rmse_mean:  12165.009","iter:  775<br />train_rmse_mean:  12157.531","iter:  776<br />train_rmse_mean:  12150.010","iter:  777<br />train_rmse_mean:  12144.611","iter:  778<br />train_rmse_mean:  12136.894","iter:  779<br />train_rmse_mean:  12130.704","iter:  780<br />train_rmse_mean:  12124.375","iter:  781<br />train_rmse_mean:  12117.764","iter:  782<br />train_rmse_mean:  12111.512","iter:  783<br />train_rmse_mean:  12104.704","iter:  784<br />train_rmse_mean:  12099.204","iter:  785<br />train_rmse_mean:  12093.817","iter:  786<br />train_rmse_mean:  12087.181","iter:  787<br />train_rmse_mean:  12080.870","iter:  788<br />train_rmse_mean:  12074.419","iter:  789<br />train_rmse_mean:  12067.416","iter:  790<br />train_rmse_mean:  12059.785","iter:  791<br />train_rmse_mean:  12052.712","iter:  792<br />train_rmse_mean:  12048.132","iter:  793<br />train_rmse_mean:  12041.700","iter:  794<br />train_rmse_mean:  12035.469","iter:  795<br />train_rmse_mean:  12027.785","iter:  796<br />train_rmse_mean:  12020.760","iter:  797<br />train_rmse_mean:  12014.800","iter:  798<br />train_rmse_mean:  12008.686","iter:  799<br />train_rmse_mean:  12002.216","iter:  800<br />train_rmse_mean:  11995.739","iter:  801<br />train_rmse_mean:  11988.723","iter:  802<br />train_rmse_mean:  11982.553","iter:  803<br />train_rmse_mean:  11975.602","iter:  804<br />train_rmse_mean:  11969.707","iter:  805<br />train_rmse_mean:  11962.545","iter:  806<br />train_rmse_mean:  11956.613","iter:  807<br />train_rmse_mean:  11950.985","iter:  808<br />train_rmse_mean:  11945.278","iter:  809<br />train_rmse_mean:  11939.633","iter:  810<br />train_rmse_mean:  11934.379","iter:  811<br />train_rmse_mean:  11927.580","iter:  812<br />train_rmse_mean:  11919.660","iter:  813<br />train_rmse_mean:  11913.135","iter:  814<br />train_rmse_mean:  11905.857","iter:  815<br />train_rmse_mean:  11898.525","iter:  816<br />train_rmse_mean:  11891.617","iter:  817<br />train_rmse_mean:  11884.656","iter:  818<br />train_rmse_mean:  11878.135","iter:  819<br />train_rmse_mean:  11872.518","iter:  820<br />train_rmse_mean:  11867.235","iter:  821<br />train_rmse_mean:  11860.300","iter:  822<br />train_rmse_mean:  11854.498","iter:  823<br />train_rmse_mean:  11847.647","iter:  824<br />train_rmse_mean:  11841.664","iter:  825<br />train_rmse_mean:  11833.940","iter:  826<br />train_rmse_mean:  11827.960","iter:  827<br />train_rmse_mean:  11821.434","iter:  828<br />train_rmse_mean:  11814.142","iter:  829<br />train_rmse_mean:  11808.058","iter:  830<br />train_rmse_mean:  11802.265","iter:  831<br />train_rmse_mean:  11796.753","iter:  832<br />train_rmse_mean:  11790.262","iter:  833<br />train_rmse_mean:  11784.708","iter:  834<br />train_rmse_mean:  11777.766","iter:  835<br />train_rmse_mean:  11771.384","iter:  836<br />train_rmse_mean:  11764.868","iter:  837<br />train_rmse_mean:  11758.050","iter:  838<br />train_rmse_mean:  11752.716","iter:  839<br />train_rmse_mean:  11747.088","iter:  840<br />train_rmse_mean:  11740.195","iter:  841<br />train_rmse_mean:  11736.077","iter:  842<br />train_rmse_mean:  11730.650","iter:  843<br />train_rmse_mean:  11724.814","iter:  844<br />train_rmse_mean:  11718.538","iter:  845<br />train_rmse_mean:  11712.361","iter:  846<br />train_rmse_mean:  11707.504","iter:  847<br />train_rmse_mean:  11701.652","iter:  848<br />train_rmse_mean:  11694.767","iter:  849<br />train_rmse_mean:  11688.576","iter:  850<br />train_rmse_mean:  11682.583","iter:  851<br />train_rmse_mean:  11676.753","iter:  852<br />train_rmse_mean:  11668.736","iter:  853<br />train_rmse_mean:  11661.234","iter:  854<br />train_rmse_mean:  11656.502","iter:  855<br />train_rmse_mean:  11650.746","iter:  856<br />train_rmse_mean:  11645.176","iter:  857<br />train_rmse_mean:  11639.610","iter:  858<br />train_rmse_mean:  11632.598","iter:  859<br />train_rmse_mean:  11626.626","iter:  860<br />train_rmse_mean:  11620.370","iter:  861<br />train_rmse_mean:  11615.241","iter:  862<br />train_rmse_mean:  11609.520","iter:  863<br />train_rmse_mean:  11602.752","iter:  864<br />train_rmse_mean:  11596.967","iter:  865<br />train_rmse_mean:  11590.654","iter:  866<br />train_rmse_mean:  11585.081","iter:  867<br />train_rmse_mean:  11579.368","iter:  868<br />train_rmse_mean:  11573.598","iter:  869<br />train_rmse_mean:  11566.795","iter:  870<br />train_rmse_mean:  11559.967","iter:  871<br />train_rmse_mean:  11553.079","iter:  872<br />train_rmse_mean:  11546.945","iter:  873<br />train_rmse_mean:  11540.635","iter:  874<br />train_rmse_mean:  11535.140","iter:  875<br />train_rmse_mean:  11530.264","iter:  876<br />train_rmse_mean:  11525.560","iter:  877<br />train_rmse_mean:  11519.036","iter:  878<br />train_rmse_mean:  11513.239","iter:  879<br />train_rmse_mean:  11506.429","iter:  880<br />train_rmse_mean:  11501.263","iter:  881<br />train_rmse_mean:  11495.821","iter:  882<br />train_rmse_mean:  11489.663","iter:  883<br />train_rmse_mean:  11483.317","iter:  884<br />train_rmse_mean:  11477.160","iter:  885<br />train_rmse_mean:  11469.387","iter:  886<br />train_rmse_mean:  11464.047","iter:  887<br />train_rmse_mean:  11458.839","iter:  888<br />train_rmse_mean:  11453.100","iter:  889<br />train_rmse_mean:  11446.909","iter:  890<br />train_rmse_mean:  11440.813","iter:  891<br />train_rmse_mean:  11434.386","iter:  892<br />train_rmse_mean:  11427.634","iter:  893<br />train_rmse_mean:  11422.049","iter:  894<br />train_rmse_mean:  11416.299","iter:  895<br />train_rmse_mean:  11411.100","iter:  896<br />train_rmse_mean:  11405.835","iter:  897<br />train_rmse_mean:  11399.292","iter:  898<br />train_rmse_mean:  11393.290","iter:  899<br />train_rmse_mean:  11388.132","iter:  900<br />train_rmse_mean:  11380.831","iter:  901<br />train_rmse_mean:  11376.468","iter:  902<br />train_rmse_mean:  11371.629","iter:  903<br />train_rmse_mean:  11365.296","iter:  904<br />train_rmse_mean:  11360.549","iter:  905<br />train_rmse_mean:  11354.220","iter:  906<br />train_rmse_mean:  11348.535","iter:  907<br />train_rmse_mean:  11344.454","iter:  908<br />train_rmse_mean:  11338.180","iter:  909<br />train_rmse_mean:  11332.669","iter:  910<br />train_rmse_mean:  11326.997","iter:  911<br />train_rmse_mean:  11322.263","iter:  912<br />train_rmse_mean:  11316.192","iter:  913<br />train_rmse_mean:  11312.137","iter:  914<br />train_rmse_mean:  11306.699","iter:  915<br />train_rmse_mean:  11299.235","iter:  916<br />train_rmse_mean:  11293.697","iter:  917<br />train_rmse_mean:  11287.076","iter:  918<br />train_rmse_mean:  11281.673","iter:  919<br />train_rmse_mean:  11277.248","iter:  920<br />train_rmse_mean:  11270.936","iter:  921<br />train_rmse_mean:  11265.755","iter:  922<br />train_rmse_mean:  11259.740","iter:  923<br />train_rmse_mean:  11254.003","iter:  924<br />train_rmse_mean:  11248.560","iter:  925<br />train_rmse_mean:  11242.316","iter:  926<br />train_rmse_mean:  11235.585","iter:  927<br />train_rmse_mean:  11228.643","iter:  928<br />train_rmse_mean:  11222.927","iter:  929<br />train_rmse_mean:  11217.024","iter:  930<br />train_rmse_mean:  11212.365","iter:  931<br />train_rmse_mean:  11208.885","iter:  932<br />train_rmse_mean:  11202.721","iter:  933<br />train_rmse_mean:  11196.955","iter:  934<br />train_rmse_mean:  11192.409","iter:  935<br />train_rmse_mean:  11187.013","iter:  936<br />train_rmse_mean:  11180.086","iter:  937<br />train_rmse_mean:  11174.014","iter:  938<br />train_rmse_mean:  11169.459","iter:  939<br />train_rmse_mean:  11163.283","iter:  940<br />train_rmse_mean:  11158.144","iter:  941<br />train_rmse_mean:  11153.984","iter:  942<br />train_rmse_mean:  11148.713","iter:  943<br />train_rmse_mean:  11143.834","iter:  944<br />train_rmse_mean:  11136.870","iter:  945<br />train_rmse_mean:  11131.800","iter:  946<br />train_rmse_mean:  11127.655","iter:  947<br />train_rmse_mean:  11123.021","iter:  948<br />train_rmse_mean:  11117.642","iter:  949<br />train_rmse_mean:  11111.207","iter:  950<br />train_rmse_mean:  11105.883","iter:  951<br />train_rmse_mean:  11100.697","iter:  952<br />train_rmse_mean:  11095.961","iter:  953<br />train_rmse_mean:  11089.597","iter:  954<br />train_rmse_mean:  11083.578","iter:  955<br />train_rmse_mean:  11077.066","iter:  956<br />train_rmse_mean:  11072.915","iter:  957<br />train_rmse_mean:  11067.578","iter:  958<br />train_rmse_mean:  11062.997","iter:  959<br />train_rmse_mean:  11056.965","iter:  960<br />train_rmse_mean:  11050.575","iter:  961<br />train_rmse_mean:  11045.723","iter:  962<br />train_rmse_mean:  11039.491","iter:  963<br />train_rmse_mean:  11033.776","iter:  964<br />train_rmse_mean:  11027.758","iter:  965<br />train_rmse_mean:  11022.302","iter:  966<br />train_rmse_mean:  11017.208","iter:  967<br />train_rmse_mean:  11012.453","iter:  968<br />train_rmse_mean:  11006.296","iter:  969<br />train_rmse_mean:  11001.007","iter:  970<br />train_rmse_mean:  10996.020","iter:  971<br />train_rmse_mean:  10990.852","iter:  972<br />train_rmse_mean:  10986.140","iter:  973<br />train_rmse_mean:  10981.108","iter:  974<br />train_rmse_mean:  10976.158","iter:  975<br />train_rmse_mean:  10970.561","iter:  976<br />train_rmse_mean:  10965.622","iter:  977<br />train_rmse_mean:  10960.631","iter:  978<br />train_rmse_mean:  10955.534","iter:  979<br />train_rmse_mean:  10951.080","iter:  980<br />train_rmse_mean:  10945.491","iter:  981<br />train_rmse_mean:  10939.147","iter:  982<br />train_rmse_mean:  10932.525","iter:  983<br />train_rmse_mean:  10925.599","iter:  984<br />train_rmse_mean:  10918.997","iter:  985<br />train_rmse_mean:  10912.953","iter:  986<br />train_rmse_mean:  10906.750","iter:  987<br />train_rmse_mean:  10900.926","iter:  988<br />train_rmse_mean:  10895.195","iter:  989<br />train_rmse_mean:  10889.801","iter:  990<br />train_rmse_mean:  10884.452","iter:  991<br />train_rmse_mean:  10879.670","iter:  992<br />train_rmse_mean:  10874.510","iter:  993<br />train_rmse_mean:  10868.674","iter:  994<br />train_rmse_mean:  10862.814","iter:  995<br />train_rmse_mean:  10857.745","iter:  996<br />train_rmse_mean:  10851.940","iter:  997<br />train_rmse_mean:  10846.574","iter:  998<br />train_rmse_mean:  10841.211","iter:  999<br />train_rmse_mean:  10835.518","iter: 1000<br />train_rmse_mean:  10830.136","iter: 1001<br />train_rmse_mean:  10823.734","iter: 1002<br />train_rmse_mean:  10819.205","iter: 1003<br />train_rmse_mean:  10813.821","iter: 1004<br />train_rmse_mean:  10808.156","iter: 1005<br />train_rmse_mean:  10802.111","iter: 1006<br />train_rmse_mean:  10796.003","iter: 1007<br />train_rmse_mean:  10790.228","iter: 1008<br />train_rmse_mean:  10784.819","iter: 1009<br />train_rmse_mean:  10779.125","iter: 1010<br />train_rmse_mean:  10774.380","iter: 1011<br />train_rmse_mean:  10768.671","iter: 1012<br />train_rmse_mean:  10763.309","iter: 1013<br />train_rmse_mean:  10757.991","iter: 1014<br />train_rmse_mean:  10753.232","iter: 1015<br />train_rmse_mean:  10748.484","iter: 1016<br />train_rmse_mean:  10742.336","iter: 1017<br />train_rmse_mean:  10737.557","iter: 1018<br />train_rmse_mean:  10733.600","iter: 1019<br />train_rmse_mean:  10729.111","iter: 1020<br />train_rmse_mean:  10723.521","iter: 1021<br />train_rmse_mean:  10718.765","iter: 1022<br />train_rmse_mean:  10713.240","iter: 1023<br />train_rmse_mean:  10708.235","iter: 1024<br />train_rmse_mean:  10702.364","iter: 1025<br />train_rmse_mean:  10697.438","iter: 1026<br />train_rmse_mean:  10691.733","iter: 1027<br />train_rmse_mean:  10686.986","iter: 1028<br />train_rmse_mean:  10682.366","iter: 1029<br />train_rmse_mean:  10678.299","iter: 1030<br />train_rmse_mean:  10674.057","iter: 1031<br />train_rmse_mean:  10669.744","iter: 1032<br />train_rmse_mean:  10664.668","iter: 1033<br />train_rmse_mean:  10660.012","iter: 1034<br />train_rmse_mean:  10655.143","iter: 1035<br />train_rmse_mean:  10649.008","iter: 1036<br />train_rmse_mean:  10643.545","iter: 1037<br />train_rmse_mean:  10638.312","iter: 1038<br />train_rmse_mean:  10633.174","iter: 1039<br />train_rmse_mean:  10627.879","iter: 1040<br />train_rmse_mean:  10622.487","iter: 1041<br />train_rmse_mean:  10618.104","iter: 1042<br />train_rmse_mean:  10612.988","iter: 1043<br />train_rmse_mean:  10607.928","iter: 1044<br />train_rmse_mean:  10602.890","iter: 1045<br />train_rmse_mean:  10597.367","iter: 1046<br />train_rmse_mean:  10592.300","iter: 1047<br />train_rmse_mean:  10586.648","iter: 1048<br />train_rmse_mean:  10580.770","iter: 1049<br />train_rmse_mean:  10576.525","iter: 1050<br />train_rmse_mean:  10570.636","iter: 1051<br />train_rmse_mean:  10565.202","iter: 1052<br />train_rmse_mean:  10559.743","iter: 1053<br />train_rmse_mean:  10554.163","iter: 1054<br />train_rmse_mean:  10548.228","iter: 1055<br />train_rmse_mean:  10542.374","iter: 1056<br />train_rmse_mean:  10537.559","iter: 1057<br />train_rmse_mean:  10533.055","iter: 1058<br />train_rmse_mean:  10528.728","iter: 1059<br />train_rmse_mean:  10524.260","iter: 1060<br />train_rmse_mean:  10518.911","iter: 1061<br />train_rmse_mean:  10514.675","iter: 1062<br />train_rmse_mean:  10509.484","iter: 1063<br />train_rmse_mean:  10505.262","iter: 1064<br />train_rmse_mean:  10500.508","iter: 1065<br />train_rmse_mean:  10495.784","iter: 1066<br />train_rmse_mean:  10490.977","iter: 1067<br />train_rmse_mean:  10486.522","iter: 1068<br />train_rmse_mean:  10480.146","iter: 1069<br />train_rmse_mean:  10475.960","iter: 1070<br />train_rmse_mean:  10471.975","iter: 1071<br />train_rmse_mean:  10467.807","iter: 1072<br />train_rmse_mean:  10463.018","iter: 1073<br />train_rmse_mean:  10457.593","iter: 1074<br />train_rmse_mean:  10453.181","iter: 1075<br />train_rmse_mean:  10448.139","iter: 1076<br />train_rmse_mean:  10443.219","iter: 1077<br />train_rmse_mean:  10438.607","iter: 1078<br />train_rmse_mean:  10433.154","iter: 1079<br />train_rmse_mean:  10428.010","iter: 1080<br />train_rmse_mean:  10422.770","iter: 1081<br />train_rmse_mean:  10418.132","iter: 1082<br />train_rmse_mean:  10414.207","iter: 1083<br />train_rmse_mean:  10408.925","iter: 1084<br />train_rmse_mean:  10403.503","iter: 1085<br />train_rmse_mean:  10399.209","iter: 1086<br />train_rmse_mean:  10395.008","iter: 1087<br />train_rmse_mean:  10390.796","iter: 1088<br />train_rmse_mean:  10386.908","iter: 1089<br />train_rmse_mean:  10382.497","iter: 1090<br />train_rmse_mean:  10378.747","iter: 1091<br />train_rmse_mean:  10372.963","iter: 1092<br />train_rmse_mean:  10369.040","iter: 1093<br />train_rmse_mean:  10364.701","iter: 1094<br />train_rmse_mean:  10360.752","iter: 1095<br />train_rmse_mean:  10356.032","iter: 1096<br />train_rmse_mean:  10350.948","iter: 1097<br />train_rmse_mean:  10346.207","iter: 1098<br />train_rmse_mean:  10340.742","iter: 1099<br />train_rmse_mean:  10336.937","iter: 1100<br />train_rmse_mean:  10331.617","iter: 1101<br />train_rmse_mean:  10326.894","iter: 1102<br />train_rmse_mean:  10323.250","iter: 1103<br />train_rmse_mean:  10318.806","iter: 1104<br />train_rmse_mean:  10313.394","iter: 1105<br />train_rmse_mean:  10309.845","iter: 1106<br />train_rmse_mean:  10304.825","iter: 1107<br />train_rmse_mean:  10300.812","iter: 1108<br />train_rmse_mean:  10296.142","iter: 1109<br />train_rmse_mean:  10291.982","iter: 1110<br />train_rmse_mean:  10286.886","iter: 1111<br />train_rmse_mean:  10282.088","iter: 1112<br />train_rmse_mean:  10277.109","iter: 1113<br />train_rmse_mean:  10273.089","iter: 1114<br />train_rmse_mean:  10269.351","iter: 1115<br />train_rmse_mean:  10263.937","iter: 1116<br />train_rmse_mean:  10259.682","iter: 1117<br />train_rmse_mean:  10255.484","iter: 1118<br />train_rmse_mean:  10250.624","iter: 1119<br />train_rmse_mean:  10245.914","iter: 1120<br />train_rmse_mean:  10241.526","iter: 1121<br />train_rmse_mean:  10237.626","iter: 1122<br />train_rmse_mean:  10232.882","iter: 1123<br />train_rmse_mean:  10227.618","iter: 1124<br />train_rmse_mean:  10221.966","iter: 1125<br />train_rmse_mean:  10218.571","iter: 1126<br />train_rmse_mean:  10214.306","iter: 1127<br />train_rmse_mean:  10209.890","iter: 1128<br />train_rmse_mean:  10204.620","iter: 1129<br />train_rmse_mean:  10200.537","iter: 1130<br />train_rmse_mean:  10195.547","iter: 1131<br />train_rmse_mean:  10192.506","iter: 1132<br />train_rmse_mean:  10188.308","iter: 1133<br />train_rmse_mean:  10184.693","iter: 1134<br />train_rmse_mean:  10179.139","iter: 1135<br />train_rmse_mean:  10173.822","iter: 1136<br />train_rmse_mean:  10169.121","iter: 1137<br />train_rmse_mean:  10163.292","iter: 1138<br />train_rmse_mean:  10158.689","iter: 1139<br />train_rmse_mean:  10154.450","iter: 1140<br />train_rmse_mean:  10151.024","iter: 1141<br />train_rmse_mean:  10146.915","iter: 1142<br />train_rmse_mean:  10142.658","iter: 1143<br />train_rmse_mean:  10138.264","iter: 1144<br />train_rmse_mean:  10134.068","iter: 1145<br />train_rmse_mean:  10129.233","iter: 1146<br />train_rmse_mean:  10124.718","iter: 1147<br />train_rmse_mean:  10120.770","iter: 1148<br />train_rmse_mean:  10116.436","iter: 1149<br />train_rmse_mean:  10111.711","iter: 1150<br />train_rmse_mean:  10107.824","iter: 1151<br />train_rmse_mean:  10104.289","iter: 1152<br />train_rmse_mean:  10100.593","iter: 1153<br />train_rmse_mean:  10096.969","iter: 1154<br />train_rmse_mean:  10091.008","iter: 1155<br />train_rmse_mean:  10087.520","iter: 1156<br />train_rmse_mean:  10082.746","iter: 1157<br />train_rmse_mean:  10078.733","iter: 1158<br />train_rmse_mean:  10074.332","iter: 1159<br />train_rmse_mean:  10069.634","iter: 1160<br />train_rmse_mean:  10064.426","iter: 1161<br />train_rmse_mean:  10059.854","iter: 1162<br />train_rmse_mean:  10056.219","iter: 1163<br />train_rmse_mean:  10052.213","iter: 1164<br />train_rmse_mean:  10047.511","iter: 1165<br />train_rmse_mean:  10042.600","iter: 1166<br />train_rmse_mean:  10038.178","iter: 1167<br />train_rmse_mean:  10034.717","iter: 1168<br />train_rmse_mean:  10029.615","iter: 1169<br />train_rmse_mean:  10024.953","iter: 1170<br />train_rmse_mean:  10021.001","iter: 1171<br />train_rmse_mean:  10016.697","iter: 1172<br />train_rmse_mean:  10012.580","iter: 1173<br />train_rmse_mean:  10007.753","iter: 1174<br />train_rmse_mean:  10002.581","iter: 1175<br />train_rmse_mean:   9997.673","iter: 1176<br />train_rmse_mean:   9993.219","iter: 1177<br />train_rmse_mean:   9989.059","iter: 1178<br />train_rmse_mean:   9984.302","iter: 1179<br />train_rmse_mean:   9979.470","iter: 1180<br />train_rmse_mean:   9974.693","iter: 1181<br />train_rmse_mean:   9970.752","iter: 1182<br />train_rmse_mean:   9966.359","iter: 1183<br />train_rmse_mean:   9961.495","iter: 1184<br />train_rmse_mean:   9956.671","iter: 1185<br />train_rmse_mean:   9952.624","iter: 1186<br />train_rmse_mean:   9948.636","iter: 1187<br />train_rmse_mean:   9944.036","iter: 1188<br />train_rmse_mean:   9939.839","iter: 1189<br />train_rmse_mean:   9935.704","iter: 1190<br />train_rmse_mean:   9931.992","iter: 1191<br />train_rmse_mean:   9927.733","iter: 1192<br />train_rmse_mean:   9923.535","iter: 1193<br />train_rmse_mean:   9919.139","iter: 1194<br />train_rmse_mean:   9915.571","iter: 1195<br />train_rmse_mean:   9911.923","iter: 1196<br />train_rmse_mean:   9907.548","iter: 1197<br />train_rmse_mean:   9904.026","iter: 1198<br />train_rmse_mean:   9900.090","iter: 1199<br />train_rmse_mean:   9895.711","iter: 1200<br />train_rmse_mean:   9891.256","iter: 1201<br />train_rmse_mean:   9886.635","iter: 1202<br />train_rmse_mean:   9882.739","iter: 1203<br />train_rmse_mean:   9878.388","iter: 1204<br />train_rmse_mean:   9874.814","iter: 1205<br />train_rmse_mean:   9869.373","iter: 1206<br />train_rmse_mean:   9865.037","iter: 1207<br />train_rmse_mean:   9860.334","iter: 1208<br />train_rmse_mean:   9856.311","iter: 1209<br />train_rmse_mean:   9852.298","iter: 1210<br />train_rmse_mean:   9848.370","iter: 1211<br />train_rmse_mean:   9843.517","iter: 1212<br />train_rmse_mean:   9838.912","iter: 1213<br />train_rmse_mean:   9834.135","iter: 1214<br />train_rmse_mean:   9828.889","iter: 1215<br />train_rmse_mean:   9824.604","iter: 1216<br />train_rmse_mean:   9819.150","iter: 1217<br />train_rmse_mean:   9813.665","iter: 1218<br />train_rmse_mean:   9808.654","iter: 1219<br />train_rmse_mean:   9803.667","iter: 1220<br />train_rmse_mean:   9799.969","iter: 1221<br />train_rmse_mean:   9795.582","iter: 1222<br />train_rmse_mean:   9791.981","iter: 1223<br />train_rmse_mean:   9787.980","iter: 1224<br />train_rmse_mean:   9784.094","iter: 1225<br />train_rmse_mean:   9779.306","iter: 1226<br />train_rmse_mean:   9774.580","iter: 1227<br />train_rmse_mean:   9770.687","iter: 1228<br />train_rmse_mean:   9766.385","iter: 1229<br />train_rmse_mean:   9760.780","iter: 1230<br />train_rmse_mean:   9756.206","iter: 1231<br />train_rmse_mean:   9753.374","iter: 1232<br />train_rmse_mean:   9749.506","iter: 1233<br />train_rmse_mean:   9746.266","iter: 1234<br />train_rmse_mean:   9742.080","iter: 1235<br />train_rmse_mean:   9738.128","iter: 1236<br />train_rmse_mean:   9733.755","iter: 1237<br />train_rmse_mean:   9729.576","iter: 1238<br />train_rmse_mean:   9725.339","iter: 1239<br />train_rmse_mean:   9720.290","iter: 1240<br />train_rmse_mean:   9715.797","iter: 1241<br />train_rmse_mean:   9712.323","iter: 1242<br />train_rmse_mean:   9707.252","iter: 1243<br />train_rmse_mean:   9703.185","iter: 1244<br />train_rmse_mean:   9699.277","iter: 1245<br />train_rmse_mean:   9694.525","iter: 1246<br />train_rmse_mean:   9690.555","iter: 1247<br />train_rmse_mean:   9686.986","iter: 1248<br />train_rmse_mean:   9682.565","iter: 1249<br />train_rmse_mean:   9678.407","iter: 1250<br />train_rmse_mean:   9674.425","iter: 1251<br />train_rmse_mean:   9671.002","iter: 1252<br />train_rmse_mean:   9666.289","iter: 1253<br />train_rmse_mean:   9662.383","iter: 1254<br />train_rmse_mean:   9657.833","iter: 1255<br />train_rmse_mean:   9654.499","iter: 1256<br />train_rmse_mean:   9650.557","iter: 1257<br />train_rmse_mean:   9647.232","iter: 1258<br />train_rmse_mean:   9643.422","iter: 1259<br />train_rmse_mean:   9639.592","iter: 1260<br />train_rmse_mean:   9635.882","iter: 1261<br />train_rmse_mean:   9631.689","iter: 1262<br />train_rmse_mean:   9627.159","iter: 1263<br />train_rmse_mean:   9622.327","iter: 1264<br />train_rmse_mean:   9618.249","iter: 1265<br />train_rmse_mean:   9614.398","iter: 1266<br />train_rmse_mean:   9610.621","iter: 1267<br />train_rmse_mean:   9606.214","iter: 1268<br />train_rmse_mean:   9601.673","iter: 1269<br />train_rmse_mean:   9597.075","iter: 1270<br />train_rmse_mean:   9592.897","iter: 1271<br />train_rmse_mean:   9588.685","iter: 1272<br />train_rmse_mean:   9585.303","iter: 1273<br />train_rmse_mean:   9581.220","iter: 1274<br />train_rmse_mean:   9578.088","iter: 1275<br />train_rmse_mean:   9572.918","iter: 1276<br />train_rmse_mean:   9568.453","iter: 1277<br />train_rmse_mean:   9563.916","iter: 1278<br />train_rmse_mean:   9559.711","iter: 1279<br />train_rmse_mean:   9555.924","iter: 1280<br />train_rmse_mean:   9551.605","iter: 1281<br />train_rmse_mean:   9548.057","iter: 1282<br />train_rmse_mean:   9543.622","iter: 1283<br />train_rmse_mean:   9540.098","iter: 1284<br />train_rmse_mean:   9536.425","iter: 1285<br />train_rmse_mean:   9532.885","iter: 1286<br />train_rmse_mean:   9528.672","iter: 1287<br />train_rmse_mean:   9524.760","iter: 1288<br />train_rmse_mean:   9520.839","iter: 1289<br />train_rmse_mean:   9516.709","iter: 1290<br />train_rmse_mean:   9512.687","iter: 1291<br />train_rmse_mean:   9508.161","iter: 1292<br />train_rmse_mean:   9504.768","iter: 1293<br />train_rmse_mean:   9500.440","iter: 1294<br />train_rmse_mean:   9495.906","iter: 1295<br />train_rmse_mean:   9492.208","iter: 1296<br />train_rmse_mean:   9488.838","iter: 1297<br />train_rmse_mean:   9484.863","iter: 1298<br />train_rmse_mean:   9481.233","iter: 1299<br />train_rmse_mean:   9477.431","iter: 1300<br />train_rmse_mean:   9473.590","iter: 1301<br />train_rmse_mean:   9468.937","iter: 1302<br />train_rmse_mean:   9464.555","iter: 1303<br />train_rmse_mean:   9460.681","iter: 1304<br />train_rmse_mean:   9455.909","iter: 1305<br />train_rmse_mean:   9452.122","iter: 1306<br />train_rmse_mean:   9448.227","iter: 1307<br />train_rmse_mean:   9443.693","iter: 1308<br />train_rmse_mean:   9439.869","iter: 1309<br />train_rmse_mean:   9436.007","iter: 1310<br />train_rmse_mean:   9432.254","iter: 1311<br />train_rmse_mean:   9428.053","iter: 1312<br />train_rmse_mean:   9423.983","iter: 1313<br />train_rmse_mean:   9420.335","iter: 1314<br />train_rmse_mean:   9416.509","iter: 1315<br />train_rmse_mean:   9413.272","iter: 1316<br />train_rmse_mean:   9408.992","iter: 1317<br />train_rmse_mean:   9404.501","iter: 1318<br />train_rmse_mean:   9401.363","iter: 1319<br />train_rmse_mean:   9397.396","iter: 1320<br />train_rmse_mean:   9393.339","iter: 1321<br />train_rmse_mean:   9389.317","iter: 1322<br />train_rmse_mean:   9385.656","iter: 1323<br />train_rmse_mean:   9382.172","iter: 1324<br />train_rmse_mean:   9377.598","iter: 1325<br />train_rmse_mean:   9373.132","iter: 1326<br />train_rmse_mean:   9368.471","iter: 1327<br />train_rmse_mean:   9364.420","iter: 1328<br />train_rmse_mean:   9359.715","iter: 1329<br />train_rmse_mean:   9355.196","iter: 1330<br />train_rmse_mean:   9350.595","iter: 1331<br />train_rmse_mean:   9346.351","iter: 1332<br />train_rmse_mean:   9342.292","iter: 1333<br />train_rmse_mean:   9338.448","iter: 1334<br />train_rmse_mean:   9334.358","iter: 1335<br />train_rmse_mean:   9331.463","iter: 1336<br />train_rmse_mean:   9327.255","iter: 1337<br />train_rmse_mean:   9323.431","iter: 1338<br />train_rmse_mean:   9320.150","iter: 1339<br />train_rmse_mean:   9316.524","iter: 1340<br />train_rmse_mean:   9312.597","iter: 1341<br />train_rmse_mean:   9308.631","iter: 1342<br />train_rmse_mean:   9303.767","iter: 1343<br />train_rmse_mean:   9299.692","iter: 1344<br />train_rmse_mean:   9296.023","iter: 1345<br />train_rmse_mean:   9291.849","iter: 1346<br />train_rmse_mean:   9288.351","iter: 1347<br />train_rmse_mean:   9283.803","iter: 1348<br />train_rmse_mean:   9279.675","iter: 1349<br />train_rmse_mean:   9275.108","iter: 1350<br />train_rmse_mean:   9271.262","iter: 1351<br />train_rmse_mean:   9266.172","iter: 1352<br />train_rmse_mean:   9262.942","iter: 1353<br />train_rmse_mean:   9259.377","iter: 1354<br />train_rmse_mean:   9256.101","iter: 1355<br />train_rmse_mean:   9252.222","iter: 1356<br />train_rmse_mean:   9248.023","iter: 1357<br />train_rmse_mean:   9244.006","iter: 1358<br />train_rmse_mean:   9239.356","iter: 1359<br />train_rmse_mean:   9234.912","iter: 1360<br />train_rmse_mean:   9229.790","iter: 1361<br />train_rmse_mean:   9225.679","iter: 1362<br />train_rmse_mean:   9221.546","iter: 1363<br />train_rmse_mean:   9217.701","iter: 1364<br />train_rmse_mean:   9214.622","iter: 1365<br />train_rmse_mean:   9211.269","iter: 1366<br />train_rmse_mean:   9207.721","iter: 1367<br />train_rmse_mean:   9203.809","iter: 1368<br />train_rmse_mean:   9200.769","iter: 1369<br />train_rmse_mean:   9197.228","iter: 1370<br />train_rmse_mean:   9193.811","iter: 1371<br />train_rmse_mean:   9190.253","iter: 1372<br />train_rmse_mean:   9186.965","iter: 1373<br />train_rmse_mean:   9183.074","iter: 1374<br />train_rmse_mean:   9179.124","iter: 1375<br />train_rmse_mean:   9174.874","iter: 1376<br />train_rmse_mean:   9170.419","iter: 1377<br />train_rmse_mean:   9166.732","iter: 1378<br />train_rmse_mean:   9162.534","iter: 1379<br />train_rmse_mean:   9158.537","iter: 1380<br />train_rmse_mean:   9154.903","iter: 1381<br />train_rmse_mean:   9151.555","iter: 1382<br />train_rmse_mean:   9147.979","iter: 1383<br />train_rmse_mean:   9144.017","iter: 1384<br />train_rmse_mean:   9140.277","iter: 1385<br />train_rmse_mean:   9136.315","iter: 1386<br />train_rmse_mean:   9133.327","iter: 1387<br />train_rmse_mean:   9129.611","iter: 1388<br />train_rmse_mean:   9126.123","iter: 1389<br />train_rmse_mean:   9121.740","iter: 1390<br />train_rmse_mean:   9118.303","iter: 1391<br />train_rmse_mean:   9114.773","iter: 1392<br />train_rmse_mean:   9110.901","iter: 1393<br />train_rmse_mean:   9105.943","iter: 1394<br />train_rmse_mean:   9102.857","iter: 1395<br />train_rmse_mean:   9098.561","iter: 1396<br />train_rmse_mean:   9094.446","iter: 1397<br />train_rmse_mean:   9090.898","iter: 1398<br />train_rmse_mean:   9087.617","iter: 1399<br />train_rmse_mean:   9084.525","iter: 1400<br />train_rmse_mean:   9080.973","iter: 1401<br />train_rmse_mean:   9077.197","iter: 1402<br />train_rmse_mean:   9073.765","iter: 1403<br />train_rmse_mean:   9070.234","iter: 1404<br />train_rmse_mean:   9066.802","iter: 1405<br />train_rmse_mean:   9063.540","iter: 1406<br />train_rmse_mean:   9060.664","iter: 1407<br />train_rmse_mean:   9057.432","iter: 1408<br />train_rmse_mean:   9054.087","iter: 1409<br />train_rmse_mean:   9050.911","iter: 1410<br />train_rmse_mean:   9047.595","iter: 1411<br />train_rmse_mean:   9043.834","iter: 1412<br />train_rmse_mean:   9040.148","iter: 1413<br />train_rmse_mean:   9035.834","iter: 1414<br />train_rmse_mean:   9032.545","iter: 1415<br />train_rmse_mean:   9029.165","iter: 1416<br />train_rmse_mean:   9025.489","iter: 1417<br />train_rmse_mean:   9022.111","iter: 1418<br />train_rmse_mean:   9017.939","iter: 1419<br />train_rmse_mean:   9014.764","iter: 1420<br />train_rmse_mean:   9010.298","iter: 1421<br />train_rmse_mean:   9005.634","iter: 1422<br />train_rmse_mean:   9001.617","iter: 1423<br />train_rmse_mean:   8998.156","iter: 1424<br />train_rmse_mean:   8994.830","iter: 1425<br />train_rmse_mean:   8991.210","iter: 1426<br />train_rmse_mean:   8987.537","iter: 1427<br />train_rmse_mean:   8983.974","iter: 1428<br />train_rmse_mean:   8980.625","iter: 1429<br />train_rmse_mean:   8976.929","iter: 1430<br />train_rmse_mean:   8973.211","iter: 1431<br />train_rmse_mean:   8969.115","iter: 1432<br />train_rmse_mean:   8965.431","iter: 1433<br />train_rmse_mean:   8961.406","iter: 1434<br />train_rmse_mean:   8958.010","iter: 1435<br />train_rmse_mean:   8954.478","iter: 1436<br />train_rmse_mean:   8951.008","iter: 1437<br />train_rmse_mean:   8948.225","iter: 1438<br />train_rmse_mean:   8944.149","iter: 1439<br />train_rmse_mean:   8940.132","iter: 1440<br />train_rmse_mean:   8937.177","iter: 1441<br />train_rmse_mean:   8934.161","iter: 1442<br />train_rmse_mean:   8930.612","iter: 1443<br />train_rmse_mean:   8927.304","iter: 1444<br />train_rmse_mean:   8924.559","iter: 1445<br />train_rmse_mean:   8921.088","iter: 1446<br />train_rmse_mean:   8916.811","iter: 1447<br />train_rmse_mean:   8912.983","iter: 1448<br />train_rmse_mean:   8908.612","iter: 1449<br />train_rmse_mean:   8904.643","iter: 1450<br />train_rmse_mean:   8901.115","iter: 1451<br />train_rmse_mean:   8897.660","iter: 1452<br />train_rmse_mean:   8894.536","iter: 1453<br />train_rmse_mean:   8890.909","iter: 1454<br />train_rmse_mean:   8888.271","iter: 1455<br />train_rmse_mean:   8884.553","iter: 1456<br />train_rmse_mean:   8880.609","iter: 1457<br />train_rmse_mean:   8877.043","iter: 1458<br />train_rmse_mean:   8873.754","iter: 1459<br />train_rmse_mean:   8869.677","iter: 1460<br />train_rmse_mean:   8866.599","iter: 1461<br />train_rmse_mean:   8863.551","iter: 1462<br />train_rmse_mean:   8859.190","iter: 1463<br />train_rmse_mean:   8855.893","iter: 1464<br />train_rmse_mean:   8853.072","iter: 1465<br />train_rmse_mean:   8849.307","iter: 1466<br />train_rmse_mean:   8846.156","iter: 1467<br />train_rmse_mean:   8843.057","iter: 1468<br />train_rmse_mean:   8840.070","iter: 1469<br />train_rmse_mean:   8836.684","iter: 1470<br />train_rmse_mean:   8832.834","iter: 1471<br />train_rmse_mean:   8828.976","iter: 1472<br />train_rmse_mean:   8825.466","iter: 1473<br />train_rmse_mean:   8821.926","iter: 1474<br />train_rmse_mean:   8818.068","iter: 1475<br />train_rmse_mean:   8814.667","iter: 1476<br />train_rmse_mean:   8811.676","iter: 1477<br />train_rmse_mean:   8808.083","iter: 1478<br />train_rmse_mean:   8804.735","iter: 1479<br />train_rmse_mean:   8801.204","iter: 1480<br />train_rmse_mean:   8798.388","iter: 1481<br />train_rmse_mean:   8795.290","iter: 1482<br />train_rmse_mean:   8791.281","iter: 1483<br />train_rmse_mean:   8788.066","iter: 1484<br />train_rmse_mean:   8784.390","iter: 1485<br />train_rmse_mean:   8780.219","iter: 1486<br />train_rmse_mean:   8776.423","iter: 1487<br />train_rmse_mean:   8772.688","iter: 1488<br />train_rmse_mean:   8769.017","iter: 1489<br />train_rmse_mean:   8765.886","iter: 1490<br />train_rmse_mean:   8762.518","iter: 1491<br />train_rmse_mean:   8759.503","iter: 1492<br />train_rmse_mean:   8755.643","iter: 1493<br />train_rmse_mean:   8752.447","iter: 1494<br />train_rmse_mean:   8749.030","iter: 1495<br />train_rmse_mean:   8746.325","iter: 1496<br />train_rmse_mean:   8742.640","iter: 1497<br />train_rmse_mean:   8739.605","iter: 1498<br />train_rmse_mean:   8735.837","iter: 1499<br />train_rmse_mean:   8731.749","iter: 1500<br />train_rmse_mean:   8728.441","iter: 1501<br />train_rmse_mean:   8725.575","iter: 1502<br />train_rmse_mean:   8722.752","iter: 1503<br />train_rmse_mean:   8719.482","iter: 1504<br />train_rmse_mean:   8716.226","iter: 1505<br />train_rmse_mean:   8712.633","iter: 1506<br />train_rmse_mean:   8709.046","iter: 1507<br />train_rmse_mean:   8706.820","iter: 1508<br />train_rmse_mean:   8703.407","iter: 1509<br />train_rmse_mean:   8699.607","iter: 1510<br />train_rmse_mean:   8696.000","iter: 1511<br />train_rmse_mean:   8692.902","iter: 1512<br />train_rmse_mean:   8689.534","iter: 1513<br />train_rmse_mean:   8686.010","iter: 1514<br />train_rmse_mean:   8683.006","iter: 1515<br />train_rmse_mean:   8679.136","iter: 1516<br />train_rmse_mean:   8675.348","iter: 1517<br />train_rmse_mean:   8671.910","iter: 1518<br />train_rmse_mean:   8669.002","iter: 1519<br />train_rmse_mean:   8665.396","iter: 1520<br />train_rmse_mean:   8661.871","iter: 1521<br />train_rmse_mean:   8658.266","iter: 1522<br />train_rmse_mean:   8654.698","iter: 1523<br />train_rmse_mean:   8651.638","iter: 1524<br />train_rmse_mean:   8647.609","iter: 1525<br />train_rmse_mean:   8644.740","iter: 1526<br />train_rmse_mean:   8641.564","iter: 1527<br />train_rmse_mean:   8638.206","iter: 1528<br />train_rmse_mean:   8634.663","iter: 1529<br />train_rmse_mean:   8631.378","iter: 1530<br />train_rmse_mean:   8628.678","iter: 1531<br />train_rmse_mean:   8624.866","iter: 1532<br />train_rmse_mean:   8621.357","iter: 1533<br />train_rmse_mean:   8618.265","iter: 1534<br />train_rmse_mean:   8613.924","iter: 1535<br />train_rmse_mean:   8610.924","iter: 1536<br />train_rmse_mean:   8607.783","iter: 1537<br />train_rmse_mean:   8604.828","iter: 1538<br />train_rmse_mean:   8601.524","iter: 1539<br />train_rmse_mean:   8598.144","iter: 1540<br />train_rmse_mean:   8594.643","iter: 1541<br />train_rmse_mean:   8591.314","iter: 1542<br />train_rmse_mean:   8588.835","iter: 1543<br />train_rmse_mean:   8586.114","iter: 1544<br />train_rmse_mean:   8582.881","iter: 1545<br />train_rmse_mean:   8579.651","iter: 1546<br />train_rmse_mean:   8576.096","iter: 1547<br />train_rmse_mean:   8573.304","iter: 1548<br />train_rmse_mean:   8569.329","iter: 1549<br />train_rmse_mean:   8565.500","iter: 1550<br />train_rmse_mean:   8562.249","iter: 1551<br />train_rmse_mean:   8559.145","iter: 1552<br />train_rmse_mean:   8556.419","iter: 1553<br />train_rmse_mean:   8553.192","iter: 1554<br />train_rmse_mean:   8549.605","iter: 1555<br />train_rmse_mean:   8545.908","iter: 1556<br />train_rmse_mean:   8542.389","iter: 1557<br />train_rmse_mean:   8538.580","iter: 1558<br />train_rmse_mean:   8535.263","iter: 1559<br />train_rmse_mean:   8531.994","iter: 1560<br />train_rmse_mean:   8529.374","iter: 1561<br />train_rmse_mean:   8526.413","iter: 1562<br />train_rmse_mean:   8523.451","iter: 1563<br />train_rmse_mean:   8520.108","iter: 1564<br />train_rmse_mean:   8516.819","iter: 1565<br />train_rmse_mean:   8512.824","iter: 1566<br />train_rmse_mean:   8510.199","iter: 1567<br />train_rmse_mean:   8507.228","iter: 1568<br />train_rmse_mean:   8504.307","iter: 1569<br />train_rmse_mean:   8501.242","iter: 1570<br />train_rmse_mean:   8498.871","iter: 1571<br />train_rmse_mean:   8496.604","iter: 1572<br />train_rmse_mean:   8493.300","iter: 1573<br />train_rmse_mean:   8489.946","iter: 1574<br />train_rmse_mean:   8486.119","iter: 1575<br />train_rmse_mean:   8482.897","iter: 1576<br />train_rmse_mean:   8479.848","iter: 1577<br />train_rmse_mean:   8476.502","iter: 1578<br />train_rmse_mean:   8473.713","iter: 1579<br />train_rmse_mean:   8470.880","iter: 1580<br />train_rmse_mean:   8467.938","iter: 1581<br />train_rmse_mean:   8464.494","iter: 1582<br />train_rmse_mean:   8461.066","iter: 1583<br />train_rmse_mean:   8458.082","iter: 1584<br />train_rmse_mean:   8454.596","iter: 1585<br />train_rmse_mean:   8451.853","iter: 1586<br />train_rmse_mean:   8448.461","iter: 1587<br />train_rmse_mean:   8444.610","iter: 1588<br />train_rmse_mean:   8441.296","iter: 1589<br />train_rmse_mean:   8437.588","iter: 1590<br />train_rmse_mean:   8434.284","iter: 1591<br />train_rmse_mean:   8431.191","iter: 1592<br />train_rmse_mean:   8428.089","iter: 1593<br />train_rmse_mean:   8424.643","iter: 1594<br />train_rmse_mean:   8420.887","iter: 1595<br />train_rmse_mean:   8417.660","iter: 1596<br />train_rmse_mean:   8414.714","iter: 1597<br />train_rmse_mean:   8410.592","iter: 1598<br />train_rmse_mean:   8407.131","iter: 1599<br />train_rmse_mean:   8404.399","iter: 1600<br />train_rmse_mean:   8401.190","iter: 1601<br />train_rmse_mean:   8398.621","iter: 1602<br />train_rmse_mean:   8395.526","iter: 1603<br />train_rmse_mean:   8392.402","iter: 1604<br />train_rmse_mean:   8388.777","iter: 1605<br />train_rmse_mean:   8385.496","iter: 1606<br />train_rmse_mean:   8382.294","iter: 1607<br />train_rmse_mean:   8378.376","iter: 1608<br />train_rmse_mean:   8375.797","iter: 1609<br />train_rmse_mean:   8372.783","iter: 1610<br />train_rmse_mean:   8369.658","iter: 1611<br />train_rmse_mean:   8367.290","iter: 1612<br />train_rmse_mean:   8364.210","iter: 1613<br />train_rmse_mean:   8360.957","iter: 1614<br />train_rmse_mean:   8357.685","iter: 1615<br />train_rmse_mean:   8354.458","iter: 1616<br />train_rmse_mean:   8351.308","iter: 1617<br />train_rmse_mean:   8347.844","iter: 1618<br />train_rmse_mean:   8344.659","iter: 1619<br />train_rmse_mean:   8341.804","iter: 1620<br />train_rmse_mean:   8339.020","iter: 1621<br />train_rmse_mean:   8335.873","iter: 1622<br />train_rmse_mean:   8332.630","iter: 1623<br />train_rmse_mean:   8329.176","iter: 1624<br />train_rmse_mean:   8326.901","iter: 1625<br />train_rmse_mean:   8323.873","iter: 1626<br />train_rmse_mean:   8321.398","iter: 1627<br />train_rmse_mean:   8317.762","iter: 1628<br />train_rmse_mean:   8314.227","iter: 1629<br />train_rmse_mean:   8311.077","iter: 1630<br />train_rmse_mean:   8308.109","iter: 1631<br />train_rmse_mean:   8304.120","iter: 1632<br />train_rmse_mean:   8301.711","iter: 1633<br />train_rmse_mean:   8298.355","iter: 1634<br />train_rmse_mean:   8295.264","iter: 1635<br />train_rmse_mean:   8291.124","iter: 1636<br />train_rmse_mean:   8287.476","iter: 1637<br />train_rmse_mean:   8284.613","iter: 1638<br />train_rmse_mean:   8281.452","iter: 1639<br />train_rmse_mean:   8277.455","iter: 1640<br />train_rmse_mean:   8274.147","iter: 1641<br />train_rmse_mean:   8270.452"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641],"y":[190847.215625,185559.146875,180386.9875,175400.50625,170471.7421875,165762.28125,161202.621875,156784.0140625,152466.1359375,148307.821875,144225.315625,140358.5445313,136599.2007812,132892.4460937,129337.7398438,125905.1039063,122539.2937499,119275.5999999,116119.2718749,113081.1070313,110141.2570312,107298.7226564,104511.2343751,101864.3312499,99266.9398437,96734.7257814,94296.4179688,91914.1578125,89686.009375,87473.2968749,85318.7875001,83252.4429688,81240.3230468,79339.3832032,77424.7328124,75609.146875,73862.961328,72187.0027346,70600.3199218,69010.0390626,67463.2066407,65958.0472657,64508.846875,63102.3265624,61813.2906251,60473.6898437,59234.1328125,58045.2867187,56881.654297,55732.8535156,54679.0671873,53645.3296874,52688.192578,51747.5097656,50814.7328126,49893.3613283,49023.1660157,48220.3183593,47436.85,46713.1382814,45938.9410157,45215.5414063,44538.1765625,43841.5601563,43219.4042968,42613.9177734,42069.6923827,41519.5353516,40996.7949218,40488.5966797,40003.2542967,39514.3552736,39076.091797,38611.8566407,38171.1228515,37790.3937499,37397.203711,37027.0019532,36653.30625,36311.5726563,35915.3146484,35611.0875,35330.6816408,35058.233203,34803.6941408,34507.3499999,34239.4121094,33941.4873047,33713.8667969,33486.6648438,33290.924414,33082.5898436,32883.484961,32662.2447266,32448.4226562,32270.7582031,32107.6404298,31920.7183594,31756.2632812,31618.7892578,31472.1177733,31358.092578,31223.324414,31074.7232422,30953.4541017,30857.3843749,30745.6234375,30634.221289,30539.6808593,30450.4228516,30363.091211,30223.4115234,30113.3617189,30047.4921876,29973.6589844,29906.6572266,29808.1089846,29738.8507813,29675.8414062,29607.5423827,29513.5947265,29426.1636719,29355.3724609,29264.5427734,29190.8347658,29158.1601562,29078.8048829,29044.4148438,28998.759375,28932.7251952,28900.7687499,28813.1533204,28784.0048827,28699.1304687,28616.6611328,28588.9783203,28538.3388673,28491.3177734,28453.3839842,28405.674414,28391.2269532,28331.7371094,28281.4734376,28242.5433593,28195.7171874,28156.5460936,28102.356836,28064.3062501,28025.103711,27962.3583984,27919.3794921,27858.609375,27804.8800782,27761.0388672,27708.7960938,27685.9488282,27647.9861327,27611.2226563,27581.8210938,27541.4488281,27528.9343751,27510.2679688,27472.8740234,27446.3031252,27422.5599608,27375.0814451,27352.7326172,27340.2195313,27321.4248047,27310.8628908,27310.8779296,27289.1222657,27241.2406249,27229.9535155,27226.0740234,27208.9398437,27219.8132812,27213.7589842,27179.8833983,27182.1498047,27164.822461,27165.3154297,27148.6509765,27104.8474609,27092.897461,27076.3068359,27069.3167968,27045.1789062,27030.2099609,27017.399414,26987.5310548,26983.8583985,26982.4476563,26967.8955077,26958.5115234,26923.6363282,26921.3251953,26890.0419922,26869.3748047,26836.4431642,26807.2535155,26766.9726563,26735.3830075,26694.8853516,26670.4150389,26656.0261718,26619.6927734,26629.8193358,26622.4119138,26601.3089843,26566.8609374,26539.3902342,26533.6935545,26520.0837891,26501.9529296,26482.7300781,26485.5871093,26497.9964845,26491.6148436,26485.6093749,26460.0624999,26448.1617189,26448.7232423,26455.8298828,26443.2339843,26417.6654296,26391.2751953,26384.0824218,26365.4685547,26337.8478515,26329.1515625,26340.6070313,26303.1000002,26275.8101563,26255.0285157,26239.3082031,26214.4783202,26194.6605469,26163.8466795,26127.7472656,26113.0591797,26071.6146484,26069.6199218,26050.8658204,26043.6404297,25999.5953124,25997.9878906,25983.2296875,25967.5765625,25947.9158204,25935.0117188,25904.3230469,25887.0078125,25879.0568358,25883.4597656,25864.2865234,25850.0158202,25840.3085937,25822.1367188,25809.4542967,25783.9148437,25772.5742187,25767.6476564,25730.5941406,25710.3273437,25693.5902345,25679.0123048,25662.1783204,25661.4238282,25670.9242186,25659.4517578,25630.0205078,25612.533789,25602.9828125,25598.5802735,25583.3324219,25566.2498047,25552.0615234,25530.4611328,25531.4011719,25529.0365233,25506.4958983,25485.0845704,25458.5931641,25450.596875,25440.2355468,25413.7265624,25405.9306641,25391.2078124,25384.7726562,25364.5300782,25356.9037109,25333.1341797,25317.2980469,25310.2931641,25307.9107422,25295.963672,25277.3607421,25261.8048828,25257.9990233,25252.0408204,25250.0984374,25229.7201171,25216.2837891,25213.773047,25215.2861327,25210.7894532,25185.8093749,25178.9132812,25171.7476562,25167.9175781,25146.3208984,25132.2839843,25128.5974609,25116.2189452,25106.9236328,25097.2578124,25091.2996094,25094.6242188,25108.1564453,25099.9849608,25112.7527344,25105.9099609,25096.3238279,25069.6906248,25058.0345704,25034.25,25029.1955078,25023.1117189,25008.4039062,24994.7914064,24992.2568358,24977.8892578,24967.2736329,24969.4722656,24957.5691406,24959.9654297,24948.6435547,24938.2617187,24920.8710938,24913.4318359,24895.6501953,24890.0728515,24873.530078,24877.4080078,24879.4535156,24859.1361328,24840.4083984,24835.8046875,24823.6919921,24816.9119141,24804.9763671,24796.3478515,24779.5175781,24767.3681641,24764.9023436,24742.517578,24745.5351562,24752.045508,24747.215625,24737.0621094,24736.1667968,24727.1058592,24717.8726563,24722.9861327,24713.0169921,24712.3750001,24703.3804687,24701.0324219,24691.9431641,24681.758203,24684.9484376,24677.9394532,24671.6767578,24661.5017578,24659.280664,24645.0443359,24642.1417969,24636.3021484,24610.3392578,24606.0253905,24608.9021484,24585.0914062,24587.0433593,24583.5347656,24569.3925782,24563.5859376,24552.5185546,24549.8888672,24545.4322266,24535.28125,24521.9091798,24519.4410156,24500.5611329,24483.7976562,24479.8978516,24474.551953,24463.9080079,24445.8705079,24449.7544924,24448.3763671,24446.865039,24450.0060547,24435.8531251,24429.1234376,24416.1181641,24400.0195312,24398.5824217,24388.0953125,24387.8730468,24390.8517578,24378.1833986,24372.6843748,24366.980664,24366.2882814,24364.6773437,24369.032422,24372.5339843,24366.0138672,24370.0695313,24364.4599609,24359.5654297,24358.6808596,24361.7576171,24360.0400391,24349.9169922,24340.6892579,24339.4683594,24346.9296875,24337.8628905,24331.5324218,24316.6357422,24312.9185546,24298.4492187,24295.0773437,24286.1816407,24281.5724611,24281.3691408,24270.4556641,24267.2099607,24274.5628905,24269.6892579,24266.632617,24254.6953124,24253.1591795,24255.6095704,24239.8107422,24237.0062501,24233.2447266,24214.1121093,24219.9775391,24200.125586,24200.8208985,24213.5990234,24200.8119141,24190.1033202,24181.3470702,24171.3146483,24163.8251952,24163.373633,24162.3041016,24153.8406251,24145.8777342,24141.4173828,24120.4460938,24114.5746095,24095.366797,24088.647656,24095.3048826,24094.6310548,24084.2466797,24083.4066406,24069.5179688,24061.2041016,24059.4349609,24061.7048827,24050.4861327,24049.1076171,24045.2669922,24035.5634766,24030.763086,24029.0865235,24026.2859375,24016.9134766,24009.913086,24002.546875,23998.3636717,23998.7845702,23986.746875,23977.3949219,23979.5437501,23976.8742188,23967.0560546,23946.7246094,23952.9863281,23949.1533204,23944.8203123,23938.4921876,23938.789258,23931.9761719,23925.9623048,23924.7001953,23913.9417969,23902.9126953,23900.8806642,23900.7816407,23901.666211,23882.940625,23890.081836,23888.9810547,23889.4712891,23881.6617188,23878.2320312,23876.5550781,23871.1076171,23855.1382813,23857.8505859,23852.8884765,23833.8222655,23829.0847657,23817.7410156,23809.6238281,23796.567578,23782.9179688,23779.5734375,23781.9128907,23787.1507813,23784.7205078,23777.4773437,23777.1634766,23770.0853517,23766.309375,23763.1113281,23754.5642578,23744.3197266,23744.0234375,23743.2441408,23744.1869139,23739.6361326,23736.75625,23736.8158204,23724.59375,23729.1132812,23728.3205077,23722.8099608,23712.3722656,23703.0431642,23703.7361327,23703.1365235,23699.8822263,23696.373828,23688.2816407,23687.8974609,23679.8978515,23675.378711,23662.2982421,23665.6761717,23653.7294921,23649.2984375,23645.1546876,23639.4878907,23633.0083984,23627.4960936,23630.1628906,23614.4917967,23609.2183595,23607.5625,23607.0470703,23606.2494141,23602.66875,23606.0378904,23598.6935547,23596.3001953,23588.5015625,23590.5781248,23586.0085937,23590.900586,23589.299414,23589.2587891,23584.4294923,23572.848828,23568.5099609,23563.7216797,23553.4039063,23553.8224609,23555.7757814,23551.6794922,23548.8695312,23550.2457031,23551.44375,23544.4503907,23539.6382813,23535.3667969,23539.5792969,23534.8916015,23526.3847657,23527.7894532,23528.0986328,23528.0453127,23523.2185547,23527.7826172,23518.1835937,23512.5789064,23513.4351562,23512.0580078,23504.6039064,23493.1689454,23485.3683594,23479.3367188,23478.2554688,23469.7382812,23471.8021484,23469.7146484,23470.4833985,23465.3154298,23463.8525389,23463.1960939,23464.5873046,23469.6787109,23453.6757812,23459.204297,23455.4091795,23449.1943359,23452.8082031,23447.7103516,23438.81875,23433.7607421,23429.2249998,23439.1929686,23433.699414,23428.1189452,23419.4951171,23424.9759766,23418.4039063,23413.7666016,23407.5146484,23406.2951172,23403.8396486,23397.4570312,23387.8343749,23386.7847656,23391.3564454,23384.5333985,23376.3164063,23370.7250001,23364.6783203,23355.9906251,23346.4283201,23343.3531249,23337.0353517,23326.5810546,23327.5197266,23330.3841796,23324.7472655,23317.3126953,23315.81875,23317.0355469,23315.9720702,23313.7525391,23311.5716798,23307.3046877,23304.0035157,23297.5402345,23294.4708984,23284.7654299,23282.7689453,23284.891797,23283.9119139,23274.7851563,23271.1839845,23271.5447265,23272.4066406,23275.9451171,23268.7107421,23265.9228514,23257.8970703,23252.6519532,23251.616211,23240.9710938,23238.6544923,23236.0791016,23235.8521485,23234.4269531,23225.4003906,23222.317871,23223.436426,23221.7490235,23224.0064452,23224.4441405,23217.2669922,23216.2105469,23207.7546875,23208.4150391,23209.500293,23202.4057617,23200.7993163,23200.7451171,23202.3885742,23194.7545899,23194.253125,23189.8927733,23189.721875,23186.7660156,23187.0148438,23187.8914064,23187.9806642,23186.3688475,23183.3572265,23180.8712891,23171.5609374,23173.2000976,23170.0694335,23164.5625974,23163.9478518,23157.3530274,23151.0263671,23151.7602539,23149.7265627,23151.2376953,23149.6294921,23152.5705079,23148.3863281,23144.272168,23138.4711914,23135.1427735,23128.3541991,23127.1515624,23122.4240234,23122.5151368,23123.2225586,23116.154785,23114.884668,23108.6420899,23107.3965821,23106.0992187,23106.8511719,23103.5301758,23102.3992186,23102.3621095,23093.6835937,23085.2338866,23080.9791015,23086.0749024,23083.4127928,23075.5749023,23075.7851562,23075.4968749,23078.4799804,23081.0732422,23073.8860352,23068.1989257,23071.5510743,23072.245996,23062.8817383,23062.5866212,23056.7686524,23062.8538087,23068.0885743,23065.1391599,23067.4434569,23058.5169921,23055.6923827,23048.4070313,23050.1896486,23051.4442383,23046.8114257,23050.4205079,23046.3050782,23042.2416015,23044.559082,23043.0661132,23038.7904296,23039.23916,23035.3729492,23034.2279298,23039.3716796,23040.3862304,23035.8536133,23032.4193358,23029.2394531,23025.1970704,23026.1634766,23028.9672851,23025.6525391,23025.0591798,23027.386914,23023.0265625,23020.9050781,23019.8068359,23020.4154296,23020.3767579,23016.8734375,23018.8997071,23012.9249999,23011.5496093,23009.0828125,23009.6548828,23006.4869139,23008.3725585,23007.1585938,22998.703418,22996.9853516,22994.9922852,22996.2065431,22986.6885741,22982.6257814,22982.7836914,22976.4411133,22969.4532225,22967.1902344,22962.1585938,22958.7943358,22958.5980468,22954.8166995,22953.6216796,22952.0494139,22945.7352539,22947.2249022,22945.2123047,22946.8427735,22945.1860353,22945.8133788,22943.4095702,22943.3667971,22942.9198241,22941.9943359,22937.8183594,22933.8606447,22931.0143555,22934.0569336,22935.8206056,22936.4124999,22938.5847656,22931.1494141,22928.7899414,22932.2201173,22931.3195313,22928.1001954,22928.9261719,22925.2105469,22923.9392578,22920.3886719,22921.2830079,22917.0003906,22913.9112305,22910.351172,22903.8191407,22901.3526367,22897.4871094,22898.3835938,22895.5795899,22895.8329103,22894.4366211,22892.0542968,22892.1918945,22888.6054688,22887.8331054,22887.0492189,22882.2007812,22876.0833009,22875.0436522,22872.2472656,22870.4033204,22873.5774416,22869.9068359,22872.5785157,22871.3186523,22872.0239257,22870.7677735,22871.3265625,22860.8739258,22861.4488282,22857.5272461,22859.0869141,22859.8663086,22860.1515626,22867.2321289,22864.8269532,22860.1734375,22853.0113281,22847.4552734,22842.2047851,22842.7572266,22845.4625001,22845.9441406,22844.6723632,22843.405078,22843.9121092,22839.1699218,22838.2344726,22832.7518554,22830.9543946,22827.7097657,22831.0391602,22830.259668,22831.6042969,22831.7347658,22835.2595702,22833.6280275,22836.9612305,22839.2791992,22838.8609377,22830.7447264,22835.9274413,22824.9600584,22826.3091796,22821.4405274,22819.9157227,22815.8604494,22818.3807617,22814.7122071,22814.7974609,22820.1337892,22818.1014648,22814.8621095,22814.3399413,22809.0191407,22809.7872071,22812.7996094,22812.8083007,22810.904004,22810.7037111,22806.5988281,22805.6237304,22806.7754884,22803.7076172,22796.1047853,22789.5723633,22781.7606444,22783.5565429,22784.1038087,22781.8225586,22781.2282225,22782.4637695,22780.1973632,22777.2743163,22779.3538085,22777.9166992,22780.4398438,22779.8466796,22778.1875001,22780.5167969,22780.1550782,22777.675586,22775.7811522,22776.6853515,22773.5736328,22771.5280273,22773.3260743,22771.3507813,22773.1179686,22767.6692384,22762.6859375,22760.3259766,22760.830664,22758.8638672,22756.1346678,22754.922461,22755.6247071,22760.2858398,22760.5564454,22758.7647461,22756.9971679,22757.2506836,22754.1185545,22753.3714845,22751.5825195,22746.6217773,22743.8585936,22741.2821289,22743.2330079,22745.080957,22748.3977538,22748.0186524,22740.0382813,22736.6320314,22736.6720704,22733.7220703,22730.4634766,22728.3549805,22727.6424804,22728.2360352,22719.7648437,22718.866504,22717.0422852,22712.0273438,22709.4549805,22706.4537109,22710.0624022,22709.1587889,22707.6282225,22710.9493163,22709.8985351,22710.3337891,22704.0969725,22694.4551759,22693.4068359,22690.2544922,22686.0410157,22681.6657226,22678.7123047,22680.0909181,22682.6509765,22681.2970704,22682.2200195,22678.1083007,22678.1376954,22676.2987305,22671.3360352,22661.440039,22663.1681642,22659.0806641,22657.6385741,22648.7806639,22647.7905272,22644.9166992,22641.8958009,22640.5836914,22638.1982421,22636.8033202,22636.4608399,22634.695996,22628.4229493,22628.2986328,22624.7532226,22620.0923829,22614.4708985,22611.4527344,22612.687793,22611.7124999,22612.6204101,22611.8833009,22611.3993163,22614.9868163,22614.138672,22615.1955078,22615.190918,22610.9624023,22612.0078124,22606.6557618,22604.4168946,22595.2692381,22591.121582,22590.0114255,22592.4168944,22593.4794923,22591.6807619,22585.6715819,22584.4593749,22585.4610351,22582.5159179,22589.0358399,22586.8083009,22584.8155274,22586.7361328,22587.0833008,22588.1143554,22588.3225585,22581.0069337,22579.3098632,22578.3044922,22579.3720702,22576.6425783,22574.6797853,22574.4501954,22570.1680663,22572.9935547,22571.0011718,22572.9213867,22571.3433595,22567.1828125,22565.40625,22565.818164,22564.0101564,22564.6886718,22564.0477539,22562.6124023,22561.8631836,22563.9393554,22565.7234374,22565.0786131,22565.8415038,22564.9446289,22560.0228517,22561.6456055,22559.9766602,22562.0947266,22561.0450195,22557.7819336,22558.1198242,22553.409082,22552.2409181,22555.4388674,22560.0798828,22561.2655274,22560.5006836,22561.56709,22561.8551757,22559.839746,22556.3211914,22555.037793,22556.0798827,22554.1890625,22553.4509766,22550.8138672,22555.387793,22553.6968751,22554.8111328,22556.3110351,22559.3722658,22556.8301758,22556.5282226,22555.1838868,22557.3796874,22552.8874023,22548.6685548,22547.5445313,22541.0454101,22535.0174804,22536.4148437,22537.5833985,22531.0054687,22531.2686524,22530.2402343,22528.5148438,22532.9531251,22533.1230469,22533.6977539,22536.7860351,22532.2313475,22529.017871,22530.5914063,22532.1508789,22533.1213867,22530.8838867,22529.6408203,22530.5131835,22530.2014649,22528.4944337,22528.9355469,22526.8287108,22530.0639651,22528.962207,22529.4455079,22527.3309571,22526.5257813,22527.4495117,22529.6060546,22528.7444335,22528.8653321,22529.9365233,22525.5106446,22524.5285156,22525.1187499,22523.0445313,22518.8300782,22519.0539062,22522.1164063,22519.9753905,22512.0348633,22508.058496,22510.4489258,22511.0833007,22509.7197264,22509.6582032,22506.580078,22506.7748048,22506.7235353,22502.4399414,22502.1219726,22494.2125,22494.5234374,22494.161914,22494.3243164,22492.1092772,22490.2280272,22488.8476562,22488.9118165,22487.390918,22490.3462891,22486.7911133,22488.0632811,22488.4267578,22489.5420898,22486.8225587,22484.9194335,22481.8524414,22482.8270508,22481.3966796,22478.924414,22476.9666016,22473.1397459,22473.1247071,22471.4888672,22471.5439454,22469.2013672,22469.4801758,22471.2793945,22471.7932616,22473.1910155,22473.0475586,22471.333789,22473.4626953,22472.4896485,22468.4536133,22467.2494141,22467.4613282,22467.9600586,22468.899707,22464.4978516,22457.6469726,22456.0017578,22452.4375977,22454.9668944,22452.4798828,22448.3033203,22449.7420899,22449.4133789,22451.0761719,22454.9182617,22454.0451172,22448.7882813,22450.0887695,22443.6081055,22443.8350586,22445.1986328,22444.8376954,22443.8752929,22440.4955079,22440.5219726,22437.3347657,22438.0429688,22435.5061524,22435.8272462,22438.8791015,22436.6892578,22438.2320312,22440.1704101,22441.7499022,22439.6992188,22436.8129882,22435.2083008,22435.9218749,22436.3090819,22439.0924805,22436.0144531,22434.4458009,22429.1846681,22424.0654297,22421.0020508,22420.703125,22419.3822266,22420.2571289,22418.3527344,22417.9548829,22419.3187501,22423.5243164,22423.2808595,22422.2603515,22420.8193359,22420.1206055,22420.3112302,22420.8070313,22419.8304686,22419.74375,22422.4322266,22419.8184571,22419.7698242,22418.6483399,22415.3172851,22414.3750001,22410.3632813,22410.1948242,22408.7631836,22408.4298829,22407.290918,22406.3506837,22404.018457,22405.9596679,22403.9951172,22402.3895509,22399.7609375,22398.4123046,22397.1570314,22395.391211,22397.0483399,22394.5736328,22392.1683594,22391.519043,22388.8724608,22387.3292967,22388.7415039,22388.3577149,22388.6987304,22386.3729492,22389.8388671,22388.451367,22390.8539064,22389.763086,22387.7628906,22386.257129,22389.2221681,22390.6245117,22388.1990234,22389.6634765,22386.9806641,22384.7648438,22386.5015625,22386.1671874,22385.9900391,22381.2433593,22380.6175782,22379.6964844,22381.7175782,22377.1609373,22373.8930662,22370.6904296,22369.8486328,22369.0839841,22367.2823242,22364.6208007,22364.165918,22364.3663086,22362.523535,22362.1385741,22359.6836915,22356.3502931,22357.5527343,22358.1427735,22357.3673828,22357.5924804,22356.9527345,22356.5101564,22357.2317383,22358.0771485,22358.6989259,22358.8608397,22355.5373046,22352.9694337,22352.5143555,22350.5205077,22345.2091797,22341.5297852,22338.8817382,22340.105957,22336.6602538,22336.0714844,22335.215625,22335.4225585,22333.6808593,22332.5414063,22337.0446289,22336.1643555,22338.8467772,22338.619336,22337.5768555,22336.1468751,22334.3945313,22333.1418946,22335.4132812,22334.6694335,22334.3149414,22333.0583984,22332.8279297,22328.5927734,22330.1055664,22328.5925782,22325.753711,22323.2568358,22322.044043,22326.1723631,22327.1210937,22327.0534179,22327.9033201,22324.5060547,22323.7074219,22326.9416017,22328.0856445,22327.5297851,22327.5848631,22328.1662111,22325.1937501,22323.5738281,22323.7543944,22323.7595704,22322.5601562,22322.5563477,22322.9338867,22323.8410158,22320.4081055,22320.3417968,22322.9870117,22325.3543944,22326.3053709,22321.2632814,22324.1391602,22322.1881837,22317.1899413,22315.8010743,22316.5985351,22318.0264649,22315.0157226,22316.1127929,22314.4898437,22315.5384766,22315.4368164,22314.1394532,22312.0633788,22310.398535,22305.7958984,22302.9019531,22302.0644531,22303.0187501,22303.4175781,22302.8295898,22302.7391601,22302.1043943,22301.571875,22299.5454101,22300.1586916,22298.7961916,22297.8912111,22298.9600586,22296.3506837,22295.7410158,22290.922461,22291.6442383,22295.8821289,22292.9902343,22292.1085937,22295.3467774,22295.5500001,22293.5069335,22288.8618164,22288.4974609,22289.2075196,22287.246582,22284.5451173,22285.3817382,22286.6104491,22287.7860352,22286.253418,22284.936914,22282.5700195,22280.4357422,22277.5277343,22276.5759765,22275.5461913,22274.0793945,22273.4011718,22271.2756837,22271.0543945,22270.8072265,22270.6244142,22271.3944335,22269.2554686,22264.3124024,22264.6136718,22267.589453,22267.4019532,22264.1503905,22263.1635741,22261.3706056,22259.6211913,22259.3547852,22259.9998047,22262.4333009,22259.5925781,22258.6671875,22256.4417968,22254.5031249,22251.5374024,22250.6066405,22246.2316408,22246.1007813,22244.5541992,22244.802539,22245.7408203,22247.3023435,22248.0711913,22250.4496094,22251.4807618,22250.7800782,22251.4874999,22249.4366212,22246.6330078,22243.1778319,22245.5645508,22244.7305664,22244.0979492,22244.6189454,22243.805664,22241.7220704,22241.6159181,22239.2037108,22239.1565431,22239.9462891,22238.944336,22243.9946289,22245.9327149,22243.5607422,22246.8611327,22246.228418,22247.1743166,22246.6876952,22247.6014649,22247.5598632,22245.3583984,22245.3882813,22244.2811524,22240.9438477,22240.8728515,22236.892871,22238.5317383,22238.9193359,22235.012207,22234.5260742,22234.4913085,22230.6169921,22228.5266601,22227.7877931,22226.5842773,22225.7029297,22225.8005861,22224.6334962,22228.3630859,22228.3723634,22228.8837889,22225.6545897,22226.0052735,22226.3706054,22224.2074218,22223.2330079,22222.9848633,22225.1363283,22225.2572266,22225.8093749,22224.7809571,22221.4533204,22220.7384765,22217.8956055,22219.4201172,22219.7468747,22219.077051,22217.9033202,22218.1429686,22218.4675782,22218.7500003,22221.184668,22223.8676759,22222.3566407,22221.7907227,22221.4254884,22220.645508,22218.084961,22218.1476561,22218.5961914,22218.6244143,22218.6085938,22216.098535,22214.369336,22215.206836,22211.6114256,22210.9703123,22211.2232422,22209.5440431,22205.7238282,22208.7700195,22208.4300782,22209.259082,22206.0055664,22205.4207031,22203.6129882,22201.5825197,22202.4726563,22203.1658203,22202.3418946,22203.7179689,22201.9308594,22203.1261718,22203.0368164,22201.7928712,22201.5654295,22201.6079102,22198.4407228,22198.3220705,22198.1843749,22197.5671875,22196.3097656,22194.5524413,22194.1955078,22197.206836,22195.6778321,22197.1579101,22198.186914,22197.0189453,22195.9362306,22198.324414,22201.0970704,22199.8850586,22201.9930664,22200.6316406,22200.3035155,22201.769629,22203.2091796,22197.5874023,22199.4614256,22198.7418947,22199.1507813,22201.1543945,22202.2761718,22204.5836915,22206.2830078,22205.0523438,22204.3790038,22207.3527343,22207.015332,22208.6185548,22205.9251953,22205.465332,22204.0037109,22202.7115234,22202.3736327,22201.9831055,22198.0320313,22199.5138672,22197.8301758,22200.4575194,22201.7479492,22201.6814452,22201.8628907,22200.7171874,22200.4995117,22200.4375001,22197.9516603,22197.1963866,22197.1612304,22197.1427735,22197.2104493,22199.0086914,22199.3898438],"text":["iter:    1<br />test_rmse_mean: 190847.22","iter:    2<br />test_rmse_mean: 185559.15","iter:    3<br />test_rmse_mean: 180386.99","iter:    4<br />test_rmse_mean: 175400.51","iter:    5<br />test_rmse_mean: 170471.74","iter:    6<br />test_rmse_mean: 165762.28","iter:    7<br />test_rmse_mean: 161202.62","iter:    8<br />test_rmse_mean: 156784.01","iter:    9<br />test_rmse_mean: 152466.14","iter:   10<br />test_rmse_mean: 148307.82","iter:   11<br />test_rmse_mean: 144225.32","iter:   12<br />test_rmse_mean: 140358.54","iter:   13<br />test_rmse_mean: 136599.20","iter:   14<br />test_rmse_mean: 132892.45","iter:   15<br />test_rmse_mean: 129337.74","iter:   16<br />test_rmse_mean: 125905.10","iter:   17<br />test_rmse_mean: 122539.29","iter:   18<br />test_rmse_mean: 119275.60","iter:   19<br />test_rmse_mean: 116119.27","iter:   20<br />test_rmse_mean: 113081.11","iter:   21<br />test_rmse_mean: 110141.26","iter:   22<br />test_rmse_mean: 107298.72","iter:   23<br />test_rmse_mean: 104511.23","iter:   24<br />test_rmse_mean: 101864.33","iter:   25<br />test_rmse_mean:  99266.94","iter:   26<br />test_rmse_mean:  96734.73","iter:   27<br />test_rmse_mean:  94296.42","iter:   28<br />test_rmse_mean:  91914.16","iter:   29<br />test_rmse_mean:  89686.01","iter:   30<br />test_rmse_mean:  87473.30","iter:   31<br />test_rmse_mean:  85318.79","iter:   32<br />test_rmse_mean:  83252.44","iter:   33<br />test_rmse_mean:  81240.32","iter:   34<br />test_rmse_mean:  79339.38","iter:   35<br />test_rmse_mean:  77424.73","iter:   36<br />test_rmse_mean:  75609.15","iter:   37<br />test_rmse_mean:  73862.96","iter:   38<br />test_rmse_mean:  72187.00","iter:   39<br />test_rmse_mean:  70600.32","iter:   40<br />test_rmse_mean:  69010.04","iter:   41<br />test_rmse_mean:  67463.21","iter:   42<br />test_rmse_mean:  65958.05","iter:   43<br />test_rmse_mean:  64508.85","iter:   44<br />test_rmse_mean:  63102.33","iter:   45<br />test_rmse_mean:  61813.29","iter:   46<br />test_rmse_mean:  60473.69","iter:   47<br />test_rmse_mean:  59234.13","iter:   48<br />test_rmse_mean:  58045.29","iter:   49<br />test_rmse_mean:  56881.65","iter:   50<br />test_rmse_mean:  55732.85","iter:   51<br />test_rmse_mean:  54679.07","iter:   52<br />test_rmse_mean:  53645.33","iter:   53<br />test_rmse_mean:  52688.19","iter:   54<br />test_rmse_mean:  51747.51","iter:   55<br />test_rmse_mean:  50814.73","iter:   56<br />test_rmse_mean:  49893.36","iter:   57<br />test_rmse_mean:  49023.17","iter:   58<br />test_rmse_mean:  48220.32","iter:   59<br />test_rmse_mean:  47436.85","iter:   60<br />test_rmse_mean:  46713.14","iter:   61<br />test_rmse_mean:  45938.94","iter:   62<br />test_rmse_mean:  45215.54","iter:   63<br />test_rmse_mean:  44538.18","iter:   64<br />test_rmse_mean:  43841.56","iter:   65<br />test_rmse_mean:  43219.40","iter:   66<br />test_rmse_mean:  42613.92","iter:   67<br />test_rmse_mean:  42069.69","iter:   68<br />test_rmse_mean:  41519.54","iter:   69<br />test_rmse_mean:  40996.79","iter:   70<br />test_rmse_mean:  40488.60","iter:   71<br />test_rmse_mean:  40003.25","iter:   72<br />test_rmse_mean:  39514.36","iter:   73<br />test_rmse_mean:  39076.09","iter:   74<br />test_rmse_mean:  38611.86","iter:   75<br />test_rmse_mean:  38171.12","iter:   76<br />test_rmse_mean:  37790.39","iter:   77<br />test_rmse_mean:  37397.20","iter:   78<br />test_rmse_mean:  37027.00","iter:   79<br />test_rmse_mean:  36653.31","iter:   80<br />test_rmse_mean:  36311.57","iter:   81<br />test_rmse_mean:  35915.31","iter:   82<br />test_rmse_mean:  35611.09","iter:   83<br />test_rmse_mean:  35330.68","iter:   84<br />test_rmse_mean:  35058.23","iter:   85<br />test_rmse_mean:  34803.69","iter:   86<br />test_rmse_mean:  34507.35","iter:   87<br />test_rmse_mean:  34239.41","iter:   88<br />test_rmse_mean:  33941.49","iter:   89<br />test_rmse_mean:  33713.87","iter:   90<br />test_rmse_mean:  33486.66","iter:   91<br />test_rmse_mean:  33290.92","iter:   92<br />test_rmse_mean:  33082.59","iter:   93<br />test_rmse_mean:  32883.48","iter:   94<br />test_rmse_mean:  32662.24","iter:   95<br />test_rmse_mean:  32448.42","iter:   96<br />test_rmse_mean:  32270.76","iter:   97<br />test_rmse_mean:  32107.64","iter:   98<br />test_rmse_mean:  31920.72","iter:   99<br />test_rmse_mean:  31756.26","iter:  100<br />test_rmse_mean:  31618.79","iter:  101<br />test_rmse_mean:  31472.12","iter:  102<br />test_rmse_mean:  31358.09","iter:  103<br />test_rmse_mean:  31223.32","iter:  104<br />test_rmse_mean:  31074.72","iter:  105<br />test_rmse_mean:  30953.45","iter:  106<br />test_rmse_mean:  30857.38","iter:  107<br />test_rmse_mean:  30745.62","iter:  108<br />test_rmse_mean:  30634.22","iter:  109<br />test_rmse_mean:  30539.68","iter:  110<br />test_rmse_mean:  30450.42","iter:  111<br />test_rmse_mean:  30363.09","iter:  112<br />test_rmse_mean:  30223.41","iter:  113<br />test_rmse_mean:  30113.36","iter:  114<br />test_rmse_mean:  30047.49","iter:  115<br />test_rmse_mean:  29973.66","iter:  116<br />test_rmse_mean:  29906.66","iter:  117<br />test_rmse_mean:  29808.11","iter:  118<br />test_rmse_mean:  29738.85","iter:  119<br />test_rmse_mean:  29675.84","iter:  120<br />test_rmse_mean:  29607.54","iter:  121<br />test_rmse_mean:  29513.59","iter:  122<br />test_rmse_mean:  29426.16","iter:  123<br />test_rmse_mean:  29355.37","iter:  124<br />test_rmse_mean:  29264.54","iter:  125<br />test_rmse_mean:  29190.83","iter:  126<br />test_rmse_mean:  29158.16","iter:  127<br />test_rmse_mean:  29078.80","iter:  128<br />test_rmse_mean:  29044.41","iter:  129<br />test_rmse_mean:  28998.76","iter:  130<br />test_rmse_mean:  28932.73","iter:  131<br />test_rmse_mean:  28900.77","iter:  132<br />test_rmse_mean:  28813.15","iter:  133<br />test_rmse_mean:  28784.00","iter:  134<br />test_rmse_mean:  28699.13","iter:  135<br />test_rmse_mean:  28616.66","iter:  136<br />test_rmse_mean:  28588.98","iter:  137<br />test_rmse_mean:  28538.34","iter:  138<br />test_rmse_mean:  28491.32","iter:  139<br />test_rmse_mean:  28453.38","iter:  140<br />test_rmse_mean:  28405.67","iter:  141<br />test_rmse_mean:  28391.23","iter:  142<br />test_rmse_mean:  28331.74","iter:  143<br />test_rmse_mean:  28281.47","iter:  144<br />test_rmse_mean:  28242.54","iter:  145<br />test_rmse_mean:  28195.72","iter:  146<br />test_rmse_mean:  28156.55","iter:  147<br />test_rmse_mean:  28102.36","iter:  148<br />test_rmse_mean:  28064.31","iter:  149<br />test_rmse_mean:  28025.10","iter:  150<br />test_rmse_mean:  27962.36","iter:  151<br />test_rmse_mean:  27919.38","iter:  152<br />test_rmse_mean:  27858.61","iter:  153<br />test_rmse_mean:  27804.88","iter:  154<br />test_rmse_mean:  27761.04","iter:  155<br />test_rmse_mean:  27708.80","iter:  156<br />test_rmse_mean:  27685.95","iter:  157<br />test_rmse_mean:  27647.99","iter:  158<br />test_rmse_mean:  27611.22","iter:  159<br />test_rmse_mean:  27581.82","iter:  160<br />test_rmse_mean:  27541.45","iter:  161<br />test_rmse_mean:  27528.93","iter:  162<br />test_rmse_mean:  27510.27","iter:  163<br />test_rmse_mean:  27472.87","iter:  164<br />test_rmse_mean:  27446.30","iter:  165<br />test_rmse_mean:  27422.56","iter:  166<br />test_rmse_mean:  27375.08","iter:  167<br />test_rmse_mean:  27352.73","iter:  168<br />test_rmse_mean:  27340.22","iter:  169<br />test_rmse_mean:  27321.42","iter:  170<br />test_rmse_mean:  27310.86","iter:  171<br />test_rmse_mean:  27310.88","iter:  172<br />test_rmse_mean:  27289.12","iter:  173<br />test_rmse_mean:  27241.24","iter:  174<br />test_rmse_mean:  27229.95","iter:  175<br />test_rmse_mean:  27226.07","iter:  176<br />test_rmse_mean:  27208.94","iter:  177<br />test_rmse_mean:  27219.81","iter:  178<br />test_rmse_mean:  27213.76","iter:  179<br />test_rmse_mean:  27179.88","iter:  180<br />test_rmse_mean:  27182.15","iter:  181<br />test_rmse_mean:  27164.82","iter:  182<br />test_rmse_mean:  27165.32","iter:  183<br />test_rmse_mean:  27148.65","iter:  184<br />test_rmse_mean:  27104.85","iter:  185<br />test_rmse_mean:  27092.90","iter:  186<br />test_rmse_mean:  27076.31","iter:  187<br />test_rmse_mean:  27069.32","iter:  188<br />test_rmse_mean:  27045.18","iter:  189<br />test_rmse_mean:  27030.21","iter:  190<br />test_rmse_mean:  27017.40","iter:  191<br />test_rmse_mean:  26987.53","iter:  192<br />test_rmse_mean:  26983.86","iter:  193<br />test_rmse_mean:  26982.45","iter:  194<br />test_rmse_mean:  26967.90","iter:  195<br />test_rmse_mean:  26958.51","iter:  196<br />test_rmse_mean:  26923.64","iter:  197<br />test_rmse_mean:  26921.33","iter:  198<br />test_rmse_mean:  26890.04","iter:  199<br />test_rmse_mean:  26869.37","iter:  200<br />test_rmse_mean:  26836.44","iter:  201<br />test_rmse_mean:  26807.25","iter:  202<br />test_rmse_mean:  26766.97","iter:  203<br />test_rmse_mean:  26735.38","iter:  204<br />test_rmse_mean:  26694.89","iter:  205<br />test_rmse_mean:  26670.42","iter:  206<br />test_rmse_mean:  26656.03","iter:  207<br />test_rmse_mean:  26619.69","iter:  208<br />test_rmse_mean:  26629.82","iter:  209<br />test_rmse_mean:  26622.41","iter:  210<br />test_rmse_mean:  26601.31","iter:  211<br />test_rmse_mean:  26566.86","iter:  212<br />test_rmse_mean:  26539.39","iter:  213<br />test_rmse_mean:  26533.69","iter:  214<br />test_rmse_mean:  26520.08","iter:  215<br />test_rmse_mean:  26501.95","iter:  216<br />test_rmse_mean:  26482.73","iter:  217<br />test_rmse_mean:  26485.59","iter:  218<br />test_rmse_mean:  26498.00","iter:  219<br />test_rmse_mean:  26491.61","iter:  220<br />test_rmse_mean:  26485.61","iter:  221<br />test_rmse_mean:  26460.06","iter:  222<br />test_rmse_mean:  26448.16","iter:  223<br />test_rmse_mean:  26448.72","iter:  224<br />test_rmse_mean:  26455.83","iter:  225<br />test_rmse_mean:  26443.23","iter:  226<br />test_rmse_mean:  26417.67","iter:  227<br />test_rmse_mean:  26391.28","iter:  228<br />test_rmse_mean:  26384.08","iter:  229<br />test_rmse_mean:  26365.47","iter:  230<br />test_rmse_mean:  26337.85","iter:  231<br />test_rmse_mean:  26329.15","iter:  232<br />test_rmse_mean:  26340.61","iter:  233<br />test_rmse_mean:  26303.10","iter:  234<br />test_rmse_mean:  26275.81","iter:  235<br />test_rmse_mean:  26255.03","iter:  236<br />test_rmse_mean:  26239.31","iter:  237<br />test_rmse_mean:  26214.48","iter:  238<br />test_rmse_mean:  26194.66","iter:  239<br />test_rmse_mean:  26163.85","iter:  240<br />test_rmse_mean:  26127.75","iter:  241<br />test_rmse_mean:  26113.06","iter:  242<br />test_rmse_mean:  26071.61","iter:  243<br />test_rmse_mean:  26069.62","iter:  244<br />test_rmse_mean:  26050.87","iter:  245<br />test_rmse_mean:  26043.64","iter:  246<br />test_rmse_mean:  25999.60","iter:  247<br />test_rmse_mean:  25997.99","iter:  248<br />test_rmse_mean:  25983.23","iter:  249<br />test_rmse_mean:  25967.58","iter:  250<br />test_rmse_mean:  25947.92","iter:  251<br />test_rmse_mean:  25935.01","iter:  252<br />test_rmse_mean:  25904.32","iter:  253<br />test_rmse_mean:  25887.01","iter:  254<br />test_rmse_mean:  25879.06","iter:  255<br />test_rmse_mean:  25883.46","iter:  256<br />test_rmse_mean:  25864.29","iter:  257<br />test_rmse_mean:  25850.02","iter:  258<br />test_rmse_mean:  25840.31","iter:  259<br />test_rmse_mean:  25822.14","iter:  260<br />test_rmse_mean:  25809.45","iter:  261<br />test_rmse_mean:  25783.91","iter:  262<br />test_rmse_mean:  25772.57","iter:  263<br />test_rmse_mean:  25767.65","iter:  264<br />test_rmse_mean:  25730.59","iter:  265<br />test_rmse_mean:  25710.33","iter:  266<br />test_rmse_mean:  25693.59","iter:  267<br />test_rmse_mean:  25679.01","iter:  268<br />test_rmse_mean:  25662.18","iter:  269<br />test_rmse_mean:  25661.42","iter:  270<br />test_rmse_mean:  25670.92","iter:  271<br />test_rmse_mean:  25659.45","iter:  272<br />test_rmse_mean:  25630.02","iter:  273<br />test_rmse_mean:  25612.53","iter:  274<br />test_rmse_mean:  25602.98","iter:  275<br />test_rmse_mean:  25598.58","iter:  276<br />test_rmse_mean:  25583.33","iter:  277<br />test_rmse_mean:  25566.25","iter:  278<br />test_rmse_mean:  25552.06","iter:  279<br />test_rmse_mean:  25530.46","iter:  280<br />test_rmse_mean:  25531.40","iter:  281<br />test_rmse_mean:  25529.04","iter:  282<br />test_rmse_mean:  25506.50","iter:  283<br />test_rmse_mean:  25485.08","iter:  284<br />test_rmse_mean:  25458.59","iter:  285<br />test_rmse_mean:  25450.60","iter:  286<br />test_rmse_mean:  25440.24","iter:  287<br />test_rmse_mean:  25413.73","iter:  288<br />test_rmse_mean:  25405.93","iter:  289<br />test_rmse_mean:  25391.21","iter:  290<br />test_rmse_mean:  25384.77","iter:  291<br />test_rmse_mean:  25364.53","iter:  292<br />test_rmse_mean:  25356.90","iter:  293<br />test_rmse_mean:  25333.13","iter:  294<br />test_rmse_mean:  25317.30","iter:  295<br />test_rmse_mean:  25310.29","iter:  296<br />test_rmse_mean:  25307.91","iter:  297<br />test_rmse_mean:  25295.96","iter:  298<br />test_rmse_mean:  25277.36","iter:  299<br />test_rmse_mean:  25261.80","iter:  300<br />test_rmse_mean:  25258.00","iter:  301<br />test_rmse_mean:  25252.04","iter:  302<br />test_rmse_mean:  25250.10","iter:  303<br />test_rmse_mean:  25229.72","iter:  304<br />test_rmse_mean:  25216.28","iter:  305<br />test_rmse_mean:  25213.77","iter:  306<br />test_rmse_mean:  25215.29","iter:  307<br />test_rmse_mean:  25210.79","iter:  308<br />test_rmse_mean:  25185.81","iter:  309<br />test_rmse_mean:  25178.91","iter:  310<br />test_rmse_mean:  25171.75","iter:  311<br />test_rmse_mean:  25167.92","iter:  312<br />test_rmse_mean:  25146.32","iter:  313<br />test_rmse_mean:  25132.28","iter:  314<br />test_rmse_mean:  25128.60","iter:  315<br />test_rmse_mean:  25116.22","iter:  316<br />test_rmse_mean:  25106.92","iter:  317<br />test_rmse_mean:  25097.26","iter:  318<br />test_rmse_mean:  25091.30","iter:  319<br />test_rmse_mean:  25094.62","iter:  320<br />test_rmse_mean:  25108.16","iter:  321<br />test_rmse_mean:  25099.98","iter:  322<br />test_rmse_mean:  25112.75","iter:  323<br />test_rmse_mean:  25105.91","iter:  324<br />test_rmse_mean:  25096.32","iter:  325<br />test_rmse_mean:  25069.69","iter:  326<br />test_rmse_mean:  25058.03","iter:  327<br />test_rmse_mean:  25034.25","iter:  328<br />test_rmse_mean:  25029.20","iter:  329<br />test_rmse_mean:  25023.11","iter:  330<br />test_rmse_mean:  25008.40","iter:  331<br />test_rmse_mean:  24994.79","iter:  332<br />test_rmse_mean:  24992.26","iter:  333<br />test_rmse_mean:  24977.89","iter:  334<br />test_rmse_mean:  24967.27","iter:  335<br />test_rmse_mean:  24969.47","iter:  336<br />test_rmse_mean:  24957.57","iter:  337<br />test_rmse_mean:  24959.97","iter:  338<br />test_rmse_mean:  24948.64","iter:  339<br />test_rmse_mean:  24938.26","iter:  340<br />test_rmse_mean:  24920.87","iter:  341<br />test_rmse_mean:  24913.43","iter:  342<br />test_rmse_mean:  24895.65","iter:  343<br />test_rmse_mean:  24890.07","iter:  344<br />test_rmse_mean:  24873.53","iter:  345<br />test_rmse_mean:  24877.41","iter:  346<br />test_rmse_mean:  24879.45","iter:  347<br />test_rmse_mean:  24859.14","iter:  348<br />test_rmse_mean:  24840.41","iter:  349<br />test_rmse_mean:  24835.80","iter:  350<br />test_rmse_mean:  24823.69","iter:  351<br />test_rmse_mean:  24816.91","iter:  352<br />test_rmse_mean:  24804.98","iter:  353<br />test_rmse_mean:  24796.35","iter:  354<br />test_rmse_mean:  24779.52","iter:  355<br />test_rmse_mean:  24767.37","iter:  356<br />test_rmse_mean:  24764.90","iter:  357<br />test_rmse_mean:  24742.52","iter:  358<br />test_rmse_mean:  24745.54","iter:  359<br />test_rmse_mean:  24752.05","iter:  360<br />test_rmse_mean:  24747.22","iter:  361<br />test_rmse_mean:  24737.06","iter:  362<br />test_rmse_mean:  24736.17","iter:  363<br />test_rmse_mean:  24727.11","iter:  364<br />test_rmse_mean:  24717.87","iter:  365<br />test_rmse_mean:  24722.99","iter:  366<br />test_rmse_mean:  24713.02","iter:  367<br />test_rmse_mean:  24712.38","iter:  368<br />test_rmse_mean:  24703.38","iter:  369<br />test_rmse_mean:  24701.03","iter:  370<br />test_rmse_mean:  24691.94","iter:  371<br />test_rmse_mean:  24681.76","iter:  372<br />test_rmse_mean:  24684.95","iter:  373<br />test_rmse_mean:  24677.94","iter:  374<br />test_rmse_mean:  24671.68","iter:  375<br />test_rmse_mean:  24661.50","iter:  376<br />test_rmse_mean:  24659.28","iter:  377<br />test_rmse_mean:  24645.04","iter:  378<br />test_rmse_mean:  24642.14","iter:  379<br />test_rmse_mean:  24636.30","iter:  380<br />test_rmse_mean:  24610.34","iter:  381<br />test_rmse_mean:  24606.03","iter:  382<br />test_rmse_mean:  24608.90","iter:  383<br />test_rmse_mean:  24585.09","iter:  384<br />test_rmse_mean:  24587.04","iter:  385<br />test_rmse_mean:  24583.53","iter:  386<br />test_rmse_mean:  24569.39","iter:  387<br />test_rmse_mean:  24563.59","iter:  388<br />test_rmse_mean:  24552.52","iter:  389<br />test_rmse_mean:  24549.89","iter:  390<br />test_rmse_mean:  24545.43","iter:  391<br />test_rmse_mean:  24535.28","iter:  392<br />test_rmse_mean:  24521.91","iter:  393<br />test_rmse_mean:  24519.44","iter:  394<br />test_rmse_mean:  24500.56","iter:  395<br />test_rmse_mean:  24483.80","iter:  396<br />test_rmse_mean:  24479.90","iter:  397<br />test_rmse_mean:  24474.55","iter:  398<br />test_rmse_mean:  24463.91","iter:  399<br />test_rmse_mean:  24445.87","iter:  400<br />test_rmse_mean:  24449.75","iter:  401<br />test_rmse_mean:  24448.38","iter:  402<br />test_rmse_mean:  24446.87","iter:  403<br />test_rmse_mean:  24450.01","iter:  404<br />test_rmse_mean:  24435.85","iter:  405<br />test_rmse_mean:  24429.12","iter:  406<br />test_rmse_mean:  24416.12","iter:  407<br />test_rmse_mean:  24400.02","iter:  408<br />test_rmse_mean:  24398.58","iter:  409<br />test_rmse_mean:  24388.10","iter:  410<br />test_rmse_mean:  24387.87","iter:  411<br />test_rmse_mean:  24390.85","iter:  412<br />test_rmse_mean:  24378.18","iter:  413<br />test_rmse_mean:  24372.68","iter:  414<br />test_rmse_mean:  24366.98","iter:  415<br />test_rmse_mean:  24366.29","iter:  416<br />test_rmse_mean:  24364.68","iter:  417<br />test_rmse_mean:  24369.03","iter:  418<br />test_rmse_mean:  24372.53","iter:  419<br />test_rmse_mean:  24366.01","iter:  420<br />test_rmse_mean:  24370.07","iter:  421<br />test_rmse_mean:  24364.46","iter:  422<br />test_rmse_mean:  24359.57","iter:  423<br />test_rmse_mean:  24358.68","iter:  424<br />test_rmse_mean:  24361.76","iter:  425<br />test_rmse_mean:  24360.04","iter:  426<br />test_rmse_mean:  24349.92","iter:  427<br />test_rmse_mean:  24340.69","iter:  428<br />test_rmse_mean:  24339.47","iter:  429<br />test_rmse_mean:  24346.93","iter:  430<br />test_rmse_mean:  24337.86","iter:  431<br />test_rmse_mean:  24331.53","iter:  432<br />test_rmse_mean:  24316.64","iter:  433<br />test_rmse_mean:  24312.92","iter:  434<br />test_rmse_mean:  24298.45","iter:  435<br />test_rmse_mean:  24295.08","iter:  436<br />test_rmse_mean:  24286.18","iter:  437<br />test_rmse_mean:  24281.57","iter:  438<br />test_rmse_mean:  24281.37","iter:  439<br />test_rmse_mean:  24270.46","iter:  440<br />test_rmse_mean:  24267.21","iter:  441<br />test_rmse_mean:  24274.56","iter:  442<br />test_rmse_mean:  24269.69","iter:  443<br />test_rmse_mean:  24266.63","iter:  444<br />test_rmse_mean:  24254.70","iter:  445<br />test_rmse_mean:  24253.16","iter:  446<br />test_rmse_mean:  24255.61","iter:  447<br />test_rmse_mean:  24239.81","iter:  448<br />test_rmse_mean:  24237.01","iter:  449<br />test_rmse_mean:  24233.24","iter:  450<br />test_rmse_mean:  24214.11","iter:  451<br />test_rmse_mean:  24219.98","iter:  452<br />test_rmse_mean:  24200.13","iter:  453<br />test_rmse_mean:  24200.82","iter:  454<br />test_rmse_mean:  24213.60","iter:  455<br />test_rmse_mean:  24200.81","iter:  456<br />test_rmse_mean:  24190.10","iter:  457<br />test_rmse_mean:  24181.35","iter:  458<br />test_rmse_mean:  24171.31","iter:  459<br />test_rmse_mean:  24163.83","iter:  460<br />test_rmse_mean:  24163.37","iter:  461<br />test_rmse_mean:  24162.30","iter:  462<br />test_rmse_mean:  24153.84","iter:  463<br />test_rmse_mean:  24145.88","iter:  464<br />test_rmse_mean:  24141.42","iter:  465<br />test_rmse_mean:  24120.45","iter:  466<br />test_rmse_mean:  24114.57","iter:  467<br />test_rmse_mean:  24095.37","iter:  468<br />test_rmse_mean:  24088.65","iter:  469<br />test_rmse_mean:  24095.30","iter:  470<br />test_rmse_mean:  24094.63","iter:  471<br />test_rmse_mean:  24084.25","iter:  472<br />test_rmse_mean:  24083.41","iter:  473<br />test_rmse_mean:  24069.52","iter:  474<br />test_rmse_mean:  24061.20","iter:  475<br />test_rmse_mean:  24059.43","iter:  476<br />test_rmse_mean:  24061.70","iter:  477<br />test_rmse_mean:  24050.49","iter:  478<br />test_rmse_mean:  24049.11","iter:  479<br />test_rmse_mean:  24045.27","iter:  480<br />test_rmse_mean:  24035.56","iter:  481<br />test_rmse_mean:  24030.76","iter:  482<br />test_rmse_mean:  24029.09","iter:  483<br />test_rmse_mean:  24026.29","iter:  484<br />test_rmse_mean:  24016.91","iter:  485<br />test_rmse_mean:  24009.91","iter:  486<br />test_rmse_mean:  24002.55","iter:  487<br />test_rmse_mean:  23998.36","iter:  488<br />test_rmse_mean:  23998.78","iter:  489<br />test_rmse_mean:  23986.75","iter:  490<br />test_rmse_mean:  23977.39","iter:  491<br />test_rmse_mean:  23979.54","iter:  492<br />test_rmse_mean:  23976.87","iter:  493<br />test_rmse_mean:  23967.06","iter:  494<br />test_rmse_mean:  23946.72","iter:  495<br />test_rmse_mean:  23952.99","iter:  496<br />test_rmse_mean:  23949.15","iter:  497<br />test_rmse_mean:  23944.82","iter:  498<br />test_rmse_mean:  23938.49","iter:  499<br />test_rmse_mean:  23938.79","iter:  500<br />test_rmse_mean:  23931.98","iter:  501<br />test_rmse_mean:  23925.96","iter:  502<br />test_rmse_mean:  23924.70","iter:  503<br />test_rmse_mean:  23913.94","iter:  504<br />test_rmse_mean:  23902.91","iter:  505<br />test_rmse_mean:  23900.88","iter:  506<br />test_rmse_mean:  23900.78","iter:  507<br />test_rmse_mean:  23901.67","iter:  508<br />test_rmse_mean:  23882.94","iter:  509<br />test_rmse_mean:  23890.08","iter:  510<br />test_rmse_mean:  23888.98","iter:  511<br />test_rmse_mean:  23889.47","iter:  512<br />test_rmse_mean:  23881.66","iter:  513<br />test_rmse_mean:  23878.23","iter:  514<br />test_rmse_mean:  23876.56","iter:  515<br />test_rmse_mean:  23871.11","iter:  516<br />test_rmse_mean:  23855.14","iter:  517<br />test_rmse_mean:  23857.85","iter:  518<br />test_rmse_mean:  23852.89","iter:  519<br />test_rmse_mean:  23833.82","iter:  520<br />test_rmse_mean:  23829.08","iter:  521<br />test_rmse_mean:  23817.74","iter:  522<br />test_rmse_mean:  23809.62","iter:  523<br />test_rmse_mean:  23796.57","iter:  524<br />test_rmse_mean:  23782.92","iter:  525<br />test_rmse_mean:  23779.57","iter:  526<br />test_rmse_mean:  23781.91","iter:  527<br />test_rmse_mean:  23787.15","iter:  528<br />test_rmse_mean:  23784.72","iter:  529<br />test_rmse_mean:  23777.48","iter:  530<br />test_rmse_mean:  23777.16","iter:  531<br />test_rmse_mean:  23770.09","iter:  532<br />test_rmse_mean:  23766.31","iter:  533<br />test_rmse_mean:  23763.11","iter:  534<br />test_rmse_mean:  23754.56","iter:  535<br />test_rmse_mean:  23744.32","iter:  536<br />test_rmse_mean:  23744.02","iter:  537<br />test_rmse_mean:  23743.24","iter:  538<br />test_rmse_mean:  23744.19","iter:  539<br />test_rmse_mean:  23739.64","iter:  540<br />test_rmse_mean:  23736.76","iter:  541<br />test_rmse_mean:  23736.82","iter:  542<br />test_rmse_mean:  23724.59","iter:  543<br />test_rmse_mean:  23729.11","iter:  544<br />test_rmse_mean:  23728.32","iter:  545<br />test_rmse_mean:  23722.81","iter:  546<br />test_rmse_mean:  23712.37","iter:  547<br />test_rmse_mean:  23703.04","iter:  548<br />test_rmse_mean:  23703.74","iter:  549<br />test_rmse_mean:  23703.14","iter:  550<br />test_rmse_mean:  23699.88","iter:  551<br />test_rmse_mean:  23696.37","iter:  552<br />test_rmse_mean:  23688.28","iter:  553<br />test_rmse_mean:  23687.90","iter:  554<br />test_rmse_mean:  23679.90","iter:  555<br />test_rmse_mean:  23675.38","iter:  556<br />test_rmse_mean:  23662.30","iter:  557<br />test_rmse_mean:  23665.68","iter:  558<br />test_rmse_mean:  23653.73","iter:  559<br />test_rmse_mean:  23649.30","iter:  560<br />test_rmse_mean:  23645.15","iter:  561<br />test_rmse_mean:  23639.49","iter:  562<br />test_rmse_mean:  23633.01","iter:  563<br />test_rmse_mean:  23627.50","iter:  564<br />test_rmse_mean:  23630.16","iter:  565<br />test_rmse_mean:  23614.49","iter:  566<br />test_rmse_mean:  23609.22","iter:  567<br />test_rmse_mean:  23607.56","iter:  568<br />test_rmse_mean:  23607.05","iter:  569<br />test_rmse_mean:  23606.25","iter:  570<br />test_rmse_mean:  23602.67","iter:  571<br />test_rmse_mean:  23606.04","iter:  572<br />test_rmse_mean:  23598.69","iter:  573<br />test_rmse_mean:  23596.30","iter:  574<br />test_rmse_mean:  23588.50","iter:  575<br />test_rmse_mean:  23590.58","iter:  576<br />test_rmse_mean:  23586.01","iter:  577<br />test_rmse_mean:  23590.90","iter:  578<br />test_rmse_mean:  23589.30","iter:  579<br />test_rmse_mean:  23589.26","iter:  580<br />test_rmse_mean:  23584.43","iter:  581<br />test_rmse_mean:  23572.85","iter:  582<br />test_rmse_mean:  23568.51","iter:  583<br />test_rmse_mean:  23563.72","iter:  584<br />test_rmse_mean:  23553.40","iter:  585<br />test_rmse_mean:  23553.82","iter:  586<br />test_rmse_mean:  23555.78","iter:  587<br />test_rmse_mean:  23551.68","iter:  588<br />test_rmse_mean:  23548.87","iter:  589<br />test_rmse_mean:  23550.25","iter:  590<br />test_rmse_mean:  23551.44","iter:  591<br />test_rmse_mean:  23544.45","iter:  592<br />test_rmse_mean:  23539.64","iter:  593<br />test_rmse_mean:  23535.37","iter:  594<br />test_rmse_mean:  23539.58","iter:  595<br />test_rmse_mean:  23534.89","iter:  596<br />test_rmse_mean:  23526.38","iter:  597<br />test_rmse_mean:  23527.79","iter:  598<br />test_rmse_mean:  23528.10","iter:  599<br />test_rmse_mean:  23528.05","iter:  600<br />test_rmse_mean:  23523.22","iter:  601<br />test_rmse_mean:  23527.78","iter:  602<br />test_rmse_mean:  23518.18","iter:  603<br />test_rmse_mean:  23512.58","iter:  604<br />test_rmse_mean:  23513.44","iter:  605<br />test_rmse_mean:  23512.06","iter:  606<br />test_rmse_mean:  23504.60","iter:  607<br />test_rmse_mean:  23493.17","iter:  608<br />test_rmse_mean:  23485.37","iter:  609<br />test_rmse_mean:  23479.34","iter:  610<br />test_rmse_mean:  23478.26","iter:  611<br />test_rmse_mean:  23469.74","iter:  612<br />test_rmse_mean:  23471.80","iter:  613<br />test_rmse_mean:  23469.71","iter:  614<br />test_rmse_mean:  23470.48","iter:  615<br />test_rmse_mean:  23465.32","iter:  616<br />test_rmse_mean:  23463.85","iter:  617<br />test_rmse_mean:  23463.20","iter:  618<br />test_rmse_mean:  23464.59","iter:  619<br />test_rmse_mean:  23469.68","iter:  620<br />test_rmse_mean:  23453.68","iter:  621<br />test_rmse_mean:  23459.20","iter:  622<br />test_rmse_mean:  23455.41","iter:  623<br />test_rmse_mean:  23449.19","iter:  624<br />test_rmse_mean:  23452.81","iter:  625<br />test_rmse_mean:  23447.71","iter:  626<br />test_rmse_mean:  23438.82","iter:  627<br />test_rmse_mean:  23433.76","iter:  628<br />test_rmse_mean:  23429.22","iter:  629<br />test_rmse_mean:  23439.19","iter:  630<br />test_rmse_mean:  23433.70","iter:  631<br />test_rmse_mean:  23428.12","iter:  632<br />test_rmse_mean:  23419.50","iter:  633<br />test_rmse_mean:  23424.98","iter:  634<br />test_rmse_mean:  23418.40","iter:  635<br />test_rmse_mean:  23413.77","iter:  636<br />test_rmse_mean:  23407.51","iter:  637<br />test_rmse_mean:  23406.30","iter:  638<br />test_rmse_mean:  23403.84","iter:  639<br />test_rmse_mean:  23397.46","iter:  640<br />test_rmse_mean:  23387.83","iter:  641<br />test_rmse_mean:  23386.78","iter:  642<br />test_rmse_mean:  23391.36","iter:  643<br />test_rmse_mean:  23384.53","iter:  644<br />test_rmse_mean:  23376.32","iter:  645<br />test_rmse_mean:  23370.73","iter:  646<br />test_rmse_mean:  23364.68","iter:  647<br />test_rmse_mean:  23355.99","iter:  648<br />test_rmse_mean:  23346.43","iter:  649<br />test_rmse_mean:  23343.35","iter:  650<br />test_rmse_mean:  23337.04","iter:  651<br />test_rmse_mean:  23326.58","iter:  652<br />test_rmse_mean:  23327.52","iter:  653<br />test_rmse_mean:  23330.38","iter:  654<br />test_rmse_mean:  23324.75","iter:  655<br />test_rmse_mean:  23317.31","iter:  656<br />test_rmse_mean:  23315.82","iter:  657<br />test_rmse_mean:  23317.04","iter:  658<br />test_rmse_mean:  23315.97","iter:  659<br />test_rmse_mean:  23313.75","iter:  660<br />test_rmse_mean:  23311.57","iter:  661<br />test_rmse_mean:  23307.30","iter:  662<br />test_rmse_mean:  23304.00","iter:  663<br />test_rmse_mean:  23297.54","iter:  664<br />test_rmse_mean:  23294.47","iter:  665<br />test_rmse_mean:  23284.77","iter:  666<br />test_rmse_mean:  23282.77","iter:  667<br />test_rmse_mean:  23284.89","iter:  668<br />test_rmse_mean:  23283.91","iter:  669<br />test_rmse_mean:  23274.79","iter:  670<br />test_rmse_mean:  23271.18","iter:  671<br />test_rmse_mean:  23271.54","iter:  672<br />test_rmse_mean:  23272.41","iter:  673<br />test_rmse_mean:  23275.95","iter:  674<br />test_rmse_mean:  23268.71","iter:  675<br />test_rmse_mean:  23265.92","iter:  676<br />test_rmse_mean:  23257.90","iter:  677<br />test_rmse_mean:  23252.65","iter:  678<br />test_rmse_mean:  23251.62","iter:  679<br />test_rmse_mean:  23240.97","iter:  680<br />test_rmse_mean:  23238.65","iter:  681<br />test_rmse_mean:  23236.08","iter:  682<br />test_rmse_mean:  23235.85","iter:  683<br />test_rmse_mean:  23234.43","iter:  684<br />test_rmse_mean:  23225.40","iter:  685<br />test_rmse_mean:  23222.32","iter:  686<br />test_rmse_mean:  23223.44","iter:  687<br />test_rmse_mean:  23221.75","iter:  688<br />test_rmse_mean:  23224.01","iter:  689<br />test_rmse_mean:  23224.44","iter:  690<br />test_rmse_mean:  23217.27","iter:  691<br />test_rmse_mean:  23216.21","iter:  692<br />test_rmse_mean:  23207.75","iter:  693<br />test_rmse_mean:  23208.42","iter:  694<br />test_rmse_mean:  23209.50","iter:  695<br />test_rmse_mean:  23202.41","iter:  696<br />test_rmse_mean:  23200.80","iter:  697<br />test_rmse_mean:  23200.75","iter:  698<br />test_rmse_mean:  23202.39","iter:  699<br />test_rmse_mean:  23194.75","iter:  700<br />test_rmse_mean:  23194.25","iter:  701<br />test_rmse_mean:  23189.89","iter:  702<br />test_rmse_mean:  23189.72","iter:  703<br />test_rmse_mean:  23186.77","iter:  704<br />test_rmse_mean:  23187.01","iter:  705<br />test_rmse_mean:  23187.89","iter:  706<br />test_rmse_mean:  23187.98","iter:  707<br />test_rmse_mean:  23186.37","iter:  708<br />test_rmse_mean:  23183.36","iter:  709<br />test_rmse_mean:  23180.87","iter:  710<br />test_rmse_mean:  23171.56","iter:  711<br />test_rmse_mean:  23173.20","iter:  712<br />test_rmse_mean:  23170.07","iter:  713<br />test_rmse_mean:  23164.56","iter:  714<br />test_rmse_mean:  23163.95","iter:  715<br />test_rmse_mean:  23157.35","iter:  716<br />test_rmse_mean:  23151.03","iter:  717<br />test_rmse_mean:  23151.76","iter:  718<br />test_rmse_mean:  23149.73","iter:  719<br />test_rmse_mean:  23151.24","iter:  720<br />test_rmse_mean:  23149.63","iter:  721<br />test_rmse_mean:  23152.57","iter:  722<br />test_rmse_mean:  23148.39","iter:  723<br />test_rmse_mean:  23144.27","iter:  724<br />test_rmse_mean:  23138.47","iter:  725<br />test_rmse_mean:  23135.14","iter:  726<br />test_rmse_mean:  23128.35","iter:  727<br />test_rmse_mean:  23127.15","iter:  728<br />test_rmse_mean:  23122.42","iter:  729<br />test_rmse_mean:  23122.52","iter:  730<br />test_rmse_mean:  23123.22","iter:  731<br />test_rmse_mean:  23116.15","iter:  732<br />test_rmse_mean:  23114.88","iter:  733<br />test_rmse_mean:  23108.64","iter:  734<br />test_rmse_mean:  23107.40","iter:  735<br />test_rmse_mean:  23106.10","iter:  736<br />test_rmse_mean:  23106.85","iter:  737<br />test_rmse_mean:  23103.53","iter:  738<br />test_rmse_mean:  23102.40","iter:  739<br />test_rmse_mean:  23102.36","iter:  740<br />test_rmse_mean:  23093.68","iter:  741<br />test_rmse_mean:  23085.23","iter:  742<br />test_rmse_mean:  23080.98","iter:  743<br />test_rmse_mean:  23086.07","iter:  744<br />test_rmse_mean:  23083.41","iter:  745<br />test_rmse_mean:  23075.57","iter:  746<br />test_rmse_mean:  23075.79","iter:  747<br />test_rmse_mean:  23075.50","iter:  748<br />test_rmse_mean:  23078.48","iter:  749<br />test_rmse_mean:  23081.07","iter:  750<br />test_rmse_mean:  23073.89","iter:  751<br />test_rmse_mean:  23068.20","iter:  752<br />test_rmse_mean:  23071.55","iter:  753<br />test_rmse_mean:  23072.25","iter:  754<br />test_rmse_mean:  23062.88","iter:  755<br />test_rmse_mean:  23062.59","iter:  756<br />test_rmse_mean:  23056.77","iter:  757<br />test_rmse_mean:  23062.85","iter:  758<br />test_rmse_mean:  23068.09","iter:  759<br />test_rmse_mean:  23065.14","iter:  760<br />test_rmse_mean:  23067.44","iter:  761<br />test_rmse_mean:  23058.52","iter:  762<br />test_rmse_mean:  23055.69","iter:  763<br />test_rmse_mean:  23048.41","iter:  764<br />test_rmse_mean:  23050.19","iter:  765<br />test_rmse_mean:  23051.44","iter:  766<br />test_rmse_mean:  23046.81","iter:  767<br />test_rmse_mean:  23050.42","iter:  768<br />test_rmse_mean:  23046.31","iter:  769<br />test_rmse_mean:  23042.24","iter:  770<br />test_rmse_mean:  23044.56","iter:  771<br />test_rmse_mean:  23043.07","iter:  772<br />test_rmse_mean:  23038.79","iter:  773<br />test_rmse_mean:  23039.24","iter:  774<br />test_rmse_mean:  23035.37","iter:  775<br />test_rmse_mean:  23034.23","iter:  776<br />test_rmse_mean:  23039.37","iter:  777<br />test_rmse_mean:  23040.39","iter:  778<br />test_rmse_mean:  23035.85","iter:  779<br />test_rmse_mean:  23032.42","iter:  780<br />test_rmse_mean:  23029.24","iter:  781<br />test_rmse_mean:  23025.20","iter:  782<br />test_rmse_mean:  23026.16","iter:  783<br />test_rmse_mean:  23028.97","iter:  784<br />test_rmse_mean:  23025.65","iter:  785<br />test_rmse_mean:  23025.06","iter:  786<br />test_rmse_mean:  23027.39","iter:  787<br />test_rmse_mean:  23023.03","iter:  788<br />test_rmse_mean:  23020.91","iter:  789<br />test_rmse_mean:  23019.81","iter:  790<br />test_rmse_mean:  23020.42","iter:  791<br />test_rmse_mean:  23020.38","iter:  792<br />test_rmse_mean:  23016.87","iter:  793<br />test_rmse_mean:  23018.90","iter:  794<br />test_rmse_mean:  23012.92","iter:  795<br />test_rmse_mean:  23011.55","iter:  796<br />test_rmse_mean:  23009.08","iter:  797<br />test_rmse_mean:  23009.65","iter:  798<br />test_rmse_mean:  23006.49","iter:  799<br />test_rmse_mean:  23008.37","iter:  800<br />test_rmse_mean:  23007.16","iter:  801<br />test_rmse_mean:  22998.70","iter:  802<br />test_rmse_mean:  22996.99","iter:  803<br />test_rmse_mean:  22994.99","iter:  804<br />test_rmse_mean:  22996.21","iter:  805<br />test_rmse_mean:  22986.69","iter:  806<br />test_rmse_mean:  22982.63","iter:  807<br />test_rmse_mean:  22982.78","iter:  808<br />test_rmse_mean:  22976.44","iter:  809<br />test_rmse_mean:  22969.45","iter:  810<br />test_rmse_mean:  22967.19","iter:  811<br />test_rmse_mean:  22962.16","iter:  812<br />test_rmse_mean:  22958.79","iter:  813<br />test_rmse_mean:  22958.60","iter:  814<br />test_rmse_mean:  22954.82","iter:  815<br />test_rmse_mean:  22953.62","iter:  816<br />test_rmse_mean:  22952.05","iter:  817<br />test_rmse_mean:  22945.74","iter:  818<br />test_rmse_mean:  22947.22","iter:  819<br />test_rmse_mean:  22945.21","iter:  820<br />test_rmse_mean:  22946.84","iter:  821<br />test_rmse_mean:  22945.19","iter:  822<br />test_rmse_mean:  22945.81","iter:  823<br />test_rmse_mean:  22943.41","iter:  824<br />test_rmse_mean:  22943.37","iter:  825<br />test_rmse_mean:  22942.92","iter:  826<br />test_rmse_mean:  22941.99","iter:  827<br />test_rmse_mean:  22937.82","iter:  828<br />test_rmse_mean:  22933.86","iter:  829<br />test_rmse_mean:  22931.01","iter:  830<br />test_rmse_mean:  22934.06","iter:  831<br />test_rmse_mean:  22935.82","iter:  832<br />test_rmse_mean:  22936.41","iter:  833<br />test_rmse_mean:  22938.58","iter:  834<br />test_rmse_mean:  22931.15","iter:  835<br />test_rmse_mean:  22928.79","iter:  836<br />test_rmse_mean:  22932.22","iter:  837<br />test_rmse_mean:  22931.32","iter:  838<br />test_rmse_mean:  22928.10","iter:  839<br />test_rmse_mean:  22928.93","iter:  840<br />test_rmse_mean:  22925.21","iter:  841<br />test_rmse_mean:  22923.94","iter:  842<br />test_rmse_mean:  22920.39","iter:  843<br />test_rmse_mean:  22921.28","iter:  844<br />test_rmse_mean:  22917.00","iter:  845<br />test_rmse_mean:  22913.91","iter:  846<br />test_rmse_mean:  22910.35","iter:  847<br />test_rmse_mean:  22903.82","iter:  848<br />test_rmse_mean:  22901.35","iter:  849<br />test_rmse_mean:  22897.49","iter:  850<br />test_rmse_mean:  22898.38","iter:  851<br />test_rmse_mean:  22895.58","iter:  852<br />test_rmse_mean:  22895.83","iter:  853<br />test_rmse_mean:  22894.44","iter:  854<br />test_rmse_mean:  22892.05","iter:  855<br />test_rmse_mean:  22892.19","iter:  856<br />test_rmse_mean:  22888.61","iter:  857<br />test_rmse_mean:  22887.83","iter:  858<br />test_rmse_mean:  22887.05","iter:  859<br />test_rmse_mean:  22882.20","iter:  860<br />test_rmse_mean:  22876.08","iter:  861<br />test_rmse_mean:  22875.04","iter:  862<br />test_rmse_mean:  22872.25","iter:  863<br />test_rmse_mean:  22870.40","iter:  864<br />test_rmse_mean:  22873.58","iter:  865<br />test_rmse_mean:  22869.91","iter:  866<br />test_rmse_mean:  22872.58","iter:  867<br />test_rmse_mean:  22871.32","iter:  868<br />test_rmse_mean:  22872.02","iter:  869<br />test_rmse_mean:  22870.77","iter:  870<br />test_rmse_mean:  22871.33","iter:  871<br />test_rmse_mean:  22860.87","iter:  872<br />test_rmse_mean:  22861.45","iter:  873<br />test_rmse_mean:  22857.53","iter:  874<br />test_rmse_mean:  22859.09","iter:  875<br />test_rmse_mean:  22859.87","iter:  876<br />test_rmse_mean:  22860.15","iter:  877<br />test_rmse_mean:  22867.23","iter:  878<br />test_rmse_mean:  22864.83","iter:  879<br />test_rmse_mean:  22860.17","iter:  880<br />test_rmse_mean:  22853.01","iter:  881<br />test_rmse_mean:  22847.46","iter:  882<br />test_rmse_mean:  22842.20","iter:  883<br />test_rmse_mean:  22842.76","iter:  884<br />test_rmse_mean:  22845.46","iter:  885<br />test_rmse_mean:  22845.94","iter:  886<br />test_rmse_mean:  22844.67","iter:  887<br />test_rmse_mean:  22843.41","iter:  888<br />test_rmse_mean:  22843.91","iter:  889<br />test_rmse_mean:  22839.17","iter:  890<br />test_rmse_mean:  22838.23","iter:  891<br />test_rmse_mean:  22832.75","iter:  892<br />test_rmse_mean:  22830.95","iter:  893<br />test_rmse_mean:  22827.71","iter:  894<br />test_rmse_mean:  22831.04","iter:  895<br />test_rmse_mean:  22830.26","iter:  896<br />test_rmse_mean:  22831.60","iter:  897<br />test_rmse_mean:  22831.73","iter:  898<br />test_rmse_mean:  22835.26","iter:  899<br />test_rmse_mean:  22833.63","iter:  900<br />test_rmse_mean:  22836.96","iter:  901<br />test_rmse_mean:  22839.28","iter:  902<br />test_rmse_mean:  22838.86","iter:  903<br />test_rmse_mean:  22830.74","iter:  904<br />test_rmse_mean:  22835.93","iter:  905<br />test_rmse_mean:  22824.96","iter:  906<br />test_rmse_mean:  22826.31","iter:  907<br />test_rmse_mean:  22821.44","iter:  908<br />test_rmse_mean:  22819.92","iter:  909<br />test_rmse_mean:  22815.86","iter:  910<br />test_rmse_mean:  22818.38","iter:  911<br />test_rmse_mean:  22814.71","iter:  912<br />test_rmse_mean:  22814.80","iter:  913<br />test_rmse_mean:  22820.13","iter:  914<br />test_rmse_mean:  22818.10","iter:  915<br />test_rmse_mean:  22814.86","iter:  916<br />test_rmse_mean:  22814.34","iter:  917<br />test_rmse_mean:  22809.02","iter:  918<br />test_rmse_mean:  22809.79","iter:  919<br />test_rmse_mean:  22812.80","iter:  920<br />test_rmse_mean:  22812.81","iter:  921<br />test_rmse_mean:  22810.90","iter:  922<br />test_rmse_mean:  22810.70","iter:  923<br />test_rmse_mean:  22806.60","iter:  924<br />test_rmse_mean:  22805.62","iter:  925<br />test_rmse_mean:  22806.78","iter:  926<br />test_rmse_mean:  22803.71","iter:  927<br />test_rmse_mean:  22796.10","iter:  928<br />test_rmse_mean:  22789.57","iter:  929<br />test_rmse_mean:  22781.76","iter:  930<br />test_rmse_mean:  22783.56","iter:  931<br />test_rmse_mean:  22784.10","iter:  932<br />test_rmse_mean:  22781.82","iter:  933<br />test_rmse_mean:  22781.23","iter:  934<br />test_rmse_mean:  22782.46","iter:  935<br />test_rmse_mean:  22780.20","iter:  936<br />test_rmse_mean:  22777.27","iter:  937<br />test_rmse_mean:  22779.35","iter:  938<br />test_rmse_mean:  22777.92","iter:  939<br />test_rmse_mean:  22780.44","iter:  940<br />test_rmse_mean:  22779.85","iter:  941<br />test_rmse_mean:  22778.19","iter:  942<br />test_rmse_mean:  22780.52","iter:  943<br />test_rmse_mean:  22780.16","iter:  944<br />test_rmse_mean:  22777.68","iter:  945<br />test_rmse_mean:  22775.78","iter:  946<br />test_rmse_mean:  22776.69","iter:  947<br />test_rmse_mean:  22773.57","iter:  948<br />test_rmse_mean:  22771.53","iter:  949<br />test_rmse_mean:  22773.33","iter:  950<br />test_rmse_mean:  22771.35","iter:  951<br />test_rmse_mean:  22773.12","iter:  952<br />test_rmse_mean:  22767.67","iter:  953<br />test_rmse_mean:  22762.69","iter:  954<br />test_rmse_mean:  22760.33","iter:  955<br />test_rmse_mean:  22760.83","iter:  956<br />test_rmse_mean:  22758.86","iter:  957<br />test_rmse_mean:  22756.13","iter:  958<br />test_rmse_mean:  22754.92","iter:  959<br />test_rmse_mean:  22755.62","iter:  960<br />test_rmse_mean:  22760.29","iter:  961<br />test_rmse_mean:  22760.56","iter:  962<br />test_rmse_mean:  22758.76","iter:  963<br />test_rmse_mean:  22757.00","iter:  964<br />test_rmse_mean:  22757.25","iter:  965<br />test_rmse_mean:  22754.12","iter:  966<br />test_rmse_mean:  22753.37","iter:  967<br />test_rmse_mean:  22751.58","iter:  968<br />test_rmse_mean:  22746.62","iter:  969<br />test_rmse_mean:  22743.86","iter:  970<br />test_rmse_mean:  22741.28","iter:  971<br />test_rmse_mean:  22743.23","iter:  972<br />test_rmse_mean:  22745.08","iter:  973<br />test_rmse_mean:  22748.40","iter:  974<br />test_rmse_mean:  22748.02","iter:  975<br />test_rmse_mean:  22740.04","iter:  976<br />test_rmse_mean:  22736.63","iter:  977<br />test_rmse_mean:  22736.67","iter:  978<br />test_rmse_mean:  22733.72","iter:  979<br />test_rmse_mean:  22730.46","iter:  980<br />test_rmse_mean:  22728.35","iter:  981<br />test_rmse_mean:  22727.64","iter:  982<br />test_rmse_mean:  22728.24","iter:  983<br />test_rmse_mean:  22719.76","iter:  984<br />test_rmse_mean:  22718.87","iter:  985<br />test_rmse_mean:  22717.04","iter:  986<br />test_rmse_mean:  22712.03","iter:  987<br />test_rmse_mean:  22709.45","iter:  988<br />test_rmse_mean:  22706.45","iter:  989<br />test_rmse_mean:  22710.06","iter:  990<br />test_rmse_mean:  22709.16","iter:  991<br />test_rmse_mean:  22707.63","iter:  992<br />test_rmse_mean:  22710.95","iter:  993<br />test_rmse_mean:  22709.90","iter:  994<br />test_rmse_mean:  22710.33","iter:  995<br />test_rmse_mean:  22704.10","iter:  996<br />test_rmse_mean:  22694.46","iter:  997<br />test_rmse_mean:  22693.41","iter:  998<br />test_rmse_mean:  22690.25","iter:  999<br />test_rmse_mean:  22686.04","iter: 1000<br />test_rmse_mean:  22681.67","iter: 1001<br />test_rmse_mean:  22678.71","iter: 1002<br />test_rmse_mean:  22680.09","iter: 1003<br />test_rmse_mean:  22682.65","iter: 1004<br />test_rmse_mean:  22681.30","iter: 1005<br />test_rmse_mean:  22682.22","iter: 1006<br />test_rmse_mean:  22678.11","iter: 1007<br />test_rmse_mean:  22678.14","iter: 1008<br />test_rmse_mean:  22676.30","iter: 1009<br />test_rmse_mean:  22671.34","iter: 1010<br />test_rmse_mean:  22661.44","iter: 1011<br />test_rmse_mean:  22663.17","iter: 1012<br />test_rmse_mean:  22659.08","iter: 1013<br />test_rmse_mean:  22657.64","iter: 1014<br />test_rmse_mean:  22648.78","iter: 1015<br />test_rmse_mean:  22647.79","iter: 1016<br />test_rmse_mean:  22644.92","iter: 1017<br />test_rmse_mean:  22641.90","iter: 1018<br />test_rmse_mean:  22640.58","iter: 1019<br />test_rmse_mean:  22638.20","iter: 1020<br />test_rmse_mean:  22636.80","iter: 1021<br />test_rmse_mean:  22636.46","iter: 1022<br />test_rmse_mean:  22634.70","iter: 1023<br />test_rmse_mean:  22628.42","iter: 1024<br />test_rmse_mean:  22628.30","iter: 1025<br />test_rmse_mean:  22624.75","iter: 1026<br />test_rmse_mean:  22620.09","iter: 1027<br />test_rmse_mean:  22614.47","iter: 1028<br />test_rmse_mean:  22611.45","iter: 1029<br />test_rmse_mean:  22612.69","iter: 1030<br />test_rmse_mean:  22611.71","iter: 1031<br />test_rmse_mean:  22612.62","iter: 1032<br />test_rmse_mean:  22611.88","iter: 1033<br />test_rmse_mean:  22611.40","iter: 1034<br />test_rmse_mean:  22614.99","iter: 1035<br />test_rmse_mean:  22614.14","iter: 1036<br />test_rmse_mean:  22615.20","iter: 1037<br />test_rmse_mean:  22615.19","iter: 1038<br />test_rmse_mean:  22610.96","iter: 1039<br />test_rmse_mean:  22612.01","iter: 1040<br />test_rmse_mean:  22606.66","iter: 1041<br />test_rmse_mean:  22604.42","iter: 1042<br />test_rmse_mean:  22595.27","iter: 1043<br />test_rmse_mean:  22591.12","iter: 1044<br />test_rmse_mean:  22590.01","iter: 1045<br />test_rmse_mean:  22592.42","iter: 1046<br />test_rmse_mean:  22593.48","iter: 1047<br />test_rmse_mean:  22591.68","iter: 1048<br />test_rmse_mean:  22585.67","iter: 1049<br />test_rmse_mean:  22584.46","iter: 1050<br />test_rmse_mean:  22585.46","iter: 1051<br />test_rmse_mean:  22582.52","iter: 1052<br />test_rmse_mean:  22589.04","iter: 1053<br />test_rmse_mean:  22586.81","iter: 1054<br />test_rmse_mean:  22584.82","iter: 1055<br />test_rmse_mean:  22586.74","iter: 1056<br />test_rmse_mean:  22587.08","iter: 1057<br />test_rmse_mean:  22588.11","iter: 1058<br />test_rmse_mean:  22588.32","iter: 1059<br />test_rmse_mean:  22581.01","iter: 1060<br />test_rmse_mean:  22579.31","iter: 1061<br />test_rmse_mean:  22578.30","iter: 1062<br />test_rmse_mean:  22579.37","iter: 1063<br />test_rmse_mean:  22576.64","iter: 1064<br />test_rmse_mean:  22574.68","iter: 1065<br />test_rmse_mean:  22574.45","iter: 1066<br />test_rmse_mean:  22570.17","iter: 1067<br />test_rmse_mean:  22572.99","iter: 1068<br />test_rmse_mean:  22571.00","iter: 1069<br />test_rmse_mean:  22572.92","iter: 1070<br />test_rmse_mean:  22571.34","iter: 1071<br />test_rmse_mean:  22567.18","iter: 1072<br />test_rmse_mean:  22565.41","iter: 1073<br />test_rmse_mean:  22565.82","iter: 1074<br />test_rmse_mean:  22564.01","iter: 1075<br />test_rmse_mean:  22564.69","iter: 1076<br />test_rmse_mean:  22564.05","iter: 1077<br />test_rmse_mean:  22562.61","iter: 1078<br />test_rmse_mean:  22561.86","iter: 1079<br />test_rmse_mean:  22563.94","iter: 1080<br />test_rmse_mean:  22565.72","iter: 1081<br />test_rmse_mean:  22565.08","iter: 1082<br />test_rmse_mean:  22565.84","iter: 1083<br />test_rmse_mean:  22564.94","iter: 1084<br />test_rmse_mean:  22560.02","iter: 1085<br />test_rmse_mean:  22561.65","iter: 1086<br />test_rmse_mean:  22559.98","iter: 1087<br />test_rmse_mean:  22562.09","iter: 1088<br />test_rmse_mean:  22561.05","iter: 1089<br />test_rmse_mean:  22557.78","iter: 1090<br />test_rmse_mean:  22558.12","iter: 1091<br />test_rmse_mean:  22553.41","iter: 1092<br />test_rmse_mean:  22552.24","iter: 1093<br />test_rmse_mean:  22555.44","iter: 1094<br />test_rmse_mean:  22560.08","iter: 1095<br />test_rmse_mean:  22561.27","iter: 1096<br />test_rmse_mean:  22560.50","iter: 1097<br />test_rmse_mean:  22561.57","iter: 1098<br />test_rmse_mean:  22561.86","iter: 1099<br />test_rmse_mean:  22559.84","iter: 1100<br />test_rmse_mean:  22556.32","iter: 1101<br />test_rmse_mean:  22555.04","iter: 1102<br />test_rmse_mean:  22556.08","iter: 1103<br />test_rmse_mean:  22554.19","iter: 1104<br />test_rmse_mean:  22553.45","iter: 1105<br />test_rmse_mean:  22550.81","iter: 1106<br />test_rmse_mean:  22555.39","iter: 1107<br />test_rmse_mean:  22553.70","iter: 1108<br />test_rmse_mean:  22554.81","iter: 1109<br />test_rmse_mean:  22556.31","iter: 1110<br />test_rmse_mean:  22559.37","iter: 1111<br />test_rmse_mean:  22556.83","iter: 1112<br />test_rmse_mean:  22556.53","iter: 1113<br />test_rmse_mean:  22555.18","iter: 1114<br />test_rmse_mean:  22557.38","iter: 1115<br />test_rmse_mean:  22552.89","iter: 1116<br />test_rmse_mean:  22548.67","iter: 1117<br />test_rmse_mean:  22547.54","iter: 1118<br />test_rmse_mean:  22541.05","iter: 1119<br />test_rmse_mean:  22535.02","iter: 1120<br />test_rmse_mean:  22536.41","iter: 1121<br />test_rmse_mean:  22537.58","iter: 1122<br />test_rmse_mean:  22531.01","iter: 1123<br />test_rmse_mean:  22531.27","iter: 1124<br />test_rmse_mean:  22530.24","iter: 1125<br />test_rmse_mean:  22528.51","iter: 1126<br />test_rmse_mean:  22532.95","iter: 1127<br />test_rmse_mean:  22533.12","iter: 1128<br />test_rmse_mean:  22533.70","iter: 1129<br />test_rmse_mean:  22536.79","iter: 1130<br />test_rmse_mean:  22532.23","iter: 1131<br />test_rmse_mean:  22529.02","iter: 1132<br />test_rmse_mean:  22530.59","iter: 1133<br />test_rmse_mean:  22532.15","iter: 1134<br />test_rmse_mean:  22533.12","iter: 1135<br />test_rmse_mean:  22530.88","iter: 1136<br />test_rmse_mean:  22529.64","iter: 1137<br />test_rmse_mean:  22530.51","iter: 1138<br />test_rmse_mean:  22530.20","iter: 1139<br />test_rmse_mean:  22528.49","iter: 1140<br />test_rmse_mean:  22528.94","iter: 1141<br />test_rmse_mean:  22526.83","iter: 1142<br />test_rmse_mean:  22530.06","iter: 1143<br />test_rmse_mean:  22528.96","iter: 1144<br />test_rmse_mean:  22529.45","iter: 1145<br />test_rmse_mean:  22527.33","iter: 1146<br />test_rmse_mean:  22526.53","iter: 1147<br />test_rmse_mean:  22527.45","iter: 1148<br />test_rmse_mean:  22529.61","iter: 1149<br />test_rmse_mean:  22528.74","iter: 1150<br />test_rmse_mean:  22528.87","iter: 1151<br />test_rmse_mean:  22529.94","iter: 1152<br />test_rmse_mean:  22525.51","iter: 1153<br />test_rmse_mean:  22524.53","iter: 1154<br />test_rmse_mean:  22525.12","iter: 1155<br />test_rmse_mean:  22523.04","iter: 1156<br />test_rmse_mean:  22518.83","iter: 1157<br />test_rmse_mean:  22519.05","iter: 1158<br />test_rmse_mean:  22522.12","iter: 1159<br />test_rmse_mean:  22519.98","iter: 1160<br />test_rmse_mean:  22512.03","iter: 1161<br />test_rmse_mean:  22508.06","iter: 1162<br />test_rmse_mean:  22510.45","iter: 1163<br />test_rmse_mean:  22511.08","iter: 1164<br />test_rmse_mean:  22509.72","iter: 1165<br />test_rmse_mean:  22509.66","iter: 1166<br />test_rmse_mean:  22506.58","iter: 1167<br />test_rmse_mean:  22506.77","iter: 1168<br />test_rmse_mean:  22506.72","iter: 1169<br />test_rmse_mean:  22502.44","iter: 1170<br />test_rmse_mean:  22502.12","iter: 1171<br />test_rmse_mean:  22494.21","iter: 1172<br />test_rmse_mean:  22494.52","iter: 1173<br />test_rmse_mean:  22494.16","iter: 1174<br />test_rmse_mean:  22494.32","iter: 1175<br />test_rmse_mean:  22492.11","iter: 1176<br />test_rmse_mean:  22490.23","iter: 1177<br />test_rmse_mean:  22488.85","iter: 1178<br />test_rmse_mean:  22488.91","iter: 1179<br />test_rmse_mean:  22487.39","iter: 1180<br />test_rmse_mean:  22490.35","iter: 1181<br />test_rmse_mean:  22486.79","iter: 1182<br />test_rmse_mean:  22488.06","iter: 1183<br />test_rmse_mean:  22488.43","iter: 1184<br />test_rmse_mean:  22489.54","iter: 1185<br />test_rmse_mean:  22486.82","iter: 1186<br />test_rmse_mean:  22484.92","iter: 1187<br />test_rmse_mean:  22481.85","iter: 1188<br />test_rmse_mean:  22482.83","iter: 1189<br />test_rmse_mean:  22481.40","iter: 1190<br />test_rmse_mean:  22478.92","iter: 1191<br />test_rmse_mean:  22476.97","iter: 1192<br />test_rmse_mean:  22473.14","iter: 1193<br />test_rmse_mean:  22473.12","iter: 1194<br />test_rmse_mean:  22471.49","iter: 1195<br />test_rmse_mean:  22471.54","iter: 1196<br />test_rmse_mean:  22469.20","iter: 1197<br />test_rmse_mean:  22469.48","iter: 1198<br />test_rmse_mean:  22471.28","iter: 1199<br />test_rmse_mean:  22471.79","iter: 1200<br />test_rmse_mean:  22473.19","iter: 1201<br />test_rmse_mean:  22473.05","iter: 1202<br />test_rmse_mean:  22471.33","iter: 1203<br />test_rmse_mean:  22473.46","iter: 1204<br />test_rmse_mean:  22472.49","iter: 1205<br />test_rmse_mean:  22468.45","iter: 1206<br />test_rmse_mean:  22467.25","iter: 1207<br />test_rmse_mean:  22467.46","iter: 1208<br />test_rmse_mean:  22467.96","iter: 1209<br />test_rmse_mean:  22468.90","iter: 1210<br />test_rmse_mean:  22464.50","iter: 1211<br />test_rmse_mean:  22457.65","iter: 1212<br />test_rmse_mean:  22456.00","iter: 1213<br />test_rmse_mean:  22452.44","iter: 1214<br />test_rmse_mean:  22454.97","iter: 1215<br />test_rmse_mean:  22452.48","iter: 1216<br />test_rmse_mean:  22448.30","iter: 1217<br />test_rmse_mean:  22449.74","iter: 1218<br />test_rmse_mean:  22449.41","iter: 1219<br />test_rmse_mean:  22451.08","iter: 1220<br />test_rmse_mean:  22454.92","iter: 1221<br />test_rmse_mean:  22454.05","iter: 1222<br />test_rmse_mean:  22448.79","iter: 1223<br />test_rmse_mean:  22450.09","iter: 1224<br />test_rmse_mean:  22443.61","iter: 1225<br />test_rmse_mean:  22443.84","iter: 1226<br />test_rmse_mean:  22445.20","iter: 1227<br />test_rmse_mean:  22444.84","iter: 1228<br />test_rmse_mean:  22443.88","iter: 1229<br />test_rmse_mean:  22440.50","iter: 1230<br />test_rmse_mean:  22440.52","iter: 1231<br />test_rmse_mean:  22437.33","iter: 1232<br />test_rmse_mean:  22438.04","iter: 1233<br />test_rmse_mean:  22435.51","iter: 1234<br />test_rmse_mean:  22435.83","iter: 1235<br />test_rmse_mean:  22438.88","iter: 1236<br />test_rmse_mean:  22436.69","iter: 1237<br />test_rmse_mean:  22438.23","iter: 1238<br />test_rmse_mean:  22440.17","iter: 1239<br />test_rmse_mean:  22441.75","iter: 1240<br />test_rmse_mean:  22439.70","iter: 1241<br />test_rmse_mean:  22436.81","iter: 1242<br />test_rmse_mean:  22435.21","iter: 1243<br />test_rmse_mean:  22435.92","iter: 1244<br />test_rmse_mean:  22436.31","iter: 1245<br />test_rmse_mean:  22439.09","iter: 1246<br />test_rmse_mean:  22436.01","iter: 1247<br />test_rmse_mean:  22434.45","iter: 1248<br />test_rmse_mean:  22429.18","iter: 1249<br />test_rmse_mean:  22424.07","iter: 1250<br />test_rmse_mean:  22421.00","iter: 1251<br />test_rmse_mean:  22420.70","iter: 1252<br />test_rmse_mean:  22419.38","iter: 1253<br />test_rmse_mean:  22420.26","iter: 1254<br />test_rmse_mean:  22418.35","iter: 1255<br />test_rmse_mean:  22417.95","iter: 1256<br />test_rmse_mean:  22419.32","iter: 1257<br />test_rmse_mean:  22423.52","iter: 1258<br />test_rmse_mean:  22423.28","iter: 1259<br />test_rmse_mean:  22422.26","iter: 1260<br />test_rmse_mean:  22420.82","iter: 1261<br />test_rmse_mean:  22420.12","iter: 1262<br />test_rmse_mean:  22420.31","iter: 1263<br />test_rmse_mean:  22420.81","iter: 1264<br />test_rmse_mean:  22419.83","iter: 1265<br />test_rmse_mean:  22419.74","iter: 1266<br />test_rmse_mean:  22422.43","iter: 1267<br />test_rmse_mean:  22419.82","iter: 1268<br />test_rmse_mean:  22419.77","iter: 1269<br />test_rmse_mean:  22418.65","iter: 1270<br />test_rmse_mean:  22415.32","iter: 1271<br />test_rmse_mean:  22414.38","iter: 1272<br />test_rmse_mean:  22410.36","iter: 1273<br />test_rmse_mean:  22410.19","iter: 1274<br />test_rmse_mean:  22408.76","iter: 1275<br />test_rmse_mean:  22408.43","iter: 1276<br />test_rmse_mean:  22407.29","iter: 1277<br />test_rmse_mean:  22406.35","iter: 1278<br />test_rmse_mean:  22404.02","iter: 1279<br />test_rmse_mean:  22405.96","iter: 1280<br />test_rmse_mean:  22404.00","iter: 1281<br />test_rmse_mean:  22402.39","iter: 1282<br />test_rmse_mean:  22399.76","iter: 1283<br />test_rmse_mean:  22398.41","iter: 1284<br />test_rmse_mean:  22397.16","iter: 1285<br />test_rmse_mean:  22395.39","iter: 1286<br />test_rmse_mean:  22397.05","iter: 1287<br />test_rmse_mean:  22394.57","iter: 1288<br />test_rmse_mean:  22392.17","iter: 1289<br />test_rmse_mean:  22391.52","iter: 1290<br />test_rmse_mean:  22388.87","iter: 1291<br />test_rmse_mean:  22387.33","iter: 1292<br />test_rmse_mean:  22388.74","iter: 1293<br />test_rmse_mean:  22388.36","iter: 1294<br />test_rmse_mean:  22388.70","iter: 1295<br />test_rmse_mean:  22386.37","iter: 1296<br />test_rmse_mean:  22389.84","iter: 1297<br />test_rmse_mean:  22388.45","iter: 1298<br />test_rmse_mean:  22390.85","iter: 1299<br />test_rmse_mean:  22389.76","iter: 1300<br />test_rmse_mean:  22387.76","iter: 1301<br />test_rmse_mean:  22386.26","iter: 1302<br />test_rmse_mean:  22389.22","iter: 1303<br />test_rmse_mean:  22390.62","iter: 1304<br />test_rmse_mean:  22388.20","iter: 1305<br />test_rmse_mean:  22389.66","iter: 1306<br />test_rmse_mean:  22386.98","iter: 1307<br />test_rmse_mean:  22384.76","iter: 1308<br />test_rmse_mean:  22386.50","iter: 1309<br />test_rmse_mean:  22386.17","iter: 1310<br />test_rmse_mean:  22385.99","iter: 1311<br />test_rmse_mean:  22381.24","iter: 1312<br />test_rmse_mean:  22380.62","iter: 1313<br />test_rmse_mean:  22379.70","iter: 1314<br />test_rmse_mean:  22381.72","iter: 1315<br />test_rmse_mean:  22377.16","iter: 1316<br />test_rmse_mean:  22373.89","iter: 1317<br />test_rmse_mean:  22370.69","iter: 1318<br />test_rmse_mean:  22369.85","iter: 1319<br />test_rmse_mean:  22369.08","iter: 1320<br />test_rmse_mean:  22367.28","iter: 1321<br />test_rmse_mean:  22364.62","iter: 1322<br />test_rmse_mean:  22364.17","iter: 1323<br />test_rmse_mean:  22364.37","iter: 1324<br />test_rmse_mean:  22362.52","iter: 1325<br />test_rmse_mean:  22362.14","iter: 1326<br />test_rmse_mean:  22359.68","iter: 1327<br />test_rmse_mean:  22356.35","iter: 1328<br />test_rmse_mean:  22357.55","iter: 1329<br />test_rmse_mean:  22358.14","iter: 1330<br />test_rmse_mean:  22357.37","iter: 1331<br />test_rmse_mean:  22357.59","iter: 1332<br />test_rmse_mean:  22356.95","iter: 1333<br />test_rmse_mean:  22356.51","iter: 1334<br />test_rmse_mean:  22357.23","iter: 1335<br />test_rmse_mean:  22358.08","iter: 1336<br />test_rmse_mean:  22358.70","iter: 1337<br />test_rmse_mean:  22358.86","iter: 1338<br />test_rmse_mean:  22355.54","iter: 1339<br />test_rmse_mean:  22352.97","iter: 1340<br />test_rmse_mean:  22352.51","iter: 1341<br />test_rmse_mean:  22350.52","iter: 1342<br />test_rmse_mean:  22345.21","iter: 1343<br />test_rmse_mean:  22341.53","iter: 1344<br />test_rmse_mean:  22338.88","iter: 1345<br />test_rmse_mean:  22340.11","iter: 1346<br />test_rmse_mean:  22336.66","iter: 1347<br />test_rmse_mean:  22336.07","iter: 1348<br />test_rmse_mean:  22335.22","iter: 1349<br />test_rmse_mean:  22335.42","iter: 1350<br />test_rmse_mean:  22333.68","iter: 1351<br />test_rmse_mean:  22332.54","iter: 1352<br />test_rmse_mean:  22337.04","iter: 1353<br />test_rmse_mean:  22336.16","iter: 1354<br />test_rmse_mean:  22338.85","iter: 1355<br />test_rmse_mean:  22338.62","iter: 1356<br />test_rmse_mean:  22337.58","iter: 1357<br />test_rmse_mean:  22336.15","iter: 1358<br />test_rmse_mean:  22334.39","iter: 1359<br />test_rmse_mean:  22333.14","iter: 1360<br />test_rmse_mean:  22335.41","iter: 1361<br />test_rmse_mean:  22334.67","iter: 1362<br />test_rmse_mean:  22334.31","iter: 1363<br />test_rmse_mean:  22333.06","iter: 1364<br />test_rmse_mean:  22332.83","iter: 1365<br />test_rmse_mean:  22328.59","iter: 1366<br />test_rmse_mean:  22330.11","iter: 1367<br />test_rmse_mean:  22328.59","iter: 1368<br />test_rmse_mean:  22325.75","iter: 1369<br />test_rmse_mean:  22323.26","iter: 1370<br />test_rmse_mean:  22322.04","iter: 1371<br />test_rmse_mean:  22326.17","iter: 1372<br />test_rmse_mean:  22327.12","iter: 1373<br />test_rmse_mean:  22327.05","iter: 1374<br />test_rmse_mean:  22327.90","iter: 1375<br />test_rmse_mean:  22324.51","iter: 1376<br />test_rmse_mean:  22323.71","iter: 1377<br />test_rmse_mean:  22326.94","iter: 1378<br />test_rmse_mean:  22328.09","iter: 1379<br />test_rmse_mean:  22327.53","iter: 1380<br />test_rmse_mean:  22327.58","iter: 1381<br />test_rmse_mean:  22328.17","iter: 1382<br />test_rmse_mean:  22325.19","iter: 1383<br />test_rmse_mean:  22323.57","iter: 1384<br />test_rmse_mean:  22323.75","iter: 1385<br />test_rmse_mean:  22323.76","iter: 1386<br />test_rmse_mean:  22322.56","iter: 1387<br />test_rmse_mean:  22322.56","iter: 1388<br />test_rmse_mean:  22322.93","iter: 1389<br />test_rmse_mean:  22323.84","iter: 1390<br />test_rmse_mean:  22320.41","iter: 1391<br />test_rmse_mean:  22320.34","iter: 1392<br />test_rmse_mean:  22322.99","iter: 1393<br />test_rmse_mean:  22325.35","iter: 1394<br />test_rmse_mean:  22326.31","iter: 1395<br />test_rmse_mean:  22321.26","iter: 1396<br />test_rmse_mean:  22324.14","iter: 1397<br />test_rmse_mean:  22322.19","iter: 1398<br />test_rmse_mean:  22317.19","iter: 1399<br />test_rmse_mean:  22315.80","iter: 1400<br />test_rmse_mean:  22316.60","iter: 1401<br />test_rmse_mean:  22318.03","iter: 1402<br />test_rmse_mean:  22315.02","iter: 1403<br />test_rmse_mean:  22316.11","iter: 1404<br />test_rmse_mean:  22314.49","iter: 1405<br />test_rmse_mean:  22315.54","iter: 1406<br />test_rmse_mean:  22315.44","iter: 1407<br />test_rmse_mean:  22314.14","iter: 1408<br />test_rmse_mean:  22312.06","iter: 1409<br />test_rmse_mean:  22310.40","iter: 1410<br />test_rmse_mean:  22305.80","iter: 1411<br />test_rmse_mean:  22302.90","iter: 1412<br />test_rmse_mean:  22302.06","iter: 1413<br />test_rmse_mean:  22303.02","iter: 1414<br />test_rmse_mean:  22303.42","iter: 1415<br />test_rmse_mean:  22302.83","iter: 1416<br />test_rmse_mean:  22302.74","iter: 1417<br />test_rmse_mean:  22302.10","iter: 1418<br />test_rmse_mean:  22301.57","iter: 1419<br />test_rmse_mean:  22299.55","iter: 1420<br />test_rmse_mean:  22300.16","iter: 1421<br />test_rmse_mean:  22298.80","iter: 1422<br />test_rmse_mean:  22297.89","iter: 1423<br />test_rmse_mean:  22298.96","iter: 1424<br />test_rmse_mean:  22296.35","iter: 1425<br />test_rmse_mean:  22295.74","iter: 1426<br />test_rmse_mean:  22290.92","iter: 1427<br />test_rmse_mean:  22291.64","iter: 1428<br />test_rmse_mean:  22295.88","iter: 1429<br />test_rmse_mean:  22292.99","iter: 1430<br />test_rmse_mean:  22292.11","iter: 1431<br />test_rmse_mean:  22295.35","iter: 1432<br />test_rmse_mean:  22295.55","iter: 1433<br />test_rmse_mean:  22293.51","iter: 1434<br />test_rmse_mean:  22288.86","iter: 1435<br />test_rmse_mean:  22288.50","iter: 1436<br />test_rmse_mean:  22289.21","iter: 1437<br />test_rmse_mean:  22287.25","iter: 1438<br />test_rmse_mean:  22284.55","iter: 1439<br />test_rmse_mean:  22285.38","iter: 1440<br />test_rmse_mean:  22286.61","iter: 1441<br />test_rmse_mean:  22287.79","iter: 1442<br />test_rmse_mean:  22286.25","iter: 1443<br />test_rmse_mean:  22284.94","iter: 1444<br />test_rmse_mean:  22282.57","iter: 1445<br />test_rmse_mean:  22280.44","iter: 1446<br />test_rmse_mean:  22277.53","iter: 1447<br />test_rmse_mean:  22276.58","iter: 1448<br />test_rmse_mean:  22275.55","iter: 1449<br />test_rmse_mean:  22274.08","iter: 1450<br />test_rmse_mean:  22273.40","iter: 1451<br />test_rmse_mean:  22271.28","iter: 1452<br />test_rmse_mean:  22271.05","iter: 1453<br />test_rmse_mean:  22270.81","iter: 1454<br />test_rmse_mean:  22270.62","iter: 1455<br />test_rmse_mean:  22271.39","iter: 1456<br />test_rmse_mean:  22269.26","iter: 1457<br />test_rmse_mean:  22264.31","iter: 1458<br />test_rmse_mean:  22264.61","iter: 1459<br />test_rmse_mean:  22267.59","iter: 1460<br />test_rmse_mean:  22267.40","iter: 1461<br />test_rmse_mean:  22264.15","iter: 1462<br />test_rmse_mean:  22263.16","iter: 1463<br />test_rmse_mean:  22261.37","iter: 1464<br />test_rmse_mean:  22259.62","iter: 1465<br />test_rmse_mean:  22259.35","iter: 1466<br />test_rmse_mean:  22260.00","iter: 1467<br />test_rmse_mean:  22262.43","iter: 1468<br />test_rmse_mean:  22259.59","iter: 1469<br />test_rmse_mean:  22258.67","iter: 1470<br />test_rmse_mean:  22256.44","iter: 1471<br />test_rmse_mean:  22254.50","iter: 1472<br />test_rmse_mean:  22251.54","iter: 1473<br />test_rmse_mean:  22250.61","iter: 1474<br />test_rmse_mean:  22246.23","iter: 1475<br />test_rmse_mean:  22246.10","iter: 1476<br />test_rmse_mean:  22244.55","iter: 1477<br />test_rmse_mean:  22244.80","iter: 1478<br />test_rmse_mean:  22245.74","iter: 1479<br />test_rmse_mean:  22247.30","iter: 1480<br />test_rmse_mean:  22248.07","iter: 1481<br />test_rmse_mean:  22250.45","iter: 1482<br />test_rmse_mean:  22251.48","iter: 1483<br />test_rmse_mean:  22250.78","iter: 1484<br />test_rmse_mean:  22251.49","iter: 1485<br />test_rmse_mean:  22249.44","iter: 1486<br />test_rmse_mean:  22246.63","iter: 1487<br />test_rmse_mean:  22243.18","iter: 1488<br />test_rmse_mean:  22245.56","iter: 1489<br />test_rmse_mean:  22244.73","iter: 1490<br />test_rmse_mean:  22244.10","iter: 1491<br />test_rmse_mean:  22244.62","iter: 1492<br />test_rmse_mean:  22243.81","iter: 1493<br />test_rmse_mean:  22241.72","iter: 1494<br />test_rmse_mean:  22241.62","iter: 1495<br />test_rmse_mean:  22239.20","iter: 1496<br />test_rmse_mean:  22239.16","iter: 1497<br />test_rmse_mean:  22239.95","iter: 1498<br />test_rmse_mean:  22238.94","iter: 1499<br />test_rmse_mean:  22243.99","iter: 1500<br />test_rmse_mean:  22245.93","iter: 1501<br />test_rmse_mean:  22243.56","iter: 1502<br />test_rmse_mean:  22246.86","iter: 1503<br />test_rmse_mean:  22246.23","iter: 1504<br />test_rmse_mean:  22247.17","iter: 1505<br />test_rmse_mean:  22246.69","iter: 1506<br />test_rmse_mean:  22247.60","iter: 1507<br />test_rmse_mean:  22247.56","iter: 1508<br />test_rmse_mean:  22245.36","iter: 1509<br />test_rmse_mean:  22245.39","iter: 1510<br />test_rmse_mean:  22244.28","iter: 1511<br />test_rmse_mean:  22240.94","iter: 1512<br />test_rmse_mean:  22240.87","iter: 1513<br />test_rmse_mean:  22236.89","iter: 1514<br />test_rmse_mean:  22238.53","iter: 1515<br />test_rmse_mean:  22238.92","iter: 1516<br />test_rmse_mean:  22235.01","iter: 1517<br />test_rmse_mean:  22234.53","iter: 1518<br />test_rmse_mean:  22234.49","iter: 1519<br />test_rmse_mean:  22230.62","iter: 1520<br />test_rmse_mean:  22228.53","iter: 1521<br />test_rmse_mean:  22227.79","iter: 1522<br />test_rmse_mean:  22226.58","iter: 1523<br />test_rmse_mean:  22225.70","iter: 1524<br />test_rmse_mean:  22225.80","iter: 1525<br />test_rmse_mean:  22224.63","iter: 1526<br />test_rmse_mean:  22228.36","iter: 1527<br />test_rmse_mean:  22228.37","iter: 1528<br />test_rmse_mean:  22228.88","iter: 1529<br />test_rmse_mean:  22225.65","iter: 1530<br />test_rmse_mean:  22226.01","iter: 1531<br />test_rmse_mean:  22226.37","iter: 1532<br />test_rmse_mean:  22224.21","iter: 1533<br />test_rmse_mean:  22223.23","iter: 1534<br />test_rmse_mean:  22222.98","iter: 1535<br />test_rmse_mean:  22225.14","iter: 1536<br />test_rmse_mean:  22225.26","iter: 1537<br />test_rmse_mean:  22225.81","iter: 1538<br />test_rmse_mean:  22224.78","iter: 1539<br />test_rmse_mean:  22221.45","iter: 1540<br />test_rmse_mean:  22220.74","iter: 1541<br />test_rmse_mean:  22217.90","iter: 1542<br />test_rmse_mean:  22219.42","iter: 1543<br />test_rmse_mean:  22219.75","iter: 1544<br />test_rmse_mean:  22219.08","iter: 1545<br />test_rmse_mean:  22217.90","iter: 1546<br />test_rmse_mean:  22218.14","iter: 1547<br />test_rmse_mean:  22218.47","iter: 1548<br />test_rmse_mean:  22218.75","iter: 1549<br />test_rmse_mean:  22221.18","iter: 1550<br />test_rmse_mean:  22223.87","iter: 1551<br />test_rmse_mean:  22222.36","iter: 1552<br />test_rmse_mean:  22221.79","iter: 1553<br />test_rmse_mean:  22221.43","iter: 1554<br />test_rmse_mean:  22220.65","iter: 1555<br />test_rmse_mean:  22218.08","iter: 1556<br />test_rmse_mean:  22218.15","iter: 1557<br />test_rmse_mean:  22218.60","iter: 1558<br />test_rmse_mean:  22218.62","iter: 1559<br />test_rmse_mean:  22218.61","iter: 1560<br />test_rmse_mean:  22216.10","iter: 1561<br />test_rmse_mean:  22214.37","iter: 1562<br />test_rmse_mean:  22215.21","iter: 1563<br />test_rmse_mean:  22211.61","iter: 1564<br />test_rmse_mean:  22210.97","iter: 1565<br />test_rmse_mean:  22211.22","iter: 1566<br />test_rmse_mean:  22209.54","iter: 1567<br />test_rmse_mean:  22205.72","iter: 1568<br />test_rmse_mean:  22208.77","iter: 1569<br />test_rmse_mean:  22208.43","iter: 1570<br />test_rmse_mean:  22209.26","iter: 1571<br />test_rmse_mean:  22206.01","iter: 1572<br />test_rmse_mean:  22205.42","iter: 1573<br />test_rmse_mean:  22203.61","iter: 1574<br />test_rmse_mean:  22201.58","iter: 1575<br />test_rmse_mean:  22202.47","iter: 1576<br />test_rmse_mean:  22203.17","iter: 1577<br />test_rmse_mean:  22202.34","iter: 1578<br />test_rmse_mean:  22203.72","iter: 1579<br />test_rmse_mean:  22201.93","iter: 1580<br />test_rmse_mean:  22203.13","iter: 1581<br />test_rmse_mean:  22203.04","iter: 1582<br />test_rmse_mean:  22201.79","iter: 1583<br />test_rmse_mean:  22201.57","iter: 1584<br />test_rmse_mean:  22201.61","iter: 1585<br />test_rmse_mean:  22198.44","iter: 1586<br />test_rmse_mean:  22198.32","iter: 1587<br />test_rmse_mean:  22198.18","iter: 1588<br />test_rmse_mean:  22197.57","iter: 1589<br />test_rmse_mean:  22196.31","iter: 1590<br />test_rmse_mean:  22194.55","iter: 1591<br />test_rmse_mean:  22194.20","iter: 1592<br />test_rmse_mean:  22197.21","iter: 1593<br />test_rmse_mean:  22195.68","iter: 1594<br />test_rmse_mean:  22197.16","iter: 1595<br />test_rmse_mean:  22198.19","iter: 1596<br />test_rmse_mean:  22197.02","iter: 1597<br />test_rmse_mean:  22195.94","iter: 1598<br />test_rmse_mean:  22198.32","iter: 1599<br />test_rmse_mean:  22201.10","iter: 1600<br />test_rmse_mean:  22199.89","iter: 1601<br />test_rmse_mean:  22201.99","iter: 1602<br />test_rmse_mean:  22200.63","iter: 1603<br />test_rmse_mean:  22200.30","iter: 1604<br />test_rmse_mean:  22201.77","iter: 1605<br />test_rmse_mean:  22203.21","iter: 1606<br />test_rmse_mean:  22197.59","iter: 1607<br />test_rmse_mean:  22199.46","iter: 1608<br />test_rmse_mean:  22198.74","iter: 1609<br />test_rmse_mean:  22199.15","iter: 1610<br />test_rmse_mean:  22201.15","iter: 1611<br />test_rmse_mean:  22202.28","iter: 1612<br />test_rmse_mean:  22204.58","iter: 1613<br />test_rmse_mean:  22206.28","iter: 1614<br />test_rmse_mean:  22205.05","iter: 1615<br />test_rmse_mean:  22204.38","iter: 1616<br />test_rmse_mean:  22207.35","iter: 1617<br />test_rmse_mean:  22207.02","iter: 1618<br />test_rmse_mean:  22208.62","iter: 1619<br />test_rmse_mean:  22205.93","iter: 1620<br />test_rmse_mean:  22205.47","iter: 1621<br />test_rmse_mean:  22204.00","iter: 1622<br />test_rmse_mean:  22202.71","iter: 1623<br />test_rmse_mean:  22202.37","iter: 1624<br />test_rmse_mean:  22201.98","iter: 1625<br />test_rmse_mean:  22198.03","iter: 1626<br />test_rmse_mean:  22199.51","iter: 1627<br />test_rmse_mean:  22197.83","iter: 1628<br />test_rmse_mean:  22200.46","iter: 1629<br />test_rmse_mean:  22201.75","iter: 1630<br />test_rmse_mean:  22201.68","iter: 1631<br />test_rmse_mean:  22201.86","iter: 1632<br />test_rmse_mean:  22200.72","iter: 1633<br />test_rmse_mean:  22200.50","iter: 1634<br />test_rmse_mean:  22200.44","iter: 1635<br />test_rmse_mean:  22197.95","iter: 1636<br />test_rmse_mean:  22197.20","iter: 1637<br />test_rmse_mean:  22197.16","iter: 1638<br />test_rmse_mean:  22197.14","iter: 1639<br />test_rmse_mean:  22197.21","iter: 1640<br />test_rmse_mean:  22199.01","iter: 1641<br />test_rmse_mean:  22199.39"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-81,1723],"tickmode":"array","ticktext":["0","500","1000","1500"],"tickvals":[0,500,1000,1500],"categoryorder":"array","categoryarray":["0","500","1000","1500"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"iter","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-879.67751202,200423.17824462],"tickmode":"array","ticktext":["0","50000","100000","150000","200000"],"tickvals":[-1.13686837721616e-13,50000,100000,150000,200000],"categoryorder":"array","categoryarray":["0","50000","100000","150000","200000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"train_rmse_mean","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"2358b12420a":{"x":{},"y":{},"type":"scatter"},"235826d2350e":{"x":{},"y":{}}},"cur_data":"2358b12420a","visdat":{"2358b12420a":["function (y) ","x"],"235826d2350e":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>We can see that the training error increased. However, the error on unseen data reaches a minimum RMSE of with iterations. With simple adaptation of our parameters we managed to decrease it to some extent. At this point it should be clear that it would take incredible effort to manually compute those errors for each possible combination of parameters that would potentially decrease the error further. Luckily, there are more elegant, automated solution for it. We can create our hyperparameter search grid along with columns to dump our results in.</p>
</div>
<div id="strategy-for-tuning" class="section level3">
<h3><span class="header-section-number">14.3.4</span> [STRATEGY FOR TUNING]</h3>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="xgboost.html#cb35-1"></a><span class="co"># Hyperparameter grid</span></span>
<span id="cb35-2"><a href="xgboost.html#cb35-2"></a>hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</span>
<span id="cb35-3"><a href="xgboost.html#cb35-3"></a>  <span class="dt">eta =</span> <span class="kw">c</span>(.<span class="dv">01</span>,<span class="fl">0.3</span>),</span>
<span id="cb35-4"><a href="xgboost.html#cb35-4"></a>  <span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>), </span>
<span id="cb35-5"><a href="xgboost.html#cb35-5"></a>  <span class="dt">min_child_weight =</span> <span class="dv">3</span>,</span>
<span id="cb35-6"><a href="xgboost.html#cb35-6"></a>  <span class="dt">subsample =</span> <span class="fl">0.5</span>, </span>
<span id="cb35-7"><a href="xgboost.html#cb35-7"></a>  <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>,</span>
<span id="cb35-8"><a href="xgboost.html#cb35-8"></a>  <span class="dt">gamma =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>),</span>
<span id="cb35-9"><a href="xgboost.html#cb35-9"></a>  <span class="dt">lambda =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e-2</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>),</span>
<span id="cb35-10"><a href="xgboost.html#cb35-10"></a>  <span class="dt">alpha =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e-2</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>),</span>
<span id="cb35-11"><a href="xgboost.html#cb35-11"></a>  <span class="dt">rmse =</span> <span class="dv">0</span>,          <span class="co"># a place to dump RMSE results</span></span>
<span id="cb35-12"><a href="xgboost.html#cb35-12"></a>  <span class="dt">trees =</span> <span class="dv">0</span>          <span class="co"># a place to dump required number of trees</span></span>
<span id="cb35-13"><a href="xgboost.html#cb35-13"></a>)</span></code></pre></div>
<p>Besides those parameters we discussed, xgboost provides additional hyperparameters <code>alpha</code>, <code>gamma</code> and <code>lambda</code> that can help to constrain model complexity and reduce overfitting. We introduced them in the grid as well. With the code above we create a pretty large search grid consisting of 1960 different hyperparameter combinations to model. It is important to note that running such a grid in a loop procedure could take a couple of hours. We will create such a loop procedure to loop through and apply a xgboost model for each hyperparameter combination (1960 in our case) and finally provide us the results in the hyper_grid data frame.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="xgboost.html#cb36-1"></a><span class="co"># Grid search</span></span>
<span id="cb36-2"><a href="xgboost.html#cb36-2"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_len</span>(<span class="kw">nrow</span>(hyper_grid))) {</span>
<span id="cb36-3"><a href="xgboost.html#cb36-3"></a>  <span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb36-4"><a href="xgboost.html#cb36-4"></a>  m &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(</span>
<span id="cb36-5"><a href="xgboost.html#cb36-5"></a>    <span class="dt">data =</span> ames_x_train,</span>
<span id="cb36-6"><a href="xgboost.html#cb36-6"></a>    <span class="dt">label =</span> ames_y_train,</span>
<span id="cb36-7"><a href="xgboost.html#cb36-7"></a>    <span class="dt">nrounds =</span> <span class="dv">4000</span>,</span>
<span id="cb36-8"><a href="xgboost.html#cb36-8"></a>    <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,</span>
<span id="cb36-9"><a href="xgboost.html#cb36-9"></a>    <span class="dt">early_stopping_rounds =</span> <span class="dv">50</span>, </span>
<span id="cb36-10"><a href="xgboost.html#cb36-10"></a>    <span class="dt">nfold =</span> <span class="dv">10</span>,</span>
<span id="cb36-11"><a href="xgboost.html#cb36-11"></a>    <span class="dt">verbose =</span> <span class="dv">0</span>,</span>
<span id="cb36-12"><a href="xgboost.html#cb36-12"></a>    <span class="dt">params =</span> <span class="kw">list</span>( </span>
<span id="cb36-13"><a href="xgboost.html#cb36-13"></a>      <span class="dt">eta =</span> hyper_grid<span class="op">$</span>eta[i], </span>
<span id="cb36-14"><a href="xgboost.html#cb36-14"></a>      <span class="dt">max_depth =</span> hyper_grid<span class="op">$</span>max_depth[i],</span>
<span id="cb36-15"><a href="xgboost.html#cb36-15"></a>      <span class="dt">min_child_weight =</span> hyper_grid<span class="op">$</span>min_child_weight[i],</span>
<span id="cb36-16"><a href="xgboost.html#cb36-16"></a>      <span class="dt">subsample =</span> hyper_grid<span class="op">$</span>subsample[i],</span>
<span id="cb36-17"><a href="xgboost.html#cb36-17"></a>      <span class="dt">colsample_bytree =</span> hyper_grid<span class="op">$</span>colsample_bytree[i],</span>
<span id="cb36-18"><a href="xgboost.html#cb36-18"></a>      <span class="dt">gamma =</span> hyper_grid<span class="op">$</span>gamma[i], </span>
<span id="cb36-19"><a href="xgboost.html#cb36-19"></a>      <span class="dt">lambda =</span> hyper_grid<span class="op">$</span>lambda[i], </span>
<span id="cb36-20"><a href="xgboost.html#cb36-20"></a>      <span class="dt">alpha =</span> hyper_grid<span class="op">$</span>alpha[i]</span>
<span id="cb36-21"><a href="xgboost.html#cb36-21"></a>    ) </span>
<span id="cb36-22"><a href="xgboost.html#cb36-22"></a>  )</span>
<span id="cb36-23"><a href="xgboost.html#cb36-23"></a>  hyper_grid<span class="op">$</span>rmse[i] &lt;-<span class="st"> </span><span class="kw">min</span>(m<span class="op">$</span>evaluation_log<span class="op">$</span>test_rmse_mean)</span>
<span id="cb36-24"><a href="xgboost.html#cb36-24"></a>  hyper_grid<span class="op">$</span>trees[i] &lt;-<span class="st"> </span>m<span class="op">$</span>best_iteration</span>
<span id="cb36-25"><a href="xgboost.html#cb36-25"></a>}</span>
<span id="cb36-26"><a href="xgboost.html#cb36-26"></a></span>
<span id="cb36-27"><a href="xgboost.html#cb36-27"></a><span class="co"># Results</span></span>
<span id="cb36-28"><a href="xgboost.html#cb36-28"></a>hg &lt;-<span class="st"> </span>hyper_grid <span class="op">%&gt;%</span></span>
<span id="cb36-29"><a href="xgboost.html#cb36-29"></a><span class="st">  </span><span class="kw">filter</span>(rmse <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span></span>
<span id="cb36-30"><a href="xgboost.html#cb36-30"></a><span class="st">  </span><span class="kw">arrange</span>(rmse) <span class="op">%&gt;%</span></span>
<span id="cb36-31"><a href="xgboost.html#cb36-31"></a><span class="st">  </span><span class="kw">glimpse</span>()</span></code></pre></div>
<p>Here is a glimpse of results we obtained after several hours of processing:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="xgboost.html#cb37-1"></a><span class="co">## Observations: 98</span></span>
<span id="cb37-2"><a href="xgboost.html#cb37-2"></a><span class="co">## Variables: 10</span></span>
<span id="cb37-3"><a href="xgboost.html#cb37-3"></a><span class="co">## $ eta              &lt;dbl&gt; 0.01, 0.01, 0.01, 0.01, 0.01, 0.0…</span></span>
<span id="cb37-4"><a href="xgboost.html#cb37-4"></a><span class="co">## $ max_depth        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …</span></span>
<span id="cb37-5"><a href="xgboost.html#cb37-5"></a><span class="co">## $ min_child_weight &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …</span></span>
<span id="cb37-6"><a href="xgboost.html#cb37-6"></a><span class="co">## $ subsample        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…</span></span>
<span id="cb37-7"><a href="xgboost.html#cb37-7"></a><span class="co">## $ colsample_bytree &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…</span></span>
<span id="cb37-8"><a href="xgboost.html#cb37-8"></a><span class="co">## $ gamma            &lt;dbl&gt; 0, 1, 10, 100, 1000, 0, 1, 10, 10…</span></span>
<span id="cb37-9"><a href="xgboost.html#cb37-9"></a><span class="co">## $ lambda           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …</span></span>
<span id="cb37-10"><a href="xgboost.html#cb37-10"></a><span class="co">## $ alpha            &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.1…</span></span>
<span id="cb37-11"><a href="xgboost.html#cb37-11"></a><span class="co">## $ rmse             &lt;dbl&gt; 20488, 20488, 20488, 20488, 20488…</span></span>
<span id="cb37-12"><a href="xgboost.html#cb37-12"></a><span class="co">## $ trees            &lt;dbl&gt; 3944, 3944, 3944, 3944, 3944, 381…</span></span></code></pre></div>
<p>In the first “column” we see the combination of parameters given that results in the lowest estimated error (RMSE) possible for the combination given. Subsequently, we will use those parameters to enhance prediction performance of our model.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="xgboost.html#cb38-1"></a><span class="co"># The list of optimal hyperparameters</span></span>
<span id="cb38-2"><a href="xgboost.html#cb38-2"></a>params_optimal &lt;-<span class="st"> </span><span class="kw">list</span>(</span>
<span id="cb38-3"><a href="xgboost.html#cb38-3"></a>  <span class="dt">eta =</span> <span class="fl">0.01</span>,</span>
<span id="cb38-4"><a href="xgboost.html#cb38-4"></a>  <span class="dt">max_depth =</span> <span class="dv">3</span>,</span>
<span id="cb38-5"><a href="xgboost.html#cb38-5"></a>  <span class="dt">min_child_weight =</span> <span class="dv">3</span>,</span>
<span id="cb38-6"><a href="xgboost.html#cb38-6"></a>  <span class="dt">subsample =</span> <span class="fl">0.5</span>,</span>
<span id="cb38-7"><a href="xgboost.html#cb38-7"></a>  <span class="dt">colsample_bytree =</span> <span class="fl">0.5</span>,</span>
<span id="cb38-8"><a href="xgboost.html#cb38-8"></a>  <span class="dt">lambda =</span> <span class="dv">1</span></span>
<span id="cb38-9"><a href="xgboost.html#cb38-9"></a>)</span>
<span id="cb38-10"><a href="xgboost.html#cb38-10"></a></span>
<span id="cb38-11"><a href="xgboost.html#cb38-11"></a><span class="co"># Train final model  with optimal combination of the given parameters </span></span>
<span id="cb38-12"><a href="xgboost.html#cb38-12"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb38-13"><a href="xgboost.html#cb38-13"></a>xgb.fit.optimal &lt;-<span class="st"> </span><span class="kw">xgboost</span>(</span>
<span id="cb38-14"><a href="xgboost.html#cb38-14"></a>  <span class="dt">params =</span> params_optimal,</span>
<span id="cb38-15"><a href="xgboost.html#cb38-15"></a>  <span class="dt">data =</span> ames_x_train,</span>
<span id="cb38-16"><a href="xgboost.html#cb38-16"></a>  <span class="dt">label =</span> ames_y_train,</span>
<span id="cb38-17"><a href="xgboost.html#cb38-17"></a>  <span class="dt">nrounds =</span> <span class="dv">3944</span>,</span>
<span id="cb38-18"><a href="xgboost.html#cb38-18"></a>  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,</span>
<span id="cb38-19"><a href="xgboost.html#cb38-19"></a>  <span class="dt">verbose =</span> <span class="dv">0</span></span>
<span id="cb38-20"><a href="xgboost.html#cb38-20"></a>)</span></code></pre></div>
<pre><code>## [14:03:38] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.</code></pre>
<p>After computing the final model, we can make inferences about how features (i.e. variables in our data set besides sale price) are influencing our model. Measurement of feature importance occurs based on the sum of the reduction in the loss function (e.g. SSE) attributed to each variable at each split in a respective tree. In simpler terms, it is the relative contribution of the respective feature to the model computed by taking each feature’s contribution for each tree in the model. Therefore, those features with the highest average decrease in SSE (for regression) are identified as the one with the highest contribution. Thus, these features are among most important ones. To visualize feature importance plot we need to create importance matrix first then plot it with ggplot-based function “xgb.ggplot.importance”.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="xgboost.html#cb40-1"></a><span class="co"># Construct importance matrix</span></span>
<span id="cb40-2"><a href="xgboost.html#cb40-2"></a>importance_matrix &lt;-<span class="st"> </span><span class="kw">xgb.importance</span>(<span class="dt">model =</span> xgb.fit.optimal)</span>
<span id="cb40-3"><a href="xgboost.html#cb40-3"></a></span>
<span id="cb40-4"><a href="xgboost.html#cb40-4"></a><span class="co"># Variable importance plot with ggplot2</span></span>
<span id="cb40-5"><a href="xgboost.html#cb40-5"></a><span class="kw">xgb.ggplot.importance</span>(importance_matrix, <span class="dt">top_n =</span> <span class="dv">15</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>As we identified the most relevant features, now we can try to understand how the response variable (i.e. predicted sale price) changes based on these variables. For this we can use partial dependence plots (PDPs). They show the marginal effect one or two features have on the predicted outcome.
Let’s consider the “Gr_Liv_Area” variable. The PDP plot below displays the average change in predicted sales price as we vary “Gr_Liv_Area” while holding all other variables constant. More specifically, it shows the movement of the predicted sales price as the square footage of the ground floor in a house changes, while holding other variables constant. It is important to mention that PDPs are valid as long as the target variable (sale price) and the variable under observation (“Gr_Liv_Area”) are not correlated. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="xgboost.html#cb41-1"></a><span class="kw">library</span>(pdp)</span>
<span id="cb41-2"><a href="xgboost.html#cb41-2"></a>pdp &lt;-<span class="st"> </span>xgb.fit.optimal <span class="op">%&gt;%</span></span>
<span id="cb41-3"><a href="xgboost.html#cb41-3"></a><span class="st">  </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;gr_liv_area&quot;</span>, <span class="dt">n.trees =</span> <span class="dv">3497</span>, <span class="dt">grid.resolution =</span> <span class="dv">100</span>, <span class="dt">train =</span> ames_x_train) <span class="op">%&gt;%</span></span>
<span id="cb41-4"><a href="xgboost.html#cb41-4"></a><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_x_train) <span class="op">+</span></span>
<span id="cb41-5"><a href="xgboost.html#cb41-5"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>dollar) <span class="op">+</span></span>
<span id="cb41-6"><a href="xgboost.html#cb41-6"></a><span class="st">  </span>ggplot2<span class="op">::</span><span class="kw">xlab</span>(<span class="dt">label=</span> <span class="st">&quot;Ground floor living area&quot;</span>)<span class="op">+</span></span>
<span id="cb41-7"><a href="xgboost.html#cb41-7"></a><span class="st">  </span>ggplot2<span class="op">::</span><span class="kw">ylab</span>(<span class="dt">label=</span><span class="st">&quot;Predicted sale price&quot;</span>)<span class="op">+</span></span>
<span id="cb41-8"><a href="xgboost.html#cb41-8"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Partial dependency plot - Influence of the ground floor size on a house sale price&quot;</span>)</span>
<span id="cb41-9"><a href="xgboost.html#cb41-9"></a><span class="kw">ggplotly</span>(pdp)</span></code></pre></div>
<div id="htmlwidget-71ef8fc44f5de9ddb372" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-71ef8fc44f5de9ddb372">{"x":{"data":[{"x":[334,387.616161616162,441.232323232323,494.848484848485,548.464646464646,602.080808080808,655.69696969697,709.313131313131,762.929292929293,816.545454545455,870.161616161616,923.777777777778,977.393939393939,1031.0101010101,1084.62626262626,1138.24242424242,1191.85858585859,1245.47474747475,1299.09090909091,1352.70707070707,1406.32323232323,1459.93939393939,1513.55555555556,1567.17171717172,1620.78787878788,1674.40404040404,1728.0202020202,1781.63636363636,1835.25252525253,1888.86868686869,1942.48484848485,1996.10101010101,2049.71717171717,2103.33333333333,2156.9494949495,2210.56565656566,2264.18181818182,2317.79797979798,2371.41414141414,2425.0303030303,2478.64646464646,2532.26262626263,2585.87878787879,2639.49494949495,2693.11111111111,2746.72727272727,2800.34343434343,2853.9595959596,2907.57575757576,2961.19191919192,3014.80808080808,3068.42424242424,3122.0404040404,3175.65656565657,3229.27272727273,3282.88888888889,3336.50505050505,3390.12121212121,3443.73737373737,3497.35353535354,3550.9696969697,3604.58585858586,3658.20202020202,3711.81818181818,3765.43434343434,3819.05050505051,3872.66666666667,3926.28282828283,3979.89898989899,4033.51515151515,4087.13131313131,4140.74747474748,4194.36363636364,4247.9797979798,4301.59595959596,4355.21212121212,4408.82828282828,4462.44444444444,4516.06060606061,4569.67676767677,4623.29292929293,4676.90909090909,4730.52525252525,4784.14141414141,4837.75757575758,4891.37373737374,4944.9898989899,4998.60606060606,5052.22222222222,5105.83838383838,5159.45454545455,5213.07070707071,5266.68686868687,5320.30303030303,5373.91919191919,5427.53535353535,5481.15151515152,5534.76767676768,5588.38383838384,5642],"y":[155379.34718438,155379.34718438,155379.34718438,155379.34718438,156003.748713773,156042.96033244,156126.314511157,156237.434223545,157429.618837144,157944.958659964,159692.638897345,161647.515449951,161878.860280687,164205.222812272,164736.618146462,164961.420984535,166362.4988736,169413.312279286,169823.906830325,172217.453220135,173014.242585165,173923.381425429,178242.530985524,179022.219429265,180613.388132459,183407.14291966,184005.322656631,188222.950780869,189761.143380114,190830.420531691,193938.50996636,198585.668058177,201687.512744307,201207.891329,201118.290333506,200811.907638973,207607.389426297,210435.799550201,212571.004364802,213799.543549075,214601.924409401,212954.109375,214330.906660984,214746.4215934,221249.661573764,221985.120376431,223231.067211094,226327.745680863,226894.758261538,225477.878550444,225547.962048679,226223.93001096,229258.912544143,234632.367606095,235992.87915931,238131.704087768,237212.481791129,235969.92416966,235801.932001187,236959.735596536,240111.011321085,242360.227723149,242675.010769301,242675.010769301,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,240994.017482038,233263.239154591,233263.239154591,222827.816921883,222827.816921883,222827.816921883,222827.816921883,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584,202002.016317584],"text":["object[[1L]]:  334.0000<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  387.6162<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  441.2323<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  494.8485<br />object[[\"yhat\"]]: 155379.3","object[[1L]]:  548.4646<br />object[[\"yhat\"]]: 156003.7","object[[1L]]:  602.0808<br />object[[\"yhat\"]]: 156043.0","object[[1L]]:  655.6970<br />object[[\"yhat\"]]: 156126.3","object[[1L]]:  709.3131<br />object[[\"yhat\"]]: 156237.4","object[[1L]]:  762.9293<br />object[[\"yhat\"]]: 157429.6","object[[1L]]:  816.5455<br />object[[\"yhat\"]]: 157945.0","object[[1L]]:  870.1616<br />object[[\"yhat\"]]: 159692.6","object[[1L]]:  923.7778<br />object[[\"yhat\"]]: 161647.5","object[[1L]]:  977.3939<br />object[[\"yhat\"]]: 161878.9","object[[1L]]: 1031.0101<br />object[[\"yhat\"]]: 164205.2","object[[1L]]: 1084.6263<br />object[[\"yhat\"]]: 164736.6","object[[1L]]: 1138.2424<br />object[[\"yhat\"]]: 164961.4","object[[1L]]: 1191.8586<br />object[[\"yhat\"]]: 166362.5","object[[1L]]: 1245.4747<br />object[[\"yhat\"]]: 169413.3","object[[1L]]: 1299.0909<br />object[[\"yhat\"]]: 169823.9","object[[1L]]: 1352.7071<br />object[[\"yhat\"]]: 172217.5","object[[1L]]: 1406.3232<br />object[[\"yhat\"]]: 173014.2","object[[1L]]: 1459.9394<br />object[[\"yhat\"]]: 173923.4","object[[1L]]: 1513.5556<br />object[[\"yhat\"]]: 178242.5","object[[1L]]: 1567.1717<br />object[[\"yhat\"]]: 179022.2","object[[1L]]: 1620.7879<br />object[[\"yhat\"]]: 180613.4","object[[1L]]: 1674.4040<br />object[[\"yhat\"]]: 183407.1","object[[1L]]: 1728.0202<br />object[[\"yhat\"]]: 184005.3","object[[1L]]: 1781.6364<br />object[[\"yhat\"]]: 188223.0","object[[1L]]: 1835.2525<br />object[[\"yhat\"]]: 189761.1","object[[1L]]: 1888.8687<br />object[[\"yhat\"]]: 190830.4","object[[1L]]: 1942.4848<br />object[[\"yhat\"]]: 193938.5","object[[1L]]: 1996.1010<br />object[[\"yhat\"]]: 198585.7","object[[1L]]: 2049.7172<br />object[[\"yhat\"]]: 201687.5","object[[1L]]: 2103.3333<br />object[[\"yhat\"]]: 201207.9","object[[1L]]: 2156.9495<br />object[[\"yhat\"]]: 201118.3","object[[1L]]: 2210.5657<br />object[[\"yhat\"]]: 200811.9","object[[1L]]: 2264.1818<br />object[[\"yhat\"]]: 207607.4","object[[1L]]: 2317.7980<br />object[[\"yhat\"]]: 210435.8","object[[1L]]: 2371.4141<br />object[[\"yhat\"]]: 212571.0","object[[1L]]: 2425.0303<br />object[[\"yhat\"]]: 213799.5","object[[1L]]: 2478.6465<br />object[[\"yhat\"]]: 214601.9","object[[1L]]: 2532.2626<br />object[[\"yhat\"]]: 212954.1","object[[1L]]: 2585.8788<br />object[[\"yhat\"]]: 214330.9","object[[1L]]: 2639.4949<br />object[[\"yhat\"]]: 214746.4","object[[1L]]: 2693.1111<br />object[[\"yhat\"]]: 221249.7","object[[1L]]: 2746.7273<br />object[[\"yhat\"]]: 221985.1","object[[1L]]: 2800.3434<br />object[[\"yhat\"]]: 223231.1","object[[1L]]: 2853.9596<br />object[[\"yhat\"]]: 226327.7","object[[1L]]: 2907.5758<br />object[[\"yhat\"]]: 226894.8","object[[1L]]: 2961.1919<br />object[[\"yhat\"]]: 225477.9","object[[1L]]: 3014.8081<br />object[[\"yhat\"]]: 225548.0","object[[1L]]: 3068.4242<br />object[[\"yhat\"]]: 226223.9","object[[1L]]: 3122.0404<br />object[[\"yhat\"]]: 229258.9","object[[1L]]: 3175.6566<br />object[[\"yhat\"]]: 234632.4","object[[1L]]: 3229.2727<br />object[[\"yhat\"]]: 235992.9","object[[1L]]: 3282.8889<br />object[[\"yhat\"]]: 238131.7","object[[1L]]: 3336.5051<br />object[[\"yhat\"]]: 237212.5","object[[1L]]: 3390.1212<br />object[[\"yhat\"]]: 235969.9","object[[1L]]: 3443.7374<br />object[[\"yhat\"]]: 235801.9","object[[1L]]: 3497.3535<br />object[[\"yhat\"]]: 236959.7","object[[1L]]: 3550.9697<br />object[[\"yhat\"]]: 240111.0","object[[1L]]: 3604.5859<br />object[[\"yhat\"]]: 242360.2","object[[1L]]: 3658.2020<br />object[[\"yhat\"]]: 242675.0","object[[1L]]: 3711.8182<br />object[[\"yhat\"]]: 242675.0","object[[1L]]: 3765.4343<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3819.0505<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3872.6667<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3926.2828<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 3979.8990<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4033.5152<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4087.1313<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4140.7475<br />object[[\"yhat\"]]: 240994.0","object[[1L]]: 4194.3636<br />object[[\"yhat\"]]: 233263.2","object[[1L]]: 4247.9798<br />object[[\"yhat\"]]: 233263.2","object[[1L]]: 4301.5960<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4355.2121<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4408.8283<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4462.4444<br />object[[\"yhat\"]]: 222827.8","object[[1L]]: 4516.0606<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4569.6768<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4623.2929<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4676.9091<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4730.5253<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4784.1414<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4837.7576<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4891.3737<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4944.9899<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 4998.6061<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5052.2222<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5105.8384<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5159.4545<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5213.0707<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5266.6869<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5320.3030<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5373.9192<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5427.5354<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5481.1515<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5534.7677<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5588.3838<br />object[[\"yhat\"]]: 202002.0","object[[1L]]: 5642.0000<br />object[[\"yhat\"]]: 202002.0"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"y":[151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436,null,151014.564005133,153895.320903436],"text":"x.rug[[1L]]: NA","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":72.3287671232877},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"Partial dependency plot - Influence of the ground floor size on a house sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[68.6,5907.4],"tickmode":"array","ticktext":["1000","2000","3000","4000","5000"],"tickvals":[1000,2000,3000,4000,5000],"categoryorder":"array","categoryarray":["1000","2000","3000","4000","5000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Ground floor living area","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[151014.564005133,247039.793948547],"tickmode":"array","ticktext":["$175,000","$200,000","$225,000"],"tickvals":[175000,200000,225000],"categoryorder":"array","categoryarray":["$175,000","$200,000","$225,000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Predicted sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"23581d8f613d":{"x":{},"y":{},"type":"scatter"},"2358556253b5":{"x":{}}},"cur_data":"23581d8f613d","visdat":{"23581d8f613d":["function (y) ","x"],"2358556253b5":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>We use predict function to predict unseen observations from the test data set we created at the beginning of the chapter. Since we already know the real sale prices from the test data set, we will be able to calculate the error of our predictive model.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="xgboost.html#cb42-1"></a><span class="co"># Predict values for test data optimal</span></span>
<span id="cb42-2"><a href="xgboost.html#cb42-2"></a>pred.optimal &lt;-<span class="st"> </span><span class="kw">predict</span>(xgb.fit.optimal, ames_x_test)</span>
<span id="cb42-3"><a href="xgboost.html#cb42-3"></a></span>
<span id="cb42-4"><a href="xgboost.html#cb42-4"></a><span class="co"># Results with optimal parameters</span></span>
<span id="cb42-5"><a href="xgboost.html#cb42-5"></a><span class="kw">RMSE</span>(pred.optimal, ames_test[,<span class="dv">308</span>])</span></code></pre></div>
<pre><code>## [1] 21988.76</code></pre>
<p>Finally, we can nicely visualize predicted and actual sale price.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="xgboost.html#cb44-1"></a><span class="co"># Plot predictions vs actual sale price</span></span>
<span id="cb44-2"><a href="xgboost.html#cb44-2"></a>ames_test&lt;-<span class="st"> </span><span class="kw">cbind</span>(ames_test,pred.optimal)</span>
<span id="cb44-3"><a href="xgboost.html#cb44-3"></a>ames_test &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(ames_test)</span>
<span id="cb44-4"><a href="xgboost.html#cb44-4"></a>p&lt;-<span class="kw">ggplot</span>(ames_test, <span class="kw">aes</span>(<span class="dt">x =</span> pred.optimal, <span class="dt">y =</span> sale_price, <span class="dt">Predicted=</span>pred.optimal, <span class="dt">Tested=</span>sale_price)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb44-5"><a href="xgboost.html#cb44-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb44-6"><a href="xgboost.html#cb44-6"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">TRUE</span>)<span class="op">+</span></span>
<span id="cb44-7"><a href="xgboost.html#cb44-7"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="dt">label =</span> <span class="st">&quot;Predicted sale price&quot;</span> )<span class="op">+</span></span>
<span id="cb44-8"><a href="xgboost.html#cb44-8"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="dt">label =</span> <span class="st">&quot;Test sale price&quot;</span>)<span class="op">+</span></span>
<span id="cb44-9"><a href="xgboost.html#cb44-9"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="dt">label =</span> <span class="st">&quot;Predicted sale price vs sales price&quot;</span>)</span>
<span id="cb44-10"><a href="xgboost.html#cb44-10"></a><span class="kw">ggplotly</span>(p,<span class="dt">tooltip =</span> <span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span>,<span class="st">&quot;Tested&quot;</span>))</span></code></pre></div>
<div id="htmlwidget-0fa449bb3750d1def09c" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-0fa449bb3750d1def09c">{"x":{"data":[{"x":[166271.390625,256515.046875,199401.265625,166460.34375,225155.5,223877.625,368326.125,113401.2734375,220700.8125,140805.265625,101971.65625,98567.53125,89476.40625,143621.203125,115159.3828125,296676.5625,205599.3125,295839.09375,165820.625,177696.546875,231610.796875,171528.625,297061.90625,385805.5625,246673.6875,129554.5,141805.203125,126570.65625,186036.25,233963.21875,300572.21875,161674.59375,161428.96875,160553.375,172177.6875,147506.28125,193459.3125,173138.25,195629.140625,149522.65625,154535.84375,142006.5625,171383.8125,170539.78125,139929.09375,208535.5,134901.40625,138378.015625,259089.625,132544.890625,176060.421875,153373.171875,138372.84375,116273.5859375,127647.765625,139465.28125,104709.5234375,131107.453125,125741.515625,156840.796875,117847.5546875,132056.859375,164390.578125,123470.0390625,161293.046875,134827.515625,129198.6484375,130815.359375,120447.109375,116926.1484375,133267.515625,130772.5,148793.359375,154093.328125,265012.28125,146097.59375,128131.09375,173764.5625,252534.078125,224337.546875,154118.65625,251924.578125,193866.203125,138395.625,155960.140625,130855.8984375,308689.5,144450.265625,182126.328125,97740.265625,109675.2109375,88958.640625,85920.1796875,124000.265625,62582.0234375,164724.71875,164124.09375,181570.296875,95982.765625,91794.1875,85455.2421875,164045.59375,159036.984375,187896.5625,224692.96875,92540.296875,127217.6015625,79107.6875,168133.828125,164828.796875,181321.140625,188156.171875,161093.484375,478695.53125,353490.5,189160.1875,166938.9375,190210.890625,180141.015625,182451.46875,158273.21875,142248.828125,190414.203125,233005.375,150057.609375,100398.546875,115107.2421875,121091.1640625,132085.9375,141957.75,138278.28125,139542.515625,142017.046875,165697.859375,340862.75,503946.3125,468924.28125,320431.21875,258773.25,371853.1875,503169.375,320362.875,489410.8125,306309.28125,195915.296875,198249.59375,207977.875,398985,243931.015625,214058.96875,169462.640625,178346.203125,189741.09375,192407.265625,207183.234375,185499.71875,218535.53125,301066.09375,313467.21875,259583.390625,253427.421875,252730.234375,215119.25,356741.03125,225461.875,231481.078125,182105.09375,136632.390625,201509.34375,158813.15625,215147.296875,197347.15625,194086.59375,131761.453125,164726.296875,69414.171875,119315.7890625,166612.34375,201669.0625,159032.140625,202200.453125,151330.1875,148461.140625,158918.796875,219082.546875,192936.171875,131355.109375,221285.359375,167833.34375,148635.46875,193297.296875,140437.875,117221.90625,127239.8515625,145058.78125,122045.5546875,165999.765625,226345.25,147813.4375,127404.1328125,139694.546875,180507.359375,131020.4609375,134795.765625,123721.6171875,129414.8671875,100391.125,122471.125,147543.84375,162698.28125,159235.90625,60427.6015625,87917.2265625,204091.546875,198683.046875,151526.640625,182105.109375,93445.7890625,120115.5234375,139347.640625,136876.578125,106529.015625,109731.359375,92143.75,96223.5703125,112303.296875,141219,51007.2109375,134826.3125,123963.3984375,108967.921875,178929.125,136449.78125,168397,144247.25,82399.0546875,75996.84375,142203.390625,139473.875,108788.0078125,91823.2578125,139814.046875,147576.234375,109972.8046875,196445.25,219606.859375,170327.296875,130373.71875,262348.28125,311990.75,340165.6875,282119.71875,266918.75,233105.8125,155296.296875,184340.03125,216493.921875,215416.765625,182522.921875,124220.234375,132183.296875,159623.46875,135198.765625,156089.53125,132050.453125,130047.8359375,139629.640625,236420.640625,312305.375,189984.234375,201520.609375,271926.875,189037.640625,177784.484375,108379.1796875,120775.1875,112632.0703125,102262.109375,81681.0625,124657.1484375,130118.8671875,116582.8125,147097.328125,118145.3359375,189893.96875,246364.109375,175892.859375,138325.578125,226791.0625,122915.21875,290950.15625,48078.31640625,244371.9375,233353.75,214170.640625,166382.53125,403252,330932.5625,220482.640625,234959.84375,134576.359375,160459.640625,97883.7265625,134661.59375,122597.828125,146224.25,200921.40625,169677.890625,204703,177710.953125,175968.984375,167880.765625,186159.234375,195615.21875,335472.25,323513.46875,369938.34375,184427.578125,182198.40625,148816.078125,319800.5625,140437.90625,123542.3515625,172410.59375,94020.921875,102221.5078125,357636.59375,465650.78125,363907.96875,278747.40625,394274.53125,366574.0625,204926.203125,182375.75,202610.75,235066.40625,192037.4375,182320.234375,219121.328125,373596.6875,268824.75,207962.921875,215675.34375,256638.078125,313613.125,249111.125,206215.15625,178793.265625,182867.125,203029.46875,198493.984375,169776.34375,161832.765625,136664.453125,130733.6328125,182827.25,137692.25,354421.65625,163707.765625,253430.640625,273379.15625,229426.8125,333305.3125,203135.5,203774.3125,152387.125,145780.234375,316436.46875,166200.921875,175294.734375,119723.2578125,146908.796875,159910.203125,155847.875,129731.25,150280.796875,145198,121750.8046875,219622.171875,156965.921875,231819.140625,160991.9375,133308.84375,115209.1796875,107105.4296875,104116.328125,126222.2578125,115286.0234375,77970.46875,163085.8125,132251.140625,115321.578125,110804.40625,123665.3515625,126486.6328125,131926.890625,77038.4296875,130729.265625,103064.7421875,148071.578125,134216.515625,53917.5390625,109783.65625,102840.6015625,118769.671875,90140.6171875,139457.46875,152949.34375,131515.109375,117237.59375,73413.4140625,195672.265625,145431.46875,141225.109375,107372.2265625,147150.421875,123158.0859375,115735.453125,78487.421875,151174.1875,194004.328125,123368.1328125,117507.1484375,116050.8515625,125682.953125,105146.2734375,120799.9921875,104629.3828125,116881.5234375,115892,139384.265625,147891.09375,176121.40625,106935.6171875,152457.8125,160460.34375,199565.453125,165896.375,148154.28125,207830.390625,118626.2890625,244537.421875,223760.90625,175129.125,241441.8125,199805.609375,208373.625,186915.984375,226950.734375,210998.578125,154750.46875,121598.5703125,235729.4375,263923.96875,279993.09375,233299.4375,189527.359375,141543.5625,202388.125,148372.828125,118823.03125,184482.921875,140780.609375,76623.125,119225.5546875,86211.46875,189470.84375,156046.109375,143180.640625,118756.875,176997.671875,172568.703125,126951.5234375,257460.75,133333.6875,110698.5234375,295261.28125,269972.6875,212724.765625,204302.671875,237609.828125,211695.265625,281012.4375,263084.5,250197.53125,166674.453125,219817.5625,231291.359375,325964.1875,269582.90625,330993.6875,128758.84375,109638.28125,140177.34375,112310.171875,85464.6484375,142182.59375,132086.171875,196387.6875,170528.4375,160415.046875,162803.828125,343199.78125,236800,379497.125,195962.9375,147521.78125,180318.953125,143471.78125,119410.6015625,116043.015625,104776.046875,140681.890625,371284.75,381954.84375,271368.875,286709.875,320856.53125,317895.9375,496971.28125,283634.125,298690.375,292908.6875,254474.4375,187748.140625,190408.890625,189984.953125,196941.734375,221516.984375,279625.9375,300599.0625,179451.796875,176547.671875,176663.984375,220071.8125,194520.3125,170503.3125,197813.8125,237195.390625,244479.171875,180147.53125,169322.0625,207027.6875,567872,313987.0625,296083.4375,399155.46875,313675.9375,341440.84375,234843.046875,145826.1875,186825.625,190970.40625,162419.734375,135268.796875,124727.578125,275641.875,142164,309569.65625,187713.859375,230297.609375,192966.28125,219302.25,176008.34375,255229.71875,277408.59375,187596.171875,201146.375,175574.6875,193799.75,239388.21875,162650.765625,193892.25,244522.3125,148426.390625,132863.703125,140861.328125,133945.609375,128057.5625,151114.390625,143514.5625,64417.50390625,131830.34375,128350.4921875,127487.4765625,155927.671875,135072.375,136356.671875,132716.484375,162981.75,144008.09375,140032.078125,123294.1875,148176.125,130698.703125,282161.84375,114934.5234375,129396.796875,106450.2265625,107172.8671875,145042.578125,127731.390625,111664.625,89753.671875,113267.390625,126494.28125,92214.65625,131224.984375,107158.046875,119548.5625,146179.78125,74055.3515625,154805.640625,130022.578125,125243.8046875,125761.78125,129285.640625,258356.15625,114463.5078125,113479.6015625,159887.75,124717.3984375,115747.640625,127322.96875,117471.234375,124556.890625,130945.359375,169585.34375,154066.40625,156753.296875,152819.296875,147973.375,81538.0625,213754.578125,107679.65625,146621.140625,94879.8125,137719.796875,95878.4375,181420.796875,195153.4375,280939.03125,223674.28125,186907.65625,259747.109375,206188.21875,215375.484375,178614.390625,282516,213788.125,208440.1875,186690.90625,193961.15625,234467.828125,142548.484375,189085.640625,182018.828125,177825.734375,175121.203125,131258.671875,135809.046875,169713.71875,143398.140625,125783.5234375,89397.53125,105047.1484375,140787.625,142991.546875,196122.609375,144291.375,294967.96875,209514.734375,122692.0546875,201533.96875,125568.1640625,134748.765625,165395.859375,129255.4296875,61046.1953125,287144.28125,168954.125,140323.5,220513.375,201368.5625,232433.21875,243802.921875,172565.671875,338061.84375,295648.8125,83466.5703125,87363.3203125,78508.9921875,185133.03125,131600.703125,126639.1015625,180214.734375,183871.640625,294832.8125,241543.6875,241958.046875,179477.046875,425047.34375,493195.03125,460147,406190.4375,180380.96875,316014.84375,429645.78125,241540.609375,213910.890625,295853.28125,168216.671875,267411.53125,176933.0625,151952.46875,133082.515625,107971.296875,94553.25,98655.421875,153615.0625,423206.8125,244889.671875,275170,404508.65625,313883.90625,274664.5625,365766.3125,303676.28125,358861.0625,190505.53125,344049.5625,179210.875,177157.71875,171499.90625,284833.90625,195476.65625,273309.8125,191171.015625,326439.84375,328552.53125,255487.875,297300.78125,192205.046875,137542.78125,145061.875,154506.71875,133579.90625,116389.3671875,129590.65625,314293.21875,258662.609375,153029.65625,168997.03125,145095.328125,203091.140625,190436.734375,132457.4375,141434.828125,160205.609375,192404.46875,121600.671875,156067.8125,127062.53125,125856.703125,140398.90625,124180.9609375,280587.03125,145892.671875,141591.78125,138770.09375,217428.859375,146333.734375,297376.125,106922.6015625,170570.546875,146087.578125,115117.4375,145515.828125,115143.765625,186823.3125,153022.21875,278870.75,211741.34375,131048.6875,77560.8984375,89190.0546875,136220.3125,137051.03125,117955.5078125,172122.046875,139933.859375,167321.640625,101342.1796875,131426.484375,94333.6953125,116663.3203125,169587.90625,149415.78125,144640,64047.87890625,152577.09375,125785.921875,97703.3359375,129268.6328125,97724.7421875,103458.328125,214828.0625,124432.515625,142277.40625,132651.640625,166702.75,158844.125,133345.5,134796.203125,164449.96875,161876.109375,132920.96875,143849.046875,137230.46875,131140.140625,226274.890625,235905.265625,256967.265625,239647.78125,267880.34375,215708.734375,223279.71875,151737.609375,194959.125,210324.390625,221314.578125,225817.40625,133013.390625,150876.921875,135448.15625,277078.28125,255182.09375,215176.59375,239893.953125,237239.84375,143125.5,147443.03125,240000.359375,196979.015625,117273.5078125,140370.90625,146548.109375,142440.8125,142487.90625,98286.46875,97476.6640625,133302.34375,120465.03125,97600.9296875,107850.921875,117131.5625,145921.828125,193662,255313.359375,197672.484375,229106.8125,242246.96875,94497.4765625,45369.69140625,91805.28125,67834.953125,222752.0625,171263.265625,214494.75,194313.9375,317802.71875,202277.796875,160477.1875,78144.109375,167069.578125,143612.40625,208354.78125],"y":[172000,244000,195500,180400,212000,164000,394432,141000,210000,142000,115000,105500,88000,149900,120000,306000,214000,319900,175500,199500,216500,180000,290000,410000,271500,99500,138500,133000,169000,190000,362500,155000,149500,152000,177500,147110,206000,218500,212500,142250,143000,136300,180500,84900,142125,197600,116500,132000,180000,136000,165500,167500,108538,108000,135000,109000,107500,129000,97500,155000,115000,130000,129000,100000,150000,128500,128000,132000,123900,109500,114900,131500,154000,163000,270000,124000,127000,186000,218000,236000,147000,245350,187000,138500,150000,128200,318000,143000,185500,155891,100000,64000,80000,128000,58500,127000,160000,185000,102776,55993,50138,190000,169900,170000,214900,83500,119500,75500,159000,157000,185000,181316,174000,501837,372500,185000,181000,154000,200000,184000,157000,152000,197500,240900,165000,97000,118000,119500,143750,148500,123000,147000,137900,148800,337500,485000,555000,325000,256300,398800,610000,296000,445000,290000,196000,184500,230000,382500,248500,254000,184000,174000,188500,184100,207500,181000,214000,265000,260000,263550,257500,287090,225000,370878,238500,263000,159000,143500,193000,153000,224243,189000,171500,120000,162000,76000,122000,164500,195000,172500,180500,150000,154000,185000,206000,197900,113000,213250,172500,154000,177500,124500,122000,128900,140000,124000,159000,256000,155000,120000,153000,176000,135000,131000,126000,129900,99900,135000,149000,142900,156500,59000,78500,190000,200000,153000,157500,92900,139000,132500,127000,94550,93000,80000,91300,110000,124900,34900,149000,119000,115000,214500,155000,155000,179900,62500,63000,149900,137000,122000,113000,139500,131000,105000,213000,239900,131000,147983,269500,297000,332000,272500,239000,221800,145000,195000,227000,230000,187100,124000,140000,150500,136500,143500,133500,133900,133000,250000,313000,198500,211000,279500,191000,178000,100000,127000,118000,85000,99900,119900,103500,160000,139500,105000,177000,234000,205000,154900,224000,121000,230000,57625,251000,240000,215000,152000,410000,316500,201000,213500,139500,162000,86000,131250,112000,130000,173000,165000,192000,180000,181000,183000,185000,189000,355000,325000,387000,175500,181900,175000,306000,133000,123000,181000,103400,100500,394617,417500,379000,250000,412500,421250,219500,154000,207000,195000,191000,179000,226750,350000,264132,227680,182000,250000,339750,256000,207500,192500,182000,193500,189000,179200,153900,135000,142000,192000,140000,372500,179400,290000,305000,217500,150000,205000,215000,147000,135000,299800,173000,178400,135750,155000,180500,174900,140000,145500,142000,119000,201800,165000,227000,163000,133000,116000,130000,110000,100000,118500,89900,171000,138000,112500,105500,127500,136870,122600,64000,139500,95000,147000,140000,46500,107900,65000,132000,98000,129400,163000,128000,116900,55000,184000,138000,108000,79500,153000,105000,113000,81300,162900,207000,130000,127500,120000,127500,89500,125000,79900,124000,109900,145000,153000,185000,108000,152400,144000,241500,177000,155000,235000,125000,262280,225000,177439,248500,207500,193000,188000,221000,231500,158000,127000,230000,287000,274000,240000,183000,136500,210000,149300,108000,165400,148000,82500,99000,98000,179500,136500,168000,130000,161900,177000,110000,263400,126000,99500,392500,271000,213000,228500,228950,241500,287000,294000,264966,167500,218689,195000,305000,298751,370000,124000,115000,152500,98000,81000,138000,103000,225000,168000,160000,160000,349265,392000,441929,192000,148000,197000,152000,123000,120500,113500,142500,356000,314813,318000,322400,318000,338931,479069,260116,317000,285000,250000,194700,204000,200000,207000,209500,277500,318061,178900,168165,171925,198444,181134,156932,172500,226500,259000,188500,165500,211000,745000,322500,290000,419005,147000,311872,250000,147000,181000,201000,179000,128000,125500,300000,129000,285000,166000,193800,208900,207500,177000,239000,301000,194000,213750,187000,190000,226000,164000,188500,255000,156000,139900,151500,145000,124000,140500,147000,64000,137000,105000,126000,175000,144000,141000,120000,163500,142000,134500,115000,153000,135000,301600,109000,128500,64500,102000,152000,141000,89471,108500,114000,124500,104500,137500,102000,90000,153575,113000,169500,139400,127000,128500,122000,200500,126000,118500,165000,123000,108000,119900,115000,134500,129000,112000,165250,150000,137000,130000,109900,243000,118000,150000,86000,130000,96000,228500,179900,301000,220000,200141,246500,203000,212999,176432,277000,187500,204750,190550,200000,211000,131500,185000,179400,168675,167000,118500,133500,171500,140000,131750,79000,110000,148500,139000,238000,140000,315000,215000,123900,170000,115000,129850,150909,118400,68104,375000,183500,134000,245000,210000,244000,267300,174000,392000,281213,85500,93900,75190,196000,129500,129500,192100,185000,320000,250000,274725,165600,457347,545224,535000,438780,169000,318000,470000,235000,241500,250000,179900,294323,181000,155000,138800,97500,91500,89000,145000,412083,252000,293000,415000,300000,275500,345000,298236,360000,202500,332200,169990,169985,172785,258000,234000,264561,173000,350000,321000,252000,290000,186500,132000,142500,150000,125000,116000,137000,310000,262000,147400,183900,139000,200000,170000,135500,157500,146000,190000,129900,157500,140000,127000,146500,121000,235000,143000,145250,156000,167000,159000,180000,105000,159000,147000,115000,159500,120000,183000,157500,277500,207500,147500,135000,87500,146000,120000,124000,169000,156500,178000,105000,111500,108000,96900,155000,144000,157000,64500,159000,114500,88000,149000,89000,109000,220000,116500,148000,142500,180000,156500,157000,145000,168000,164000,130000,142500,136900,149900,209000,221500,233555,260000,294900,209700,220000,145000,193000,217000,217000,205000,132500,157500,128500,275000,230000,210900,274300,216837,133000,155900,233230,203160,98000,145000,140000,130000,137500,92000,107000,104900,125000,121000,128000,102000,131000,140000,250000,218000,239000,257000,102000,72000,106500,78000,228000,176000,250000,202000,312500,215000,164000,71000,131000,142500,188000],"text":["pred.optimal: 166271.39<br />sale_price: 172000","pred.optimal: 256515.05<br />sale_price: 244000","pred.optimal: 199401.27<br />sale_price: 195500","pred.optimal: 166460.34<br />sale_price: 180400","pred.optimal: 225155.50<br />sale_price: 212000","pred.optimal: 223877.62<br />sale_price: 164000","pred.optimal: 368326.12<br />sale_price: 394432","pred.optimal: 113401.27<br />sale_price: 141000","pred.optimal: 220700.81<br />sale_price: 210000","pred.optimal: 140805.27<br />sale_price: 142000","pred.optimal: 101971.66<br />sale_price: 115000","pred.optimal:  98567.53<br />sale_price: 105500","pred.optimal:  89476.41<br />sale_price:  88000","pred.optimal: 143621.20<br />sale_price: 149900","pred.optimal: 115159.38<br />sale_price: 120000","pred.optimal: 296676.56<br />sale_price: 306000","pred.optimal: 205599.31<br />sale_price: 214000","pred.optimal: 295839.09<br />sale_price: 319900","pred.optimal: 165820.62<br />sale_price: 175500","pred.optimal: 177696.55<br />sale_price: 199500","pred.optimal: 231610.80<br />sale_price: 216500","pred.optimal: 171528.62<br />sale_price: 180000","pred.optimal: 297061.91<br />sale_price: 290000","pred.optimal: 385805.56<br />sale_price: 410000","pred.optimal: 246673.69<br />sale_price: 271500","pred.optimal: 129554.50<br />sale_price:  99500","pred.optimal: 141805.20<br />sale_price: 138500","pred.optimal: 126570.66<br />sale_price: 133000","pred.optimal: 186036.25<br />sale_price: 169000","pred.optimal: 233963.22<br />sale_price: 190000","pred.optimal: 300572.22<br />sale_price: 362500","pred.optimal: 161674.59<br />sale_price: 155000","pred.optimal: 161428.97<br />sale_price: 149500","pred.optimal: 160553.38<br />sale_price: 152000","pred.optimal: 172177.69<br />sale_price: 177500","pred.optimal: 147506.28<br />sale_price: 147110","pred.optimal: 193459.31<br />sale_price: 206000","pred.optimal: 173138.25<br />sale_price: 218500","pred.optimal: 195629.14<br />sale_price: 212500","pred.optimal: 149522.66<br />sale_price: 142250","pred.optimal: 154535.84<br />sale_price: 143000","pred.optimal: 142006.56<br />sale_price: 136300","pred.optimal: 171383.81<br />sale_price: 180500","pred.optimal: 170539.78<br />sale_price:  84900","pred.optimal: 139929.09<br />sale_price: 142125","pred.optimal: 208535.50<br />sale_price: 197600","pred.optimal: 134901.41<br />sale_price: 116500","pred.optimal: 138378.02<br />sale_price: 132000","pred.optimal: 259089.62<br />sale_price: 180000","pred.optimal: 132544.89<br />sale_price: 136000","pred.optimal: 176060.42<br />sale_price: 165500","pred.optimal: 153373.17<br />sale_price: 167500","pred.optimal: 138372.84<br />sale_price: 108538","pred.optimal: 116273.59<br />sale_price: 108000","pred.optimal: 127647.77<br />sale_price: 135000","pred.optimal: 139465.28<br />sale_price: 109000","pred.optimal: 104709.52<br />sale_price: 107500","pred.optimal: 131107.45<br />sale_price: 129000","pred.optimal: 125741.52<br />sale_price:  97500","pred.optimal: 156840.80<br />sale_price: 155000","pred.optimal: 117847.55<br />sale_price: 115000","pred.optimal: 132056.86<br />sale_price: 130000","pred.optimal: 164390.58<br />sale_price: 129000","pred.optimal: 123470.04<br />sale_price: 100000","pred.optimal: 161293.05<br />sale_price: 150000","pred.optimal: 134827.52<br />sale_price: 128500","pred.optimal: 129198.65<br />sale_price: 128000","pred.optimal: 130815.36<br />sale_price: 132000","pred.optimal: 120447.11<br />sale_price: 123900","pred.optimal: 116926.15<br />sale_price: 109500","pred.optimal: 133267.52<br />sale_price: 114900","pred.optimal: 130772.50<br />sale_price: 131500","pred.optimal: 148793.36<br />sale_price: 154000","pred.optimal: 154093.33<br />sale_price: 163000","pred.optimal: 265012.28<br />sale_price: 270000","pred.optimal: 146097.59<br />sale_price: 124000","pred.optimal: 128131.09<br />sale_price: 127000","pred.optimal: 173764.56<br />sale_price: 186000","pred.optimal: 252534.08<br />sale_price: 218000","pred.optimal: 224337.55<br />sale_price: 236000","pred.optimal: 154118.66<br />sale_price: 147000","pred.optimal: 251924.58<br />sale_price: 245350","pred.optimal: 193866.20<br />sale_price: 187000","pred.optimal: 138395.62<br />sale_price: 138500","pred.optimal: 155960.14<br />sale_price: 150000","pred.optimal: 130855.90<br />sale_price: 128200","pred.optimal: 308689.50<br />sale_price: 318000","pred.optimal: 144450.27<br />sale_price: 143000","pred.optimal: 182126.33<br />sale_price: 185500","pred.optimal:  97740.27<br />sale_price: 155891","pred.optimal: 109675.21<br />sale_price: 100000","pred.optimal:  88958.64<br />sale_price:  64000","pred.optimal:  85920.18<br />sale_price:  80000","pred.optimal: 124000.27<br />sale_price: 128000","pred.optimal:  62582.02<br />sale_price:  58500","pred.optimal: 164724.72<br />sale_price: 127000","pred.optimal: 164124.09<br />sale_price: 160000","pred.optimal: 181570.30<br />sale_price: 185000","pred.optimal:  95982.77<br />sale_price: 102776","pred.optimal:  91794.19<br />sale_price:  55993","pred.optimal:  85455.24<br />sale_price:  50138","pred.optimal: 164045.59<br />sale_price: 190000","pred.optimal: 159036.98<br />sale_price: 169900","pred.optimal: 187896.56<br />sale_price: 170000","pred.optimal: 224692.97<br />sale_price: 214900","pred.optimal:  92540.30<br />sale_price:  83500","pred.optimal: 127217.60<br />sale_price: 119500","pred.optimal:  79107.69<br />sale_price:  75500","pred.optimal: 168133.83<br />sale_price: 159000","pred.optimal: 164828.80<br />sale_price: 157000","pred.optimal: 181321.14<br />sale_price: 185000","pred.optimal: 188156.17<br />sale_price: 181316","pred.optimal: 161093.48<br />sale_price: 174000","pred.optimal: 478695.53<br />sale_price: 501837","pred.optimal: 353490.50<br />sale_price: 372500","pred.optimal: 189160.19<br />sale_price: 185000","pred.optimal: 166938.94<br />sale_price: 181000","pred.optimal: 190210.89<br />sale_price: 154000","pred.optimal: 180141.02<br />sale_price: 200000","pred.optimal: 182451.47<br />sale_price: 184000","pred.optimal: 158273.22<br />sale_price: 157000","pred.optimal: 142248.83<br />sale_price: 152000","pred.optimal: 190414.20<br />sale_price: 197500","pred.optimal: 233005.38<br />sale_price: 240900","pred.optimal: 150057.61<br />sale_price: 165000","pred.optimal: 100398.55<br />sale_price:  97000","pred.optimal: 115107.24<br />sale_price: 118000","pred.optimal: 121091.16<br />sale_price: 119500","pred.optimal: 132085.94<br />sale_price: 143750","pred.optimal: 141957.75<br />sale_price: 148500","pred.optimal: 138278.28<br />sale_price: 123000","pred.optimal: 139542.52<br />sale_price: 147000","pred.optimal: 142017.05<br />sale_price: 137900","pred.optimal: 165697.86<br />sale_price: 148800","pred.optimal: 340862.75<br />sale_price: 337500","pred.optimal: 503946.31<br />sale_price: 485000","pred.optimal: 468924.28<br />sale_price: 555000","pred.optimal: 320431.22<br />sale_price: 325000","pred.optimal: 258773.25<br />sale_price: 256300","pred.optimal: 371853.19<br />sale_price: 398800","pred.optimal: 503169.38<br />sale_price: 610000","pred.optimal: 320362.88<br />sale_price: 296000","pred.optimal: 489410.81<br />sale_price: 445000","pred.optimal: 306309.28<br />sale_price: 290000","pred.optimal: 195915.30<br />sale_price: 196000","pred.optimal: 198249.59<br />sale_price: 184500","pred.optimal: 207977.88<br />sale_price: 230000","pred.optimal: 398985.00<br />sale_price: 382500","pred.optimal: 243931.02<br />sale_price: 248500","pred.optimal: 214058.97<br />sale_price: 254000","pred.optimal: 169462.64<br />sale_price: 184000","pred.optimal: 178346.20<br />sale_price: 174000","pred.optimal: 189741.09<br />sale_price: 188500","pred.optimal: 192407.27<br />sale_price: 184100","pred.optimal: 207183.23<br />sale_price: 207500","pred.optimal: 185499.72<br />sale_price: 181000","pred.optimal: 218535.53<br />sale_price: 214000","pred.optimal: 301066.09<br />sale_price: 265000","pred.optimal: 313467.22<br />sale_price: 260000","pred.optimal: 259583.39<br />sale_price: 263550","pred.optimal: 253427.42<br />sale_price: 257500","pred.optimal: 252730.23<br />sale_price: 287090","pred.optimal: 215119.25<br />sale_price: 225000","pred.optimal: 356741.03<br />sale_price: 370878","pred.optimal: 225461.88<br />sale_price: 238500","pred.optimal: 231481.08<br />sale_price: 263000","pred.optimal: 182105.09<br />sale_price: 159000","pred.optimal: 136632.39<br />sale_price: 143500","pred.optimal: 201509.34<br />sale_price: 193000","pred.optimal: 158813.16<br />sale_price: 153000","pred.optimal: 215147.30<br />sale_price: 224243","pred.optimal: 197347.16<br />sale_price: 189000","pred.optimal: 194086.59<br />sale_price: 171500","pred.optimal: 131761.45<br />sale_price: 120000","pred.optimal: 164726.30<br />sale_price: 162000","pred.optimal:  69414.17<br />sale_price:  76000","pred.optimal: 119315.79<br />sale_price: 122000","pred.optimal: 166612.34<br />sale_price: 164500","pred.optimal: 201669.06<br />sale_price: 195000","pred.optimal: 159032.14<br />sale_price: 172500","pred.optimal: 202200.45<br />sale_price: 180500","pred.optimal: 151330.19<br />sale_price: 150000","pred.optimal: 148461.14<br />sale_price: 154000","pred.optimal: 158918.80<br />sale_price: 185000","pred.optimal: 219082.55<br />sale_price: 206000","pred.optimal: 192936.17<br />sale_price: 197900","pred.optimal: 131355.11<br />sale_price: 113000","pred.optimal: 221285.36<br />sale_price: 213250","pred.optimal: 167833.34<br />sale_price: 172500","pred.optimal: 148635.47<br />sale_price: 154000","pred.optimal: 193297.30<br />sale_price: 177500","pred.optimal: 140437.88<br />sale_price: 124500","pred.optimal: 117221.91<br />sale_price: 122000","pred.optimal: 127239.85<br />sale_price: 128900","pred.optimal: 145058.78<br />sale_price: 140000","pred.optimal: 122045.55<br />sale_price: 124000","pred.optimal: 165999.77<br />sale_price: 159000","pred.optimal: 226345.25<br />sale_price: 256000","pred.optimal: 147813.44<br />sale_price: 155000","pred.optimal: 127404.13<br />sale_price: 120000","pred.optimal: 139694.55<br />sale_price: 153000","pred.optimal: 180507.36<br />sale_price: 176000","pred.optimal: 131020.46<br />sale_price: 135000","pred.optimal: 134795.77<br />sale_price: 131000","pred.optimal: 123721.62<br />sale_price: 126000","pred.optimal: 129414.87<br />sale_price: 129900","pred.optimal: 100391.12<br />sale_price:  99900","pred.optimal: 122471.12<br />sale_price: 135000","pred.optimal: 147543.84<br />sale_price: 149000","pred.optimal: 162698.28<br />sale_price: 142900","pred.optimal: 159235.91<br />sale_price: 156500","pred.optimal:  60427.60<br />sale_price:  59000","pred.optimal:  87917.23<br />sale_price:  78500","pred.optimal: 204091.55<br />sale_price: 190000","pred.optimal: 198683.05<br />sale_price: 200000","pred.optimal: 151526.64<br />sale_price: 153000","pred.optimal: 182105.11<br />sale_price: 157500","pred.optimal:  93445.79<br />sale_price:  92900","pred.optimal: 120115.52<br />sale_price: 139000","pred.optimal: 139347.64<br />sale_price: 132500","pred.optimal: 136876.58<br />sale_price: 127000","pred.optimal: 106529.02<br />sale_price:  94550","pred.optimal: 109731.36<br />sale_price:  93000","pred.optimal:  92143.75<br />sale_price:  80000","pred.optimal:  96223.57<br />sale_price:  91300","pred.optimal: 112303.30<br />sale_price: 110000","pred.optimal: 141219.00<br />sale_price: 124900","pred.optimal:  51007.21<br />sale_price:  34900","pred.optimal: 134826.31<br />sale_price: 149000","pred.optimal: 123963.40<br />sale_price: 119000","pred.optimal: 108967.92<br />sale_price: 115000","pred.optimal: 178929.12<br />sale_price: 214500","pred.optimal: 136449.78<br />sale_price: 155000","pred.optimal: 168397.00<br />sale_price: 155000","pred.optimal: 144247.25<br />sale_price: 179900","pred.optimal:  82399.05<br />sale_price:  62500","pred.optimal:  75996.84<br />sale_price:  63000","pred.optimal: 142203.39<br />sale_price: 149900","pred.optimal: 139473.88<br />sale_price: 137000","pred.optimal: 108788.01<br />sale_price: 122000","pred.optimal:  91823.26<br />sale_price: 113000","pred.optimal: 139814.05<br />sale_price: 139500","pred.optimal: 147576.23<br />sale_price: 131000","pred.optimal: 109972.80<br />sale_price: 105000","pred.optimal: 196445.25<br />sale_price: 213000","pred.optimal: 219606.86<br />sale_price: 239900","pred.optimal: 170327.30<br />sale_price: 131000","pred.optimal: 130373.72<br />sale_price: 147983","pred.optimal: 262348.28<br />sale_price: 269500","pred.optimal: 311990.75<br />sale_price: 297000","pred.optimal: 340165.69<br />sale_price: 332000","pred.optimal: 282119.72<br />sale_price: 272500","pred.optimal: 266918.75<br />sale_price: 239000","pred.optimal: 233105.81<br />sale_price: 221800","pred.optimal: 155296.30<br />sale_price: 145000","pred.optimal: 184340.03<br />sale_price: 195000","pred.optimal: 216493.92<br />sale_price: 227000","pred.optimal: 215416.77<br />sale_price: 230000","pred.optimal: 182522.92<br />sale_price: 187100","pred.optimal: 124220.23<br />sale_price: 124000","pred.optimal: 132183.30<br />sale_price: 140000","pred.optimal: 159623.47<br />sale_price: 150500","pred.optimal: 135198.77<br />sale_price: 136500","pred.optimal: 156089.53<br />sale_price: 143500","pred.optimal: 132050.45<br />sale_price: 133500","pred.optimal: 130047.84<br />sale_price: 133900","pred.optimal: 139629.64<br />sale_price: 133000","pred.optimal: 236420.64<br />sale_price: 250000","pred.optimal: 312305.38<br />sale_price: 313000","pred.optimal: 189984.23<br />sale_price: 198500","pred.optimal: 201520.61<br />sale_price: 211000","pred.optimal: 271926.88<br />sale_price: 279500","pred.optimal: 189037.64<br />sale_price: 191000","pred.optimal: 177784.48<br />sale_price: 178000","pred.optimal: 108379.18<br />sale_price: 100000","pred.optimal: 120775.19<br />sale_price: 127000","pred.optimal: 112632.07<br />sale_price: 118000","pred.optimal: 102262.11<br />sale_price:  85000","pred.optimal:  81681.06<br />sale_price:  99900","pred.optimal: 124657.15<br />sale_price: 119900","pred.optimal: 130118.87<br />sale_price: 103500","pred.optimal: 116582.81<br />sale_price: 160000","pred.optimal: 147097.33<br />sale_price: 139500","pred.optimal: 118145.34<br />sale_price: 105000","pred.optimal: 189893.97<br />sale_price: 177000","pred.optimal: 246364.11<br />sale_price: 234000","pred.optimal: 175892.86<br />sale_price: 205000","pred.optimal: 138325.58<br />sale_price: 154900","pred.optimal: 226791.06<br />sale_price: 224000","pred.optimal: 122915.22<br />sale_price: 121000","pred.optimal: 290950.16<br />sale_price: 230000","pred.optimal:  48078.32<br />sale_price:  57625","pred.optimal: 244371.94<br />sale_price: 251000","pred.optimal: 233353.75<br />sale_price: 240000","pred.optimal: 214170.64<br />sale_price: 215000","pred.optimal: 166382.53<br />sale_price: 152000","pred.optimal: 403252.00<br />sale_price: 410000","pred.optimal: 330932.56<br />sale_price: 316500","pred.optimal: 220482.64<br />sale_price: 201000","pred.optimal: 234959.84<br />sale_price: 213500","pred.optimal: 134576.36<br />sale_price: 139500","pred.optimal: 160459.64<br />sale_price: 162000","pred.optimal:  97883.73<br />sale_price:  86000","pred.optimal: 134661.59<br />sale_price: 131250","pred.optimal: 122597.83<br />sale_price: 112000","pred.optimal: 146224.25<br />sale_price: 130000","pred.optimal: 200921.41<br />sale_price: 173000","pred.optimal: 169677.89<br />sale_price: 165000","pred.optimal: 204703.00<br />sale_price: 192000","pred.optimal: 177710.95<br />sale_price: 180000","pred.optimal: 175968.98<br />sale_price: 181000","pred.optimal: 167880.77<br />sale_price: 183000","pred.optimal: 186159.23<br />sale_price: 185000","pred.optimal: 195615.22<br />sale_price: 189000","pred.optimal: 335472.25<br />sale_price: 355000","pred.optimal: 323513.47<br />sale_price: 325000","pred.optimal: 369938.34<br />sale_price: 387000","pred.optimal: 184427.58<br />sale_price: 175500","pred.optimal: 182198.41<br />sale_price: 181900","pred.optimal: 148816.08<br />sale_price: 175000","pred.optimal: 319800.56<br />sale_price: 306000","pred.optimal: 140437.91<br />sale_price: 133000","pred.optimal: 123542.35<br />sale_price: 123000","pred.optimal: 172410.59<br />sale_price: 181000","pred.optimal:  94020.92<br />sale_price: 103400","pred.optimal: 102221.51<br />sale_price: 100500","pred.optimal: 357636.59<br />sale_price: 394617","pred.optimal: 465650.78<br />sale_price: 417500","pred.optimal: 363907.97<br />sale_price: 379000","pred.optimal: 278747.41<br />sale_price: 250000","pred.optimal: 394274.53<br />sale_price: 412500","pred.optimal: 366574.06<br />sale_price: 421250","pred.optimal: 204926.20<br />sale_price: 219500","pred.optimal: 182375.75<br />sale_price: 154000","pred.optimal: 202610.75<br />sale_price: 207000","pred.optimal: 235066.41<br />sale_price: 195000","pred.optimal: 192037.44<br />sale_price: 191000","pred.optimal: 182320.23<br />sale_price: 179000","pred.optimal: 219121.33<br />sale_price: 226750","pred.optimal: 373596.69<br />sale_price: 350000","pred.optimal: 268824.75<br />sale_price: 264132","pred.optimal: 207962.92<br />sale_price: 227680","pred.optimal: 215675.34<br />sale_price: 182000","pred.optimal: 256638.08<br />sale_price: 250000","pred.optimal: 313613.12<br />sale_price: 339750","pred.optimal: 249111.12<br />sale_price: 256000","pred.optimal: 206215.16<br />sale_price: 207500","pred.optimal: 178793.27<br />sale_price: 192500","pred.optimal: 182867.12<br />sale_price: 182000","pred.optimal: 203029.47<br />sale_price: 193500","pred.optimal: 198493.98<br />sale_price: 189000","pred.optimal: 169776.34<br />sale_price: 179200","pred.optimal: 161832.77<br />sale_price: 153900","pred.optimal: 136664.45<br />sale_price: 135000","pred.optimal: 130733.63<br />sale_price: 142000","pred.optimal: 182827.25<br />sale_price: 192000","pred.optimal: 137692.25<br />sale_price: 140000","pred.optimal: 354421.66<br />sale_price: 372500","pred.optimal: 163707.77<br />sale_price: 179400","pred.optimal: 253430.64<br />sale_price: 290000","pred.optimal: 273379.16<br />sale_price: 305000","pred.optimal: 229426.81<br />sale_price: 217500","pred.optimal: 333305.31<br />sale_price: 150000","pred.optimal: 203135.50<br />sale_price: 205000","pred.optimal: 203774.31<br />sale_price: 215000","pred.optimal: 152387.12<br />sale_price: 147000","pred.optimal: 145780.23<br />sale_price: 135000","pred.optimal: 316436.47<br />sale_price: 299800","pred.optimal: 166200.92<br />sale_price: 173000","pred.optimal: 175294.73<br />sale_price: 178400","pred.optimal: 119723.26<br />sale_price: 135750","pred.optimal: 146908.80<br />sale_price: 155000","pred.optimal: 159910.20<br />sale_price: 180500","pred.optimal: 155847.88<br />sale_price: 174900","pred.optimal: 129731.25<br />sale_price: 140000","pred.optimal: 150280.80<br />sale_price: 145500","pred.optimal: 145198.00<br />sale_price: 142000","pred.optimal: 121750.80<br />sale_price: 119000","pred.optimal: 219622.17<br />sale_price: 201800","pred.optimal: 156965.92<br />sale_price: 165000","pred.optimal: 231819.14<br />sale_price: 227000","pred.optimal: 160991.94<br />sale_price: 163000","pred.optimal: 133308.84<br />sale_price: 133000","pred.optimal: 115209.18<br />sale_price: 116000","pred.optimal: 107105.43<br />sale_price: 130000","pred.optimal: 104116.33<br />sale_price: 110000","pred.optimal: 126222.26<br />sale_price: 100000","pred.optimal: 115286.02<br />sale_price: 118500","pred.optimal:  77970.47<br />sale_price:  89900","pred.optimal: 163085.81<br />sale_price: 171000","pred.optimal: 132251.14<br />sale_price: 138000","pred.optimal: 115321.58<br />sale_price: 112500","pred.optimal: 110804.41<br />sale_price: 105500","pred.optimal: 123665.35<br />sale_price: 127500","pred.optimal: 126486.63<br />sale_price: 136870","pred.optimal: 131926.89<br />sale_price: 122600","pred.optimal:  77038.43<br />sale_price:  64000","pred.optimal: 130729.27<br />sale_price: 139500","pred.optimal: 103064.74<br />sale_price:  95000","pred.optimal: 148071.58<br />sale_price: 147000","pred.optimal: 134216.52<br />sale_price: 140000","pred.optimal:  53917.54<br />sale_price:  46500","pred.optimal: 109783.66<br />sale_price: 107900","pred.optimal: 102840.60<br />sale_price:  65000","pred.optimal: 118769.67<br />sale_price: 132000","pred.optimal:  90140.62<br />sale_price:  98000","pred.optimal: 139457.47<br />sale_price: 129400","pred.optimal: 152949.34<br />sale_price: 163000","pred.optimal: 131515.11<br />sale_price: 128000","pred.optimal: 117237.59<br />sale_price: 116900","pred.optimal:  73413.41<br />sale_price:  55000","pred.optimal: 195672.27<br />sale_price: 184000","pred.optimal: 145431.47<br />sale_price: 138000","pred.optimal: 141225.11<br />sale_price: 108000","pred.optimal: 107372.23<br />sale_price:  79500","pred.optimal: 147150.42<br />sale_price: 153000","pred.optimal: 123158.09<br />sale_price: 105000","pred.optimal: 115735.45<br />sale_price: 113000","pred.optimal:  78487.42<br />sale_price:  81300","pred.optimal: 151174.19<br />sale_price: 162900","pred.optimal: 194004.33<br />sale_price: 207000","pred.optimal: 123368.13<br />sale_price: 130000","pred.optimal: 117507.15<br />sale_price: 127500","pred.optimal: 116050.85<br />sale_price: 120000","pred.optimal: 125682.95<br />sale_price: 127500","pred.optimal: 105146.27<br />sale_price:  89500","pred.optimal: 120799.99<br />sale_price: 125000","pred.optimal: 104629.38<br />sale_price:  79900","pred.optimal: 116881.52<br />sale_price: 124000","pred.optimal: 115892.00<br />sale_price: 109900","pred.optimal: 139384.27<br />sale_price: 145000","pred.optimal: 147891.09<br />sale_price: 153000","pred.optimal: 176121.41<br />sale_price: 185000","pred.optimal: 106935.62<br />sale_price: 108000","pred.optimal: 152457.81<br />sale_price: 152400","pred.optimal: 160460.34<br />sale_price: 144000","pred.optimal: 199565.45<br />sale_price: 241500","pred.optimal: 165896.38<br />sale_price: 177000","pred.optimal: 148154.28<br />sale_price: 155000","pred.optimal: 207830.39<br />sale_price: 235000","pred.optimal: 118626.29<br />sale_price: 125000","pred.optimal: 244537.42<br />sale_price: 262280","pred.optimal: 223760.91<br />sale_price: 225000","pred.optimal: 175129.12<br />sale_price: 177439","pred.optimal: 241441.81<br />sale_price: 248500","pred.optimal: 199805.61<br />sale_price: 207500","pred.optimal: 208373.62<br />sale_price: 193000","pred.optimal: 186915.98<br />sale_price: 188000","pred.optimal: 226950.73<br />sale_price: 221000","pred.optimal: 210998.58<br />sale_price: 231500","pred.optimal: 154750.47<br />sale_price: 158000","pred.optimal: 121598.57<br />sale_price: 127000","pred.optimal: 235729.44<br />sale_price: 230000","pred.optimal: 263923.97<br />sale_price: 287000","pred.optimal: 279993.09<br />sale_price: 274000","pred.optimal: 233299.44<br />sale_price: 240000","pred.optimal: 189527.36<br />sale_price: 183000","pred.optimal: 141543.56<br />sale_price: 136500","pred.optimal: 202388.12<br />sale_price: 210000","pred.optimal: 148372.83<br />sale_price: 149300","pred.optimal: 118823.03<br />sale_price: 108000","pred.optimal: 184482.92<br />sale_price: 165400","pred.optimal: 140780.61<br />sale_price: 148000","pred.optimal:  76623.12<br />sale_price:  82500","pred.optimal: 119225.55<br />sale_price:  99000","pred.optimal:  86211.47<br />sale_price:  98000","pred.optimal: 189470.84<br />sale_price: 179500","pred.optimal: 156046.11<br />sale_price: 136500","pred.optimal: 143180.64<br />sale_price: 168000","pred.optimal: 118756.88<br />sale_price: 130000","pred.optimal: 176997.67<br />sale_price: 161900","pred.optimal: 172568.70<br />sale_price: 177000","pred.optimal: 126951.52<br />sale_price: 110000","pred.optimal: 257460.75<br />sale_price: 263400","pred.optimal: 133333.69<br />sale_price: 126000","pred.optimal: 110698.52<br />sale_price:  99500","pred.optimal: 295261.28<br />sale_price: 392500","pred.optimal: 269972.69<br />sale_price: 271000","pred.optimal: 212724.77<br />sale_price: 213000","pred.optimal: 204302.67<br />sale_price: 228500","pred.optimal: 237609.83<br />sale_price: 228950","pred.optimal: 211695.27<br />sale_price: 241500","pred.optimal: 281012.44<br />sale_price: 287000","pred.optimal: 263084.50<br />sale_price: 294000","pred.optimal: 250197.53<br />sale_price: 264966","pred.optimal: 166674.45<br />sale_price: 167500","pred.optimal: 219817.56<br />sale_price: 218689","pred.optimal: 231291.36<br />sale_price: 195000","pred.optimal: 325964.19<br />sale_price: 305000","pred.optimal: 269582.91<br />sale_price: 298751","pred.optimal: 330993.69<br />sale_price: 370000","pred.optimal: 128758.84<br />sale_price: 124000","pred.optimal: 109638.28<br />sale_price: 115000","pred.optimal: 140177.34<br />sale_price: 152500","pred.optimal: 112310.17<br />sale_price:  98000","pred.optimal:  85464.65<br />sale_price:  81000","pred.optimal: 142182.59<br />sale_price: 138000","pred.optimal: 132086.17<br />sale_price: 103000","pred.optimal: 196387.69<br />sale_price: 225000","pred.optimal: 170528.44<br />sale_price: 168000","pred.optimal: 160415.05<br />sale_price: 160000","pred.optimal: 162803.83<br />sale_price: 160000","pred.optimal: 343199.78<br />sale_price: 349265","pred.optimal: 236800.00<br />sale_price: 392000","pred.optimal: 379497.12<br />sale_price: 441929","pred.optimal: 195962.94<br />sale_price: 192000","pred.optimal: 147521.78<br />sale_price: 148000","pred.optimal: 180318.95<br />sale_price: 197000","pred.optimal: 143471.78<br />sale_price: 152000","pred.optimal: 119410.60<br />sale_price: 123000","pred.optimal: 116043.02<br />sale_price: 120500","pred.optimal: 104776.05<br />sale_price: 113500","pred.optimal: 140681.89<br />sale_price: 142500","pred.optimal: 371284.75<br />sale_price: 356000","pred.optimal: 381954.84<br />sale_price: 314813","pred.optimal: 271368.88<br />sale_price: 318000","pred.optimal: 286709.88<br />sale_price: 322400","pred.optimal: 320856.53<br />sale_price: 318000","pred.optimal: 317895.94<br />sale_price: 338931","pred.optimal: 496971.28<br />sale_price: 479069","pred.optimal: 283634.12<br />sale_price: 260116","pred.optimal: 298690.38<br />sale_price: 317000","pred.optimal: 292908.69<br />sale_price: 285000","pred.optimal: 254474.44<br />sale_price: 250000","pred.optimal: 187748.14<br />sale_price: 194700","pred.optimal: 190408.89<br />sale_price: 204000","pred.optimal: 189984.95<br />sale_price: 200000","pred.optimal: 196941.73<br />sale_price: 207000","pred.optimal: 221516.98<br />sale_price: 209500","pred.optimal: 279625.94<br />sale_price: 277500","pred.optimal: 300599.06<br />sale_price: 318061","pred.optimal: 179451.80<br />sale_price: 178900","pred.optimal: 176547.67<br />sale_price: 168165","pred.optimal: 176663.98<br />sale_price: 171925","pred.optimal: 220071.81<br />sale_price: 198444","pred.optimal: 194520.31<br />sale_price: 181134","pred.optimal: 170503.31<br />sale_price: 156932","pred.optimal: 197813.81<br />sale_price: 172500","pred.optimal: 237195.39<br />sale_price: 226500","pred.optimal: 244479.17<br />sale_price: 259000","pred.optimal: 180147.53<br />sale_price: 188500","pred.optimal: 169322.06<br />sale_price: 165500","pred.optimal: 207027.69<br />sale_price: 211000","pred.optimal: 567872.00<br />sale_price: 745000","pred.optimal: 313987.06<br />sale_price: 322500","pred.optimal: 296083.44<br />sale_price: 290000","pred.optimal: 399155.47<br />sale_price: 419005","pred.optimal: 313675.94<br />sale_price: 147000","pred.optimal: 341440.84<br />sale_price: 311872","pred.optimal: 234843.05<br />sale_price: 250000","pred.optimal: 145826.19<br />sale_price: 147000","pred.optimal: 186825.62<br />sale_price: 181000","pred.optimal: 190970.41<br />sale_price: 201000","pred.optimal: 162419.73<br />sale_price: 179000","pred.optimal: 135268.80<br />sale_price: 128000","pred.optimal: 124727.58<br />sale_price: 125500","pred.optimal: 275641.88<br />sale_price: 300000","pred.optimal: 142164.00<br />sale_price: 129000","pred.optimal: 309569.66<br />sale_price: 285000","pred.optimal: 187713.86<br />sale_price: 166000","pred.optimal: 230297.61<br />sale_price: 193800","pred.optimal: 192966.28<br />sale_price: 208900","pred.optimal: 219302.25<br />sale_price: 207500","pred.optimal: 176008.34<br />sale_price: 177000","pred.optimal: 255229.72<br />sale_price: 239000","pred.optimal: 277408.59<br />sale_price: 301000","pred.optimal: 187596.17<br />sale_price: 194000","pred.optimal: 201146.38<br />sale_price: 213750","pred.optimal: 175574.69<br />sale_price: 187000","pred.optimal: 193799.75<br />sale_price: 190000","pred.optimal: 239388.22<br />sale_price: 226000","pred.optimal: 162650.77<br />sale_price: 164000","pred.optimal: 193892.25<br />sale_price: 188500","pred.optimal: 244522.31<br />sale_price: 255000","pred.optimal: 148426.39<br />sale_price: 156000","pred.optimal: 132863.70<br />sale_price: 139900","pred.optimal: 140861.33<br />sale_price: 151500","pred.optimal: 133945.61<br />sale_price: 145000","pred.optimal: 128057.56<br />sale_price: 124000","pred.optimal: 151114.39<br />sale_price: 140500","pred.optimal: 143514.56<br />sale_price: 147000","pred.optimal:  64417.50<br />sale_price:  64000","pred.optimal: 131830.34<br />sale_price: 137000","pred.optimal: 128350.49<br />sale_price: 105000","pred.optimal: 127487.48<br />sale_price: 126000","pred.optimal: 155927.67<br />sale_price: 175000","pred.optimal: 135072.38<br />sale_price: 144000","pred.optimal: 136356.67<br />sale_price: 141000","pred.optimal: 132716.48<br />sale_price: 120000","pred.optimal: 162981.75<br />sale_price: 163500","pred.optimal: 144008.09<br />sale_price: 142000","pred.optimal: 140032.08<br />sale_price: 134500","pred.optimal: 123294.19<br />sale_price: 115000","pred.optimal: 148176.12<br />sale_price: 153000","pred.optimal: 130698.70<br />sale_price: 135000","pred.optimal: 282161.84<br />sale_price: 301600","pred.optimal: 114934.52<br />sale_price: 109000","pred.optimal: 129396.80<br />sale_price: 128500","pred.optimal: 106450.23<br />sale_price:  64500","pred.optimal: 107172.87<br />sale_price: 102000","pred.optimal: 145042.58<br />sale_price: 152000","pred.optimal: 127731.39<br />sale_price: 141000","pred.optimal: 111664.62<br />sale_price:  89471","pred.optimal:  89753.67<br />sale_price: 108500","pred.optimal: 113267.39<br />sale_price: 114000","pred.optimal: 126494.28<br />sale_price: 124500","pred.optimal:  92214.66<br />sale_price: 104500","pred.optimal: 131224.98<br />sale_price: 137500","pred.optimal: 107158.05<br />sale_price: 102000","pred.optimal: 119548.56<br />sale_price:  90000","pred.optimal: 146179.78<br />sale_price: 153575","pred.optimal:  74055.35<br />sale_price: 113000","pred.optimal: 154805.64<br />sale_price: 169500","pred.optimal: 130022.58<br />sale_price: 139400","pred.optimal: 125243.80<br />sale_price: 127000","pred.optimal: 125761.78<br />sale_price: 128500","pred.optimal: 129285.64<br />sale_price: 122000","pred.optimal: 258356.16<br />sale_price: 200500","pred.optimal: 114463.51<br />sale_price: 126000","pred.optimal: 113479.60<br />sale_price: 118500","pred.optimal: 159887.75<br />sale_price: 165000","pred.optimal: 124717.40<br />sale_price: 123000","pred.optimal: 115747.64<br />sale_price: 108000","pred.optimal: 127322.97<br />sale_price: 119900","pred.optimal: 117471.23<br />sale_price: 115000","pred.optimal: 124556.89<br />sale_price: 134500","pred.optimal: 130945.36<br />sale_price: 129000","pred.optimal: 169585.34<br />sale_price: 112000","pred.optimal: 154066.41<br />sale_price: 165250","pred.optimal: 156753.30<br />sale_price: 150000","pred.optimal: 152819.30<br />sale_price: 137000","pred.optimal: 147973.38<br />sale_price: 130000","pred.optimal:  81538.06<br />sale_price: 109900","pred.optimal: 213754.58<br />sale_price: 243000","pred.optimal: 107679.66<br />sale_price: 118000","pred.optimal: 146621.14<br />sale_price: 150000","pred.optimal:  94879.81<br />sale_price:  86000","pred.optimal: 137719.80<br />sale_price: 130000","pred.optimal:  95878.44<br />sale_price:  96000","pred.optimal: 181420.80<br />sale_price: 228500","pred.optimal: 195153.44<br />sale_price: 179900","pred.optimal: 280939.03<br />sale_price: 301000","pred.optimal: 223674.28<br />sale_price: 220000","pred.optimal: 186907.66<br />sale_price: 200141","pred.optimal: 259747.11<br />sale_price: 246500","pred.optimal: 206188.22<br />sale_price: 203000","pred.optimal: 215375.48<br />sale_price: 212999","pred.optimal: 178614.39<br />sale_price: 176432","pred.optimal: 282516.00<br />sale_price: 277000","pred.optimal: 213788.12<br />sale_price: 187500","pred.optimal: 208440.19<br />sale_price: 204750","pred.optimal: 186690.91<br />sale_price: 190550","pred.optimal: 193961.16<br />sale_price: 200000","pred.optimal: 234467.83<br />sale_price: 211000","pred.optimal: 142548.48<br />sale_price: 131500","pred.optimal: 189085.64<br />sale_price: 185000","pred.optimal: 182018.83<br />sale_price: 179400","pred.optimal: 177825.73<br />sale_price: 168675","pred.optimal: 175121.20<br />sale_price: 167000","pred.optimal: 131258.67<br />sale_price: 118500","pred.optimal: 135809.05<br />sale_price: 133500","pred.optimal: 169713.72<br />sale_price: 171500","pred.optimal: 143398.14<br />sale_price: 140000","pred.optimal: 125783.52<br />sale_price: 131750","pred.optimal:  89397.53<br />sale_price:  79000","pred.optimal: 105047.15<br />sale_price: 110000","pred.optimal: 140787.62<br />sale_price: 148500","pred.optimal: 142991.55<br />sale_price: 139000","pred.optimal: 196122.61<br />sale_price: 238000","pred.optimal: 144291.38<br />sale_price: 140000","pred.optimal: 294967.97<br />sale_price: 315000","pred.optimal: 209514.73<br />sale_price: 215000","pred.optimal: 122692.05<br />sale_price: 123900","pred.optimal: 201533.97<br />sale_price: 170000","pred.optimal: 125568.16<br />sale_price: 115000","pred.optimal: 134748.77<br />sale_price: 129850","pred.optimal: 165395.86<br />sale_price: 150909","pred.optimal: 129255.43<br />sale_price: 118400","pred.optimal:  61046.20<br />sale_price:  68104","pred.optimal: 287144.28<br />sale_price: 375000","pred.optimal: 168954.12<br />sale_price: 183500","pred.optimal: 140323.50<br />sale_price: 134000","pred.optimal: 220513.38<br />sale_price: 245000","pred.optimal: 201368.56<br />sale_price: 210000","pred.optimal: 232433.22<br />sale_price: 244000","pred.optimal: 243802.92<br />sale_price: 267300","pred.optimal: 172565.67<br />sale_price: 174000","pred.optimal: 338061.84<br />sale_price: 392000","pred.optimal: 295648.81<br />sale_price: 281213","pred.optimal:  83466.57<br />sale_price:  85500","pred.optimal:  87363.32<br />sale_price:  93900","pred.optimal:  78508.99<br />sale_price:  75190","pred.optimal: 185133.03<br />sale_price: 196000","pred.optimal: 131600.70<br />sale_price: 129500","pred.optimal: 126639.10<br />sale_price: 129500","pred.optimal: 180214.73<br />sale_price: 192100","pred.optimal: 183871.64<br />sale_price: 185000","pred.optimal: 294832.81<br />sale_price: 320000","pred.optimal: 241543.69<br />sale_price: 250000","pred.optimal: 241958.05<br />sale_price: 274725","pred.optimal: 179477.05<br />sale_price: 165600","pred.optimal: 425047.34<br />sale_price: 457347","pred.optimal: 493195.03<br />sale_price: 545224","pred.optimal: 460147.00<br />sale_price: 535000","pred.optimal: 406190.44<br />sale_price: 438780","pred.optimal: 180380.97<br />sale_price: 169000","pred.optimal: 316014.84<br />sale_price: 318000","pred.optimal: 429645.78<br />sale_price: 470000","pred.optimal: 241540.61<br />sale_price: 235000","pred.optimal: 213910.89<br />sale_price: 241500","pred.optimal: 295853.28<br />sale_price: 250000","pred.optimal: 168216.67<br />sale_price: 179900","pred.optimal: 267411.53<br />sale_price: 294323","pred.optimal: 176933.06<br />sale_price: 181000","pred.optimal: 151952.47<br />sale_price: 155000","pred.optimal: 133082.52<br />sale_price: 138800","pred.optimal: 107971.30<br />sale_price:  97500","pred.optimal:  94553.25<br />sale_price:  91500","pred.optimal:  98655.42<br />sale_price:  89000","pred.optimal: 153615.06<br />sale_price: 145000","pred.optimal: 423206.81<br />sale_price: 412083","pred.optimal: 244889.67<br />sale_price: 252000","pred.optimal: 275170.00<br />sale_price: 293000","pred.optimal: 404508.66<br />sale_price: 415000","pred.optimal: 313883.91<br />sale_price: 300000","pred.optimal: 274664.56<br />sale_price: 275500","pred.optimal: 365766.31<br />sale_price: 345000","pred.optimal: 303676.28<br />sale_price: 298236","pred.optimal: 358861.06<br />sale_price: 360000","pred.optimal: 190505.53<br />sale_price: 202500","pred.optimal: 344049.56<br />sale_price: 332200","pred.optimal: 179210.88<br />sale_price: 169990","pred.optimal: 177157.72<br />sale_price: 169985","pred.optimal: 171499.91<br />sale_price: 172785","pred.optimal: 284833.91<br />sale_price: 258000","pred.optimal: 195476.66<br />sale_price: 234000","pred.optimal: 273309.81<br />sale_price: 264561","pred.optimal: 191171.02<br />sale_price: 173000","pred.optimal: 326439.84<br />sale_price: 350000","pred.optimal: 328552.53<br />sale_price: 321000","pred.optimal: 255487.88<br />sale_price: 252000","pred.optimal: 297300.78<br />sale_price: 290000","pred.optimal: 192205.05<br />sale_price: 186500","pred.optimal: 137542.78<br />sale_price: 132000","pred.optimal: 145061.88<br />sale_price: 142500","pred.optimal: 154506.72<br />sale_price: 150000","pred.optimal: 133579.91<br />sale_price: 125000","pred.optimal: 116389.37<br />sale_price: 116000","pred.optimal: 129590.66<br />sale_price: 137000","pred.optimal: 314293.22<br />sale_price: 310000","pred.optimal: 258662.61<br />sale_price: 262000","pred.optimal: 153029.66<br />sale_price: 147400","pred.optimal: 168997.03<br />sale_price: 183900","pred.optimal: 145095.33<br />sale_price: 139000","pred.optimal: 203091.14<br />sale_price: 200000","pred.optimal: 190436.73<br />sale_price: 170000","pred.optimal: 132457.44<br />sale_price: 135500","pred.optimal: 141434.83<br />sale_price: 157500","pred.optimal: 160205.61<br />sale_price: 146000","pred.optimal: 192404.47<br />sale_price: 190000","pred.optimal: 121600.67<br />sale_price: 129900","pred.optimal: 156067.81<br />sale_price: 157500","pred.optimal: 127062.53<br />sale_price: 140000","pred.optimal: 125856.70<br />sale_price: 127000","pred.optimal: 140398.91<br />sale_price: 146500","pred.optimal: 124180.96<br />sale_price: 121000","pred.optimal: 280587.03<br />sale_price: 235000","pred.optimal: 145892.67<br />sale_price: 143000","pred.optimal: 141591.78<br />sale_price: 145250","pred.optimal: 138770.09<br />sale_price: 156000","pred.optimal: 217428.86<br />sale_price: 167000","pred.optimal: 146333.73<br />sale_price: 159000","pred.optimal: 297376.12<br />sale_price: 180000","pred.optimal: 106922.60<br />sale_price: 105000","pred.optimal: 170570.55<br />sale_price: 159000","pred.optimal: 146087.58<br />sale_price: 147000","pred.optimal: 115117.44<br />sale_price: 115000","pred.optimal: 145515.83<br />sale_price: 159500","pred.optimal: 115143.77<br />sale_price: 120000","pred.optimal: 186823.31<br />sale_price: 183000","pred.optimal: 153022.22<br />sale_price: 157500","pred.optimal: 278870.75<br />sale_price: 277500","pred.optimal: 211741.34<br />sale_price: 207500","pred.optimal: 131048.69<br />sale_price: 147500","pred.optimal:  77560.90<br />sale_price: 135000","pred.optimal:  89190.05<br />sale_price:  87500","pred.optimal: 136220.31<br />sale_price: 146000","pred.optimal: 137051.03<br />sale_price: 120000","pred.optimal: 117955.51<br />sale_price: 124000","pred.optimal: 172122.05<br />sale_price: 169000","pred.optimal: 139933.86<br />sale_price: 156500","pred.optimal: 167321.64<br />sale_price: 178000","pred.optimal: 101342.18<br />sale_price: 105000","pred.optimal: 131426.48<br />sale_price: 111500","pred.optimal:  94333.70<br />sale_price: 108000","pred.optimal: 116663.32<br />sale_price:  96900","pred.optimal: 169587.91<br />sale_price: 155000","pred.optimal: 149415.78<br />sale_price: 144000","pred.optimal: 144640.00<br />sale_price: 157000","pred.optimal:  64047.88<br />sale_price:  64500","pred.optimal: 152577.09<br />sale_price: 159000","pred.optimal: 125785.92<br />sale_price: 114500","pred.optimal:  97703.34<br />sale_price:  88000","pred.optimal: 129268.63<br />sale_price: 149000","pred.optimal:  97724.74<br />sale_price:  89000","pred.optimal: 103458.33<br />sale_price: 109000","pred.optimal: 214828.06<br />sale_price: 220000","pred.optimal: 124432.52<br />sale_price: 116500","pred.optimal: 142277.41<br />sale_price: 148000","pred.optimal: 132651.64<br />sale_price: 142500","pred.optimal: 166702.75<br />sale_price: 180000","pred.optimal: 158844.12<br />sale_price: 156500","pred.optimal: 133345.50<br />sale_price: 157000","pred.optimal: 134796.20<br />sale_price: 145000","pred.optimal: 164449.97<br />sale_price: 168000","pred.optimal: 161876.11<br />sale_price: 164000","pred.optimal: 132920.97<br />sale_price: 130000","pred.optimal: 143849.05<br />sale_price: 142500","pred.optimal: 137230.47<br />sale_price: 136900","pred.optimal: 131140.14<br />sale_price: 149900","pred.optimal: 226274.89<br />sale_price: 209000","pred.optimal: 235905.27<br />sale_price: 221500","pred.optimal: 256967.27<br />sale_price: 233555","pred.optimal: 239647.78<br />sale_price: 260000","pred.optimal: 267880.34<br />sale_price: 294900","pred.optimal: 215708.73<br />sale_price: 209700","pred.optimal: 223279.72<br />sale_price: 220000","pred.optimal: 151737.61<br />sale_price: 145000","pred.optimal: 194959.12<br />sale_price: 193000","pred.optimal: 210324.39<br />sale_price: 217000","pred.optimal: 221314.58<br />sale_price: 217000","pred.optimal: 225817.41<br />sale_price: 205000","pred.optimal: 133013.39<br />sale_price: 132500","pred.optimal: 150876.92<br />sale_price: 157500","pred.optimal: 135448.16<br />sale_price: 128500","pred.optimal: 277078.28<br />sale_price: 275000","pred.optimal: 255182.09<br />sale_price: 230000","pred.optimal: 215176.59<br />sale_price: 210900","pred.optimal: 239893.95<br />sale_price: 274300","pred.optimal: 237239.84<br />sale_price: 216837","pred.optimal: 143125.50<br />sale_price: 133000","pred.optimal: 147443.03<br />sale_price: 155900","pred.optimal: 240000.36<br />sale_price: 233230","pred.optimal: 196979.02<br />sale_price: 203160","pred.optimal: 117273.51<br />sale_price:  98000","pred.optimal: 140370.91<br />sale_price: 145000","pred.optimal: 146548.11<br />sale_price: 140000","pred.optimal: 142440.81<br />sale_price: 130000","pred.optimal: 142487.91<br />sale_price: 137500","pred.optimal:  98286.47<br />sale_price:  92000","pred.optimal:  97476.66<br />sale_price: 107000","pred.optimal: 133302.34<br />sale_price: 104900","pred.optimal: 120465.03<br />sale_price: 125000","pred.optimal:  97600.93<br />sale_price: 121000","pred.optimal: 107850.92<br />sale_price: 128000","pred.optimal: 117131.56<br />sale_price: 102000","pred.optimal: 145921.83<br />sale_price: 131000","pred.optimal: 193662.00<br />sale_price: 140000","pred.optimal: 255313.36<br />sale_price: 250000","pred.optimal: 197672.48<br />sale_price: 218000","pred.optimal: 229106.81<br />sale_price: 239000","pred.optimal: 242246.97<br />sale_price: 257000","pred.optimal:  94497.48<br />sale_price: 102000","pred.optimal:  45369.69<br />sale_price:  72000","pred.optimal:  91805.28<br />sale_price: 106500","pred.optimal:  67834.95<br />sale_price:  78000","pred.optimal: 222752.06<br />sale_price: 228000","pred.optimal: 171263.27<br />sale_price: 176000","pred.optimal: 214494.75<br />sale_price: 250000","pred.optimal: 194313.94<br />sale_price: 202000","pred.optimal: 317802.72<br />sale_price: 312500","pred.optimal: 202277.80<br />sale_price: 215000","pred.optimal: 160477.19<br />sale_price: 164000","pred.optimal:  78144.11<br />sale_price:  71000","pred.optimal: 167069.58<br />sale_price: 131000","pred.optimal: 143612.41<br />sale_price: 142500","pred.optimal: 208354.78<br />sale_price: 188000"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[45369.69140625,51983.6446795886,58597.5979529272,65211.5512262658,71825.5044996044,78439.457772943,85053.4110462816,91667.3643196202,98281.3175929589,104895.270866297,111509.224139636,118123.177412975,124737.130686313,131351.083959652,137965.03723299,144578.990506329,151192.943779668,157806.897053006,164420.850326345,171034.803599684,177648.756873022,184262.710146361,190876.663419699,197490.616693038,204104.569966377,210718.523239715,217332.476513054,223946.429786392,230560.383059731,237174.33633307,243788.289606408,250402.242879747,257016.196153085,263630.149426424,270244.102699763,276858.055973101,283472.00924644,290085.962519779,296699.915793117,303313.869066456,309927.822339794,316541.775613133,323155.728886472,329769.68215981,336383.635433149,342997.588706487,349611.541979826,356225.495253165,362839.448526503,369453.401799842,376067.35507318,382681.308346519,389295.261619858,395909.214893196,402523.168166535,409137.121439873,415751.074713212,422365.027986551,428978.981259889,435592.934533228,442206.887806566,448820.841079905,455434.794353244,462048.747626582,468662.700899921,475276.65417326,481890.607446598,488504.560719937,495118.513993275,501732.467266614,508346.420539953,514960.373813291,521574.32708663,528188.280359968,534802.233633307,541416.186906646,548030.140179984,554644.093453323,561258.046726661,567872],"y":[41318.9466390204,48141.1583931865,54963.3701473527,61785.5819015188,68607.793655685,75430.0054098511,82252.2171640173,89074.4289181834,95896.6406723496,102718.852426516,109541.064180682,116363.275934848,123185.487689014,130007.69944318,136829.911197346,143652.122951513,150474.334705679,157296.546459845,164118.758214011,170940.969968177,177763.181722343,184585.39347651,191407.605230676,198229.816984842,205052.028739008,211874.240493174,218696.45224734,225518.664001506,232340.875755673,239163.087509839,245985.299264005,252807.511018171,259629.722772337,266451.934526503,273274.146280669,280096.358034836,286918.569789002,293740.781543168,300562.993297334,307385.2050515,314207.416805666,321029.628559833,327851.840313999,334674.052068165,341496.263822331,348318.475576497,355140.687330663,361962.899084829,368785.110838996,375607.322593162,382429.534347328,389251.746101494,396073.95785566,402896.169609826,409718.381363992,416540.593118159,423362.804872325,430185.016626491,437007.228380657,443829.440134823,450651.651888989,457473.863643155,464296.075397322,471118.287151488,477940.498905654,484762.71065982,491584.922413986,498407.134168152,505229.345922319,512051.557676485,518873.769430651,525695.981184817,532518.192938983,539340.404693149,546162.616447316,552984.828201482,559807.039955648,566629.251709814,573451.46346398,580273.675218146],"text":"","type":"scatter","mode":"lines","name":"fitted values","line":{"width":3.77952755905512,"color":"rgba(51,102,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[45369.69140625,51983.6446795886,58597.5979529272,65211.5512262658,71825.5044996044,78439.457772943,85053.4110462816,91667.3643196202,98281.3175929589,104895.270866297,111509.224139636,118123.177412975,124737.130686313,131351.083959652,137965.03723299,144578.990506329,151192.943779668,157806.897053006,164420.850326345,171034.803599684,177648.756873022,184262.710146361,190876.663419699,197490.616693038,204104.569966377,210718.523239715,217332.476513054,223946.429786392,230560.383059731,237174.33633307,243788.289606408,250402.242879747,257016.196153085,263630.149426424,270244.102699763,276858.055973101,283472.00924644,290085.962519779,296699.915793117,303313.869066456,309927.822339794,316541.775613133,323155.728886472,329769.68215981,336383.635433149,342997.588706487,349611.541979826,356225.495253165,362839.448526503,369453.401799842,376067.35507318,382681.308346519,389295.261619858,395909.214893196,402523.168166535,409137.121439873,415751.074713212,422365.027986551,428978.981259889,435592.934533228,442206.887806566,448820.841079905,455434.794353244,462048.747626582,468662.700899921,475276.65417326,481890.607446598,488504.560719937,495118.513993275,501732.467266614,508346.420539953,514960.373813291,521574.32708663,528188.280359968,534802.233633307,541416.186906646,548030.140179984,554644.093453323,561258.046726661,567872,567872,567872,561258.046726661,554644.093453323,548030.140179984,541416.186906646,534802.233633307,528188.280359968,521574.32708663,514960.373813291,508346.420539953,501732.467266614,495118.513993275,488504.560719937,481890.607446598,475276.65417326,468662.700899921,462048.747626582,455434.794353244,448820.841079905,442206.887806566,435592.934533228,428978.981259889,422365.027986551,415751.074713212,409137.121439873,402523.168166535,395909.214893196,389295.261619858,382681.308346519,376067.35507318,369453.401799842,362839.448526503,356225.495253165,349611.541979826,342997.588706487,336383.635433149,329769.68215981,323155.728886472,316541.775613133,309927.822339794,303313.869066456,296699.915793117,290085.962519779,283472.00924644,276858.055973101,270244.102699763,263630.149426424,257016.196153085,250402.242879747,243788.289606408,237174.33633307,230560.383059731,223946.429786392,217332.476513054,210718.523239715,204104.569966377,197490.616693038,190876.663419699,184262.710146361,177648.756873022,171034.803599684,164420.850326345,157806.897053006,151192.943779668,144578.990506329,137965.03723299,131351.083959652,124737.130686313,118123.177412975,111509.224139636,104895.270866297,98281.3175929589,91667.3643196202,85053.4110462816,78439.457772943,71825.5044996044,65211.5512262658,58597.5979529272,51983.6446795886,45369.69140625,45369.69140625],"y":[38348.4802177588,45279.6067817182,52209.3169340957,59137.4381763965,66063.7720583989,72988.0899327899,79910.128114074,86829.5824535635,93746.1024158498,100659.284863477,107568.66794466,114473.725753423,121373.864804561,128268.423825595,135156.678851413,142037.855964881,148911.153994476,155775.778700216,162630.988126707,169476.145849818,176310.775383238,183134.606358727,189947.602847297,196749.967239456,203542.118745178,210324.651605399,217098.28208623,223863.794031476,230621.990656937,237373.656885673,244119.533323205,250860.300771666,257596.573109022,264328.896143929,271057.750331063,277783.555698259,284506.677813432,291227.434021104,297946.099481642,304662.912758294,311378.080836257,318091.783544003,324804.177396716,331515.398907588,338225.5674238,344934.787546402,351643.151190786,358350.739339526,365057.623533513,371763.867141284,378469.526440807,385174.651542871,391879.287180709,398583.473386611,405287.246072978,411990.637532451,418693.676869405,425396.390373119,432098.801841284,438800.932861142,445502.803054386,452204.430290997,458905.830876414,465607.019715715,472308.010457983,479008.815623518,485709.446716179,492409.914322811,499110.228201414,505810.397359498,512510.430123851,519210.334202773,525910.116741705,532609.78437304,539309.343260801,546008.799140789,552708.15735672,559407.422892804,566106.600403165,572805.694238446,572805.694238446,587741.656197846,580796.326524795,573851.080526824,566905.922554576,559960.857262174,553015.88963383,546071.025013258,539126.269136261,532181.628166861,525237.10873745,518292.717993471,511348.463643223,504404.354013494,497460.398111793,490516.605696122,483572.987353325,476629.554587261,469686.319918229,462743.296995314,455800.500723593,448857.947408504,441915.65492003,434973.642879863,428031.932875244,421090.548703866,414149.516655007,407208.865833041,400268.628530611,393328.840660117,386389.542253849,379450.77804504,372512.598144478,365575.058830133,358638.223470541,351702.163606592,344766.960220862,337832.705228742,330899.503231281,323967.473575663,317036.752775076,310107.497344707,303179.887113026,296254.129065232,289330.461764572,282409.160371412,275490.542230276,268574.972909078,261662.872435653,254754.721264676,247851.065204805,240952.518134005,234059.760854408,227173.533971537,220294.62240845,213423.829380949,206561.938732838,199709.666730228,192867.607614054,186036.180594292,179215.588061449,172405.794086536,165606.528301315,158817.314219474,152037.515416882,145266.389938144,138503.14354328,131746.975060765,124997.110573468,118252.826116273,111513.460416704,104778.419989554,98047.1789288493,91319.2753828033,84594.3062139605,77871.9208869123,71151.815252971,64433.7256266411,57717.4233606096,51002.7100046548,44289.4130602819,38348.4802177588],"text":"","type":"scatter","mode":"lines","line":{"width":3.77952755905512,"color":"transparent","dash":"solid"},"fill":"toself","fillcolor":"rgba(153,153,153,0.4)","hoveron":"points","hoverinfo":"x+y","showlegend":false,"xaxis":"x","yaxis":"y","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":60.6392694063927},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"Predicted sale price vs sales price","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[19244.5759765625,593997.115429688],"tickmode":"array","ticktext":["100000","200000","300000","400000","500000"],"tickvals":[100000,200000,300000,400000,500000],"categoryorder":"array","categoryarray":["100000","200000","300000","400000","500000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Predicted sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-605,780505],"tickmode":"array","ticktext":["0","200000","400000","600000"],"tickvals":[0,200000,400000,600000],"categoryorder":"array","categoryarray":["0","200000","400000","600000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Test sale price","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"23584f7a2376":{"x":{},"y":{},"Predicted":{},"Tested":{},"type":"scatter"},"23584273933":{"x":{},"y":{},"Predicted":{},"Tested":{}}},"cur_data":"23584f7a2376","visdat":{"23584f7a2376":["function (y) ","x"],"23584273933":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="questionnaire-design.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exercises.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
